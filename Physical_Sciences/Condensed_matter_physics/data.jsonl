{"id": 276815565, "title": "Multispectral non-line-of-sight imaging via deep fusion photography", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Deep Learning (Multispectral Data Fusion)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Reconstructing hidden scenes from indirect multispectral reflections suffers from wavelength-dependent scattering and material reflectance variations, limiting conventional single-band NLOS methods.", "adaptation_ground_truth": "A deep convolutional network fuses visible and infrared measurements through learned attention mechanisms, integrating cross-spectral features to resolve material-specific light transport and enhance hidden scene reconstruction.", "ground_truth_reasoning": "This approach addresses atomic constraints by jointly modeling spectral dependencies (Constraint 1), leveraging complementary SNR across bands (Constraint 3), and enforcing cross-band consistency through fused feature learning (Constraint 4), overcoming wavelength-specific scattering limitations.", "atomic_constraints": ["Constraint 1: Spectral Reflectance Dependency - Materials exhibit wavelength-specific reflectance properties requiring joint spectral analysis for accurate scene reconstruction.", "Constraint 2: Wavelength-Dependent Scattering - Photon transport in NLOS scenarios varies nonlinearly with wavelength due to material dispersion and surface interactions.", "Constraint 3: Infrared Band SNR Limitation - Infrared measurements suffer from low photon counts and thermal noise, necessitating fusion with visible bands for robust signal recovery.", "Constraint 4: Cross-Spectral Consistency - Reconstructed scenes must maintain physical coherence across spectral bands to avoid material misidentification artifacts."], "distractors": [{"option": "A vision transformer processes multispectral patches via self-attention, leveraging pretrained weights from natural images. Fine-tuning uses NLOS datasets to capture global dependencies across spectral channels for hidden scene inference.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 by ignoring wavelength-specific scattering physics through generic attention, and Constraint 3 due to high data requirements exceeding infrared band limitations."}, {"option": "Independent U-Nets process each spectral band, with outputs merged via late concatenation. Each network is trained separately using L1 loss on simulated albedo data, followed by volumetric rendering for 3D reconstruction.", "label": "Naive Application", "analysis": "Violates Constraint 1 by neglecting material reflectance correlations across bands and Constraint 4 due to inconsistent reconstructions from disjoint spectral processing."}, {"option": "Polarization-resolved measurements capture Stokes parameters at the relay wall. Inverse rendering optimizes surface normals using polarized light transport models, exploiting polarization signatures for depth estimation.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 by lacking multispectral fusion, failing to leverage infrared data for SNR enhancement, and Constraint 1 due to polarization-only material ambiguity."}]}}
{"id": 278031743, "title": "In-sensor compressing via programmable optoelectronic sensors based on van der Waals heterostructures for intelligent machine vision", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Compressed Sensing"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Conventional sensors generate excessive temporal/spectral data, causing bandwidth/energy bottlenecks in edge vision systems. In-sensor computing lacks efficient compression for multidimensional signals.", "adaptation_ground_truth": "Programmable 2D van der Waals heterostructure sensors integrating optical capture, non-volatile memory, and computational encoding. Achieves in-device snapshot compression (8:1) via photogating-controlled charge trapping in floating gates.", "ground_truth_reasoning": "This adaptation satisfies constraints by: 1) Using atomically thin materials enabling gate-tunable photoresponse for in-pixel computation, 2) Exploiting heterostructure band alignment for non-destructive charge trapping memory, 3) Leveraging 2D flexibility for monolithic integration of sensing/memory, 4) Utilizing quantum tunneling barriers for high-fidelity signal retention, 5) Enabling pixel-level programmability for adaptive compression kernels without external processors.", "atomic_constraints": ["Constraint 1: Quantum Tunneling Barrier - Must maintain sub-nm barrier thickness for precise charge trapping without leakage in floating-gate memory.", "Constraint 2: Photogating Volatility - Requires non-volatile photoresponse modulation to retain compressed data between capture cycles.", "Constraint 3: Monolithic Integration - Demands co-location of sensing, memory, and computation within single-pixel footprint.", "Constraint 4: Spectral-Temporal Coupling - Compression must preserve spatiotemporal correlations in multidimensional optical signals.", "Constraint 5: Sub-thermal Operation - Energy per operation must remain below kT for edge deployment viability."], "distractors": [{"option": "Implementing a vision transformer with learned compression tokens on raw sensor data. The model uses self-attention across spectral-temporal dimensions, followed by lightweight CNN classification on latent representations.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 and 5: Requires full data transfer to external processor, exceeding energy limits. Attention mechanisms lack physical coupling to photogating dynamics."}, {"option": "Standard compressed sensing with random Bernoulli measurement matrices. A CMOS sensor captures video frames, then an external FPGA solves l1-minimization for reconstruction before classification via cloud-based ResNet.", "label": "Naive Application", "analysis": "Violates Constraint 3 and 5: Separates sensing/computation, increasing latency. External reconstruction ignores photogating memory capabilities, wasting 2D heterostructure advantages."}, {"option": "Event camera with adaptive temporal binning for video compression. Dynamic vision sensor outputs encode intensity changes as spikes, grouped into fixed-interval packets for convolutional action recognition.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Lacks spectral resolution and non-volatile memory. Event-based sampling discards absolute intensity data critical for multidimensional compression."}]}}
{"id": 279404591, "title": "Dynamic Evolution of Complex Networks: A Reinforcement Learning Approach Applying Evolutionary Games to Community Structure", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Reinforcement Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Modeling how community structures in condensed matter networks co-evolve with strategic agent behaviors under competitive interactions, where local decisions impact global topology.", "adaptation_ground_truth": "Reinforcement learning agents update game-theoretic strategies through local neighbor interactions, dynamically rewiring connections to optimize cooperation payoffs while preserving scalable community formation.", "ground_truth_reasoning": "This RL approach respects locality constraints by limiting agent observations to neighbors, enables strategy-topology co-evolution via reward-driven rewiring, and scales efficiently through decentralized learning—addressing atomic constraints of local interactions, dynamic adaptation, and computational tractability.", "atomic_constraints": ["Constraint 1: Local Information Access - Agents observe only immediate neighbors due to physical interaction limits.", "Constraint 2: Dynamic Co-evolution - Network topology and agent strategies must adapt simultaneously.", "Constraint 3: Emergent Cooperation - Models must capture self-organizing cooperative behaviors from competitive games.", "Constraint 4: Scalability - Solutions must handle O(10^4+) nodes with minimal computational overhead."], "distractors": [{"option": "A transformer-based model processes global network embeddings to predict strategy shifts, using attention mechanisms to capture long-range dependencies between all nodes simultaneously.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by requiring full global state access, ignoring locality; also breaches Constraint 4 due to quadratic attention scaling."}, {"option": "Standard Q-learning optimizes individual rewards with centralized critic updates, while topology evolves via random link adjustments based on strategy fitness thresholds.", "label": "Naive Application", "analysis": "Breaches Constraint 2 by decoupling strategy/topology updates and Constraint 3 through isolated reward maximization without game-theoretic incentives."}, {"option": "Static community detection identifies modules via modularity optimization, followed by evolutionary game simulations on fixed partitions to analyze cooperation patterns.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 by freezing network structure, preventing co-evolution; ignores Constraint 3's dynamic feedback between topology and strategies."}]}}
{"id": 277665241, "title": "An integrated large-scale photonic accelerator with ultralow latency", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Diffractive Deep Neural Networks"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "The need for ultralow-latency computation in condensed matter physics simulations, hindered by electronic processing bottlenecks and energy inefficiency in conventional hardware.", "adaptation_ground_truth": "Monolithic integration of a diffractive deep neural network on a CMOS-compatible photonic chip, using wavelength-division multiplexing and optimized diffractive layers for all-optical parallel processing, minimizing electronic conversions and optical path lengths.", "ground_truth_reasoning": "This adaptation addresses atomic constraints by leveraging light-speed optical propagation to reduce latency, integrated waveguides to mitigate loss/crosstalk, dispersion management through wavelength control, and nanoscale fabrication for precise phase modulation. It enables energy-efficient, large-scale analog computations without electronic bottlenecks.", "atomic_constraints": ["Constraint 1: Speed of Light Propagation - Computation latency is fundamentally constrained by photon transit time, requiring minimized optical path lengths.", "Constraint 2: Optical Loss and Crosstalk - Integrated photonic systems suffer signal degradation from waveguide attenuation and inter-channel interference, limiting scalability.", "Constraint 3: Material Dispersion - Wavelength-dependent refractive indices in silicon cause phase distortion, restricting operational bandwidth.", "Constraint 4: Fabrication Precision - Nanoscale diffractive elements demand sub-wavelength feature accuracy to maintain phase control and signal integrity."], "distractors": [{"option": "Implementing a transformer-based model on an electronic GPU cluster with tensor core optimization, leveraging attention mechanisms for large-scale condensed matter simulations and high-throughput parallel processing.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: Electronic processing introduces signal delays exceeding light-speed limits, while data movement between components amplifies energy loss and latency, contradicting photonic efficiency goals."}, {"option": "Deploying a standard diffractive neural network with discrete spatial light modulators and free-space optics, using iterative phase tuning for all-optical inference tasks in condensed matter systems.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 4: Extended free-space optical paths increase latency beyond integrated limits, while macroscopic components lack nanoscale precision for phase control, causing signal degradation."}, {"option": "Utilizing photonic recurrent Ising machines with coherent laser networks and in-situ training, mapping condensed matter problems to spin Hamiltonians for parallel optimization on integrated circuits.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 and 4: Coherent interactions amplify dispersion-induced phase errors, and Hamiltonian mapping requires precision beyond typical fabrication tolerances for large-scale deployment."}]}}
{"id": 276212759, "title": "Real-time holographic camera for obtaining real 3D scene hologram", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Deep Neural Networks (DNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Conventional holographic cameras cannot capture dynamic 3D scenes in real-time due to computational complexity and physical wavefront constraints.", "adaptation_ground_truth": "A diffraction model-driven neural network that unrolls iterative physical propagation steps into network layers, enabling physics-compliant hologram synthesis at 4K resolution.", "ground_truth_reasoning": "This architecture embeds wave optics constraints directly into the network via differentiable diffraction modeling, satisfying real-time requirements through parallelized computation while maintaining physical accuracy in light propagation and interference patterns.", "atomic_constraints": ["Constraint 1: Wave Optics Compliance - Must obey Helmholtz equation for coherent light propagation through scattering media.", "Constraint 2: Temporal Latency - Computation must complete within 33ms for 30fps dynamic scene capture.", "Constraint 3: Spatial Bandwidth - Must resolve micron-scale interference patterns across 4K detector arrays.", "Constraint 4: Phase Stability - Phase reconstruction requires sub-wavelength path-length precision during wavefront sensing."], "distractors": [{"option": "A transformer-based architecture with cross-attention mechanisms processing multi-view inputs, trained on synthetic hologram datasets using self-supervised contrastive learning objectives.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by lacking embedded wave optics priors, causing unphysical interference artifacts. Attention mechanisms also exceed Constraint 2's latency requirements."}, {"option": "Standard U-Net architecture trained end-to-end with perceptual losses, using complex-valued convolutions to process interferogram inputs and output phase-retrieved holograms.", "label": "Naive Application", "analysis": "Violates Constraint 1 due to absence of diffraction modeling, and Constraint 3 by insufficient resolution handling. Generic architecture fails Constraint 4's phase precision requirements."}, {"option": "Layer-based CGH using unsupervised diffraction model optimization with Fourier neural operators, iteratively refining phase profiles through simulated propagation cycles.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 due to iterative refinement latency. Physical accuracy (Constraint 1) is maintained but real-time performance is compromised by sequential processing."}]}}
{"id": 278391702, "title": "AIVT: Inference of turbulent thermal convection from measured 3D velocity data by physics-informed Kolmogorov-Arnold networks", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Physics-Informed Kolmogorov-Arnold Networks (KANs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Inferring 3D temperature and pressure fields from sparse experimental velocity data in turbulent Rayleigh-Bénard convection, where multi-scale physics and strict adherence to governing equations are critical.", "adaptation_ground_truth": "Physics-informed Kolmogorov-Arnold networks (KANs) with spline-based learnable activation functions replace standard MLP weights. This architecture directly incorporates Navier-Stokes and energy equation residuals during training, enabling adaptive resolution of turbulent scales while maintaining physical consistency from limited velocity inputs.", "ground_truth_reasoning": "KANs address multi-scale turbulence through flexible spline activations that adapt to high-frequency features without manual scaling. Their compact structure handles sparse data efficiently, while physics-informed losses enforce solution validity. Boundary conditions are embedded via analytical distance functions, satisfying no-slip and thermal constraints critical to convection.", "atomic_constraints": ["Constraint 1: Multi-scale Resolution - Simultaneous capture of large-scale circulation and Kolmogorov-scale turbulence in 3D velocity fields.", "Constraint 2: Sparse Data Robustness - Inference from limited experimental velocity measurements with inherent noise and gaps.", "Constraint 3: PDE Compliance - Strict adherence to incompressible Navier-Stokes and Boussinesq energy equations for physical plausibility.", "Constraint 4: Boundary Enforcement - Exact satisfaction of no-slip velocity and fixed-temperature conditions at domain walls."], "distractors": [{"option": "Fine-tuning a pre-trained vision transformer with physics-informed loss terms. The model processes velocity field patches through self-attention layers, minimizing PDE residuals while leveraging transfer learning from fluid dynamics simulation datasets.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 due to high data requirements; transformers need extensive pre-training data unavailable experimentally. Attention mechanisms also struggle with localized small-scale turbulence resolution."}, {"option": "Standard physics-informed neural networks (PINNs) using MLP backbones. The network inputs spatial coordinates and velocity data, with loss functions combining measurement discrepancies and PDE residuals. Adaptive sampling focuses on high-error regions during optimization.", "label": "Naive Application", "analysis": "Violates Constraint 1 as MLPs exhibit spectral bias, failing to resolve high-frequency turbulent structures. Fixed activation functions limit multi-scale adaptation without architectural changes."}, {"option": "Multi-scale DNNs with Fourier feature embeddings for input encoding. Parallel subnetworks process different frequency bands of velocity data, with physics losses enforcing equation compliance. Domain decomposition handles complex boundary geometries.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 due to fixed Fourier bases; predetermined frequency bands cannot adaptively represent turbulent cascade interactions, leading to PDE residual errors at intermediate scales."}]}}
{"id": 276182382, "title": "Deep reinforcement learning for active flow control in a turbulent separation bubble", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Deep Reinforcement Learning (Proximal Policy Optimization)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Suppressing turbulent separation bubbles in aerodynamic flows requires real-time adaptive control due to nonlinear vortex dynamics and high-frequency disturbances that cause energy losses and instability.", "adaptation_ground_truth": "Proximal Policy Optimization with localized pressure sensor inputs and physics-informed reward balancing separation reduction and actuation cost, enabling real-time adaptive control through efficient policy updates.", "ground_truth_reasoning": "PPO's stable policy updates handle high-dimensional turbulence states, while localized sensors address partial observability. The physics-shaped reward function respects actuation limits and flow response timescales, avoiding control saturation.", "atomic_constraints": ["Constraint 1: Actuation Bandwidth - Synthetic jet actuators have finite frequency response (<1kHz) and amplitude limits.", "Constraint 2: Partial Observability - Full flow field measurements are infeasible; only sparse wall-pressure sensors are available.", "Constraint 3: Time-Scale Separation - Control must respond within vortex shedding timescales (O(ms)) to prevent separation growth.", "Constraint 4: Energy Budget - Zero-net-mass-flux actuators require minimal energy injection while maximizing separation suppression."], "distractors": [{"option": "Vision Transformer processing full 3D velocity fields with self-attention, pre-trained on DNS simulations to predict optimal actuation sequences for separation control.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Requires full-field measurements unavailable in experiments. Transformers' computational latency exceeds Constraint 3's real-time requirements."}, {"option": "Standard PPO with raw pressure sensor inputs mapped directly to jet commands using dense neural networks, optimizing solely for separation area reduction without actuation penalties.", "label": "Naive Application", "analysis": "Ignores Constraint 1 and 4: Unconstrained actuation causes actuator saturation. Raw inputs violate Constraint 2 by lacking noise filtering for sparse sensors."}, {"option": "Convolutional neural networks with translational symmetry priors processing velocity snapshots, trained via Q-learning to output periodic jet actuation for vortex synchronization.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: Requires full velocity fields. Assumed translational symmetry conflicts with separation bubble's spatial inhomogeneity (Constraint 4)."}]}}
{"id": 278365385, "title": "Toward Quantum Federated Learning", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Federated Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Integrating distributed machine learning with quantum systems faces fundamental physical limitations in coherence preservation, secure data handling, and hardware connectivity.", "adaptation_ground_truth": "Quantum Federated Learning framework combining quantum computing with federated learning, using a taxonomy of quantum techniques to optimize for coherence times, secure data transmission, and hardware connectivity constraints.", "ground_truth_reasoning": "The framework addresses quantum decoherence by limiting computation depth within coherence windows, respects no-cloning through in-situ quantum data processing, and accommodates sparse connectivity via distributed quantum operations. This holistic integration leverages quantum properties for privacy-preserving, resource-efficient learning.", "atomic_constraints": ["Constraint 1: Quantum Decoherence - Quantum states degrade within nanoseconds, requiring algorithms to complete before coherence loss.", "Constraint 2: No-Cloning Theorem - Quantum data cannot be duplicated, necessitating in-situ processing without raw data transfer.", "Constraint 3: Sparse Qubit Connectivity - Limited physical qubit connections restrict communication topology in distributed systems."], "distractors": [{"option": "Implement federated transformers where clients pre-train large language models on quantum simulation data. Centralized aggregation of attention weights enables knowledge transfer while maintaining data localization through classical encryption protocols.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by ignoring coherence windows during lengthy transformer training. Violates Constraint 2 through classical data encryption that cannot prevent quantum state collapse during measurement."}, {"option": "Standard federated averaging with quantum datasets: Clients locally train neural networks on measured quantum states. The server averages model weights periodically using encrypted classical communication channels for parameter aggregation.", "label": "Naive Application", "analysis": "Violates Constraint 2 by requiring quantum state measurements that destroy superposition. Violates Constraint 3 through unrestricted classical communication that ignores physical qubit connectivity limitations."}, {"option": "Differential privacy-enhanced federated learning with Laplace noise injection. Clients add calibrated noise to local model gradients before transmission, providing formal privacy guarantees for quantum material datasets through classical information theory.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 by extending computation beyond coherence times. Violates Constraint 2 through classical gradient transmission that necessitates destructive quantum state measurements."}]}}
{"id": 275236569, "title": "TOPS-speed complex-valued convolutional accelerator for feature extraction and inference", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Convolutional Neural Networks (CNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Processing phase-sensitive wave phenomena data (e.g., SAR images) requires complex-valued computations at ultra-high speeds for real-time satellite analysis, but electronic hardware faces bandwidth and energy limitations.", "adaptation_ground_truth": "An optical convolution accelerator using coherent nanophotonic circuits with designed phasors to process complex-valued data at 2+ TOPS, leveraging light's inherent amplitude/phase properties for SAR recognition.", "ground_truth_reasoning": "The accelerator exploits optics' natural ability to encode phase/amplitude in light waves, enabling parallel analog processing of complex-valued SAR data. This bypasses digital electronics' serial bottlenecks, achieving tera-scale operations with low latency while respecting phase sensitivity and computational density requirements.", "atomic_constraints": ["Constraint 1: Phase Sensitivity - Data represents wave phenomena (SAR) where phase carries critical information, requiring complex-valued operations.", "Constraint 2: Computational Density - Satellite data volumes demand tera-operations/second throughput for real-time analysis.", "Constraint 3: Analog Compatibility - Physical wave-based data aligns with analog optical processing, avoiding digital quantization losses.", "Constraint 4: Latency Sensitivity - Dynamic environments require sub-millisecond inference for feature extraction.", "Constraint 5: Energy Footprint - Edge deployment (satellites) necessitates ultra-low power per operation."], "distractors": [{"option": "A vision transformer pre-trained on SAR imagery, fine-tuned with complex-valued weights. Deployed on a tensor processing unit cluster for inference acceleration, utilizing mixed-precision quantization to enhance throughput.", "label": "SOTA Bias", "analysis": "Violates Constraints 2, 3, and 5: Transformers require excessive digital computations and lack inherent phase processing, leading to high latency/power. TPUs introduce analog-digital conversion bottlenecks."}, {"option": "Electronic complex-valued CNNs on a CMOS ASIC with systolic arrays. Uses 8-bit fixed-point arithmetic for matrix multiplications and on-chip memory hierarchies to optimize data reuse during convolution.", "label": "Naive Application", "analysis": "Violates Constraints 2 and 5: Digital electronics face von Neumann bottlenecks and serial execution, limiting throughput to below tera-scale ops while consuming orders of magnitude more energy than optics."}, {"option": "Memristor-based convolutional network with resistive crossbars mapping complex weights. Implements phase encoding via differential memristor pairs and analog activation functions for in-memory computing.", "label": "Cluster Competitor", "analysis": "Violates Constraints 2 and 4: Memristor write speeds and analog drift limit operation rates to giga-scale ops, introducing latency incompatible with real-time satellite data streams."}]}}
{"id": 276482779, "title": "Topology-driven quantum architecture search framework", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Graph Neural Network (GNN)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Designing noise-robust quantum circuits for condensed matter systems requires architectures that respect topological symmetries and hardware constraints.", "adaptation_ground_truth": "A GNN-based framework that encodes material topology as graphs to generate hardware-adapted quantum circuits, preserving symmetries during architecture search.", "ground_truth_reasoning": "GNNs inherently capture graph-structured symmetries in condensed matter systems, enforce qubit connectivity constraints through topology-aware circuit generation, and scale efficiently for large material graphs without manual feature engineering.", "atomic_constraints": ["Constraint 1: Topological Symmetry - Quantum circuits must preserve crystal/molecular graph symmetries for physical validity.", "Constraint 2: Qubit Connectivity - Circuit designs must align with hardware-specific qubit connection graphs to minimize swap gates.", "Constraint 3: Parameter Efficiency - Limited NISQ coherence times demand circuit architectures with minimal trainable parameters.", "Constraint 4: Scalability - Methods must handle combinatorial complexity in material systems with variable atom counts."], "distractors": [{"option": "A transformer-based architecture search processing material structures as token sequences, using self-attention to optimize quantum gate arrangements for target Hamiltonians.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by treating topological data as sequential tokens, breaking rotational symmetries critical for condensed matter systems."}, {"option": "Standard GNNs predicting circuit parameters from atom positions without topology constraints, followed by generic hardware transpilation for qubit mapping.", "label": "Naive Application", "analysis": "Violates Constraint 2 by decoupling material topology from hardware graphs, generating circuits requiring excessive swap operations."}, {"option": "Differentiable quantum architecture search optimizing gate sequences via gradient-based methods, directly minimizing Hamiltonian simulation errors on target systems.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 due to high parameter counts from continuous optimization, exceeding NISQ device coherence limits."}]}}
{"id": 279119200, "title": "Improved belief propagation is sufficient for real-time decoding of quantum memory", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Belief Propagation"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Real-time decoding of quantum memory is hindered by error degeneracy and correlated noise in quantum LDPC codes, where multiple error configurations produce identical syndromes.", "adaptation_ground_truth": "Enhanced belief propagation incorporates degeneracy-aware message weighting, where probabilities are scaled by the multiplicity of equivalent error paths. This modification maintains low computational latency while resolving quantum-specific ambiguities.", "ground_truth_reasoning": "The adaptation directly addresses degeneracy by weighting messages based on equivalent error configurations, satisfying real-time constraints through efficient parallel updates. It respects quantum topology by operating natively on sparse Tanner graphs without introducing computational overhead from symmetry-breaking methods.", "atomic_constraints": ["Constraint 1: Degeneracy Equivalence - Quantum syndromes map to equivalence classes of errors rather than unique configurations.", "Constraint 2: Real-Time Latency - Decoding must complete within quantum coherence timescales (<μs).", "Constraint 3: Sparse Graph Dynamics - Tanner graphs for quantum LDPC codes contain unavoidable short loops.", "Constraint 4: Correlated Noise Resilience - Errors exhibit spatial correlations from environmental interactions."], "distractors": [{"option": "A transformer-based decoder processes syndrome sequences using self-attention layers, trained on simulated error patterns. This captures long-range dependencies through contextual embeddings for quantum error correction.", "label": "SOTA Bias", "analysis": "Violates Real-Time Latency: Transformer inference latency exceeds coherence times due to sequential attention computations. Data hunger from required training simulations ignores sparse experimental data constraints."}, {"option": "Standard belief propagation runs with parallel message passing on the code's Tanner graph. Marginal probabilities are computed after fixed iterations, selecting maximum-likelihood errors for syndrome matching.", "label": "Naive Application", "analysis": "Violates Degeneracy Equivalence: Treats degenerate errors as distinct, reducing accuracy. Short loops in quantum Tanner graphs cause oscillatory behavior, worsening performance with correlated noise."}, {"option": "A union-find decoder constructs clusters from syndrome vertices, merging them via minimum-weight paths. Peeling algorithms resolve grown clusters to identify correctable errors in constant time per round.", "label": "Cluster Competitor", "analysis": "Violates Correlated Noise Resilience: Cluster growth heuristics underweight spatially correlated errors. Degeneracy handling relies on post-processing, increasing latency for complex syndromes beyond real-time limits."}]}}
{"id": 277044010, "title": "Tesseract: A Search-Based Decoder for Quantum Error Correction", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Parallelized Dual Revised Simplex Method"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Real-time decoding of topological quantum codes under hardware latency constraints, where degeneracy and sparse syndrome measurements require minimum-weight error matching without logical failures.", "adaptation_ground_truth": "A parallelized dual revised simplex method solves minimum-weight perfect matching via linear programming. Distributed processing accelerates syndrome-solving iterations while preserving degeneracy awareness through slack variable optimization for sparse quantum codes.", "ground_truth_reasoning": "The parallel simplex adaptation handles topological constraints by distributing lattice-based matching operations across cores. It respects quantum degeneracy through slack variables in the dual formulation, processes sparse syndromes via matrix sparsity exploitation, and meets real-time demands via parallel pivoting operations. The revised method's incremental updates suit dynamic error chains.", "atomic_constraints": ["Constraint 1: Topological Locality - Decoding must respect the geometric connectivity of qubits in 2D/3D lattices for fault paths.", "Constraint 2: Syndrome Sparsity - Algorithms must process O(1) non-zero measurements per round despite exponential error configurations.", "Constraint 3: Degeneracy Equivalence - Solutions must treat all stabilizer-equivalent errors as equally valid corrections.", "Constraint 4: Coherence-Time Latency - Full decoding must complete within quantum hardware's μs-scale coherence windows."], "distractors": [{"option": "A graph neural network processes the quantum code's Tanner graph, where attention layers aggregate neighborhood syndrome features. Training on simulated error data enables adaptive weight learning for topological correlations across the lattice structure.", "label": "SOTA Bias", "analysis": "Violates Constraint 4: GNNs' training latency and inference overhead exceed coherence-time requirements. Violates Constraint 3 by treating degenerate solutions as distinct during classification."}, {"option": "The standard dual simplex method solves the matching problem serially with tableau operations. Bland's rule prevents cycling during pivoting, while sparse matrix storage optimizes memory for syndrome graphs.", "label": "Naive Application", "analysis": "Violates Constraint 4: Serial pivoting causes latency exceeding coherence windows. Violates Constraint 1 by ignoring parallel topological path evaluations."}, {"option": "Belief propagation with custom message-passing schedules iteratively updates marginal probabilities over the code's factor graph. Residual belief scaling prioritizes high-uncertainty qubits, exploiting sparsity through dynamic message pruning.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: BP treats degenerate errors as probabilistically distinct. Violates Constraint 2 due to convergence failures under high syndrome sparsity."}]}}
{"id": 275474142, "title": "Physics-informed neural networks with adaptive loss weighting algorithm for solving partial differential equations", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Physics-Informed Neural Networks (PINNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Imbalanced convergence in multi-term PDE loss optimization due to disparate physical scales and competing constraints in condensed matter systems.", "adaptation_ground_truth": "A PINN framework implementing dynamic loss weighting where weights for PDE residual, boundary, and initial condition terms are continuously adjusted using gradient statistics to balance learning rates.", "ground_truth_reasoning": "This adaptation directly addresses scale disparities and evolving training priorities by monitoring gradient magnitudes to proportionally scale loss contributions, ensuring simultaneous convergence across all physical constraints without manual tuning.", "atomic_constraints": ["Constraint 1: Scale Disparity - PDE residual, boundary, and initial condition losses exhibit orders-of-magnitude differences in magnitude.", "Constraint 2: Dynamic Priority - Relative importance of loss components shifts nonlinearly during optimization.", "Constraint 3: Synchronized Convergence - Physical validity requires all constraints satisfied within tight tolerance simultaneously.", "Constraint 4: Gradient Conflict - Backpropagation updates for different loss terms exhibit antagonistic directions."], "distractors": [{"option": "Vision Transformer architecture pretrained on PDE solution datasets, with equal weighting of residual and boundary losses through all training phases.", "label": "SOTA Bias", "analysis": "Violates Scale Disparity Constraint: Fixed equal weights ignore magnitude differences between loss terms, causing boundary conditions to dominate residual minimization in condensed matter systems."}, {"option": "Standard PINN with manual loss weighting coefficients optimized via grid search, incorporating residual, boundary, and initial condition terms in a static summation.", "label": "Naive Application", "analysis": "Violates Dynamic Priority Constraint: Static weights cannot adapt to shifting optimization landscapes during training, leading to premature stagnation in one physical domain."}, {"option": "Multi-task learning with shared memory layers for joint boundary-residual optimization, applying homoscedastic uncertainty weighting from initial calibration.", "label": "Cluster Competitor", "analysis": "Violates Synchronized Convergence Constraint: Fixed uncertainty weights lack real-time adjustment for evolving gradient conflicts, causing asynchronous satisfaction of physical conditions."}]}}
{"id": 276574690, "title": "Decision-tree decoders for general quantum LDPC codes", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Decision Trees"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Quantum LDPC codes suffer from degenerate error syndromes where multiple error patterns produce identical measurements, requiring decoders that identify equivalence classes rather than unique errors.", "adaptation_ground_truth": "Recursive decision trees traverse syndrome-dependent error paths by dynamically branching on high-confidence qubit corrections, grouping degenerate errors through adaptive path selection without explicit enumeration.", "ground_truth_reasoning": "Decision trees exploit sparsity by localizing decisions to qubit subsets, handle degeneracy through path grouping instead of probabilistic weights, and achieve real-time operation via parallelizable shallow trees compatible with quantum hardware cycles.", "atomic_constraints": ["Constraint 1: Degeneracy Equivalence - Correctable errors form equivalence classes under stabilizers; decoders must treat degenerate errors identically.", "Constraint 2: Sparsity Exploitation - Each syndrome measurement involves few qubits, requiring locally concentrated decision processes.", "Constraint 3: Real-Time Latency - Decoding must complete within quantum coherence timescales (~μs), prohibiting iterative refinement.", "Constraint 4: Non-Commuting Observables - Measurement sequences must respect anticommutation relations in parity checks."], "distractors": [{"option": "A transformer architecture processes full syndrome vectors via self-attention layers, trained end-to-end with simulated error data to predict optimal qubit corrections for quantum LDPC codes.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 (Real-Time Latency) due to high computational load from attention mechanisms, and Constraint 2 (Sparsity Exploitation) by globally processing all syndromes despite local qubit connectivity."}, {"option": "Standard decision trees with fixed-depth traversal classify syndromes using precomputed error probability tables, selecting corrections by minimizing Hamming distance to observed syndromes.", "label": "Naive Application", "analysis": "Violates Constraint 1 (Degeneracy Equivalence) by treating errors as unique rather than equivalent classes, and Constraint 4 (Non-Commuting Observables) through static probability models ignoring measurement incompatibility."}, {"option": "Ordered-statistics soft decoding ranks qubit-flip likelihoods from syndrome data, then applies belief propagation with weighted edges to resolve conflicts in quantum LDPC parity checks.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 (Real-Time Latency) due to iterative message-passing requirements, and Constraint 1 (Degeneracy Equivalence) by optimizing individual error probabilities over equivalence classes."}]}}
{"id": 276776392, "title": "Automorphism Ensemble Decoding of Quantum LDPC Codes", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Belief Propagation (BP)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Standard belief propagation struggles with degenerate errors and graph symmetries in quantum LDPC codes, causing poor decoding accuracy due to oscillatory behavior and convergence to local minima.", "adaptation_ground_truth": "Leveraging the automorphism group to generate diverse Tanner graph representations. Ensemble decoding runs parallel BP instances on these symmetry-equivalent graphs, aggregating results via syndrome-consistent voting to resolve degeneracy.", "ground_truth_reasoning": "The automorphism ensemble exploits code symmetries to create structurally varied decoding environments. Parallel BP executions avoid shared local minima, while voting over symmetry-equivalent solutions inherently handles quantum degeneracy. This respects graph topology constraints without data dependency.", "atomic_constraints": ["Constraint 1: Degeneracy - Quantum syndromes map to equivalence classes of error operators, requiring decoders to identify coset leaders rather than unique solutions.", "Constraint 2: Graph Symmetry - Tanner graphs contain automorphism-induced symmetric traps that cause iterative decoders to stagnate in correlated local minima.", "Constraint 3: Non-commutativity - Quantum measurements impose non-Abelian constraints on check nodes, limiting classical message-update rules."], "distractors": [{"option": "Transformer-based syndrome mapping using attention mechanisms over error configurations. Pre-trained on simulated noise models, the architecture outputs maximum-likelihood error patterns via multi-head cross-correlation layers.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Requires exponentially large training data to cover degenerate error classes, lacking inherent symmetry exploitation for unseen equivalence groups."}, {"option": "Parallel flooding-schedule BP with normalized check-node updates. Includes damping and early termination heuristics, optimized via Monte Carlo sampling over Pauli noise channels for quantum stabilizer codes.", "label": "Naive Application", "analysis": "Violates Constraint 2: Symmetric graph traps cause self-reinforcing message errors across nodes, amplifying oscillatory behavior without symmetry-breaking mechanisms."}, {"option": "Turbo-decoding with layered scheduling for quantum quasi-cyclic codes. Iterates between soft-input-soft-output decoders for subcodes, exchanging extrinsic information through interleaved syndrome matching.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Assumes commutative message passing incompatible with non-Abelian stabilizer constraints, causing gauge-symmetry violations in syndrome validation."}]}}
{"id": 277628190, "title": "Quantum annealing for combinatorial optimization: a benchmarking study", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Annealing"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Combinatorial optimization problems in condensed matter physics exhibit exponential computational complexity, making classical solutions intractable for large-scale systems like spin glasses or lattice models.", "adaptation_ground_truth": "Benchmarking quantum annealing on D-Wave hardware by mapping optimization problems to Ising spin Hamiltonians. Performance is evaluated against classical solvers using time-to-solution metrics across varying problem sizes and connectivity constraints.", "ground_truth_reasoning": "Quantum annealing leverages quantum tunneling to escape local minima in energy landscapes, directly addressing the exponential complexity of spin glass systems. Mapping to native Ising Hamiltonians respects hardware connectivity limits while exploiting quantum effects for ground-state sampling.", "atomic_constraints": ["Constraint 1: Native Ising Representation - Problems must map to quadratic Hamiltonians compatible with qubit interactions on quantum annealers.", "Constraint 2: Limited Qubit Connectivity - Hardware topology (e.g., Chimera/Pegasus graphs) restricts direct long-range interactions between spins.", "Constraint 3: Quantum Decoherence Times - Annealing schedules must complete within coherence windows to maintain quantum superposition."], "distractors": [{"option": "Training a graph neural network on Ising model instances to predict ground states. The architecture incorporates message-passing layers to capture spin interactions, with weights optimized via stochastic gradient descent on synthetic datasets.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 by ignoring decoherence timelines; neural networks require extensive classical training data incompatible with quantum coherence windows."}, {"option": "Implementing simulated annealing with parallel tempering for Ising optimization. Temperature schedules are optimized via adaptive Monte Carlo methods, and spin configurations are updated using Metropolis-Hastings sampling across replicas.", "label": "Naive Application", "analysis": "Violates Constraint 1 as classical sampling lacks quantum tunneling, preventing efficient escape from local minima in rough energy landscapes."}, {"option": "Applying quantum approximate optimization (QAOA) with superconducting circuits. Parameterized unitary layers optimize variational wavefunctions, and quantum gates implement Ising couplings with iterative classical feedback loops.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 due to gate-based hardware requiring all-to-all connectivity unrealizable on limited-qubit annealers."}]}}
{"id": 277263414, "title": "Ultra-compact multi-task processor based on in-memory optical computing", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Artificial Neural Networks (ANNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Simultaneous multi-task processing in photonic hardware faces severe spatial constraints and thermal crosstalk when scaling operations within ultra-compact footprints.", "adaptation_ground_truth": "Integrated phase-change material cells enable in-memory optical computing, storing ANN weights non-volatilely. Multi-wavelength light signals perform parallel matrix operations directly within the memory substrate, eliminating separate processing units.", "ground_truth_reasoning": "Phase-change materials (e.g., GST) provide non-volatile, low-footprint weight storage compatible with photonic waveguides. Multi-wavelength operation leverages material dispersion to isolate tasks, avoiding spatial separation. This satisfies ultra-compactness while mitigating thermal interference through localized heating.", "atomic_constraints": ["Constraint 1: Photonic Footprint - Component density must exceed λ/10 for viable on-chip integration at optical wavelengths.", "Constraint 2: Thermal Cross-talk - Active elements within 5μm proximity induce >10% phase error in adjacent waveguides.", "Constraint 3: Material Dispersion - Wavelength-dependent nonlinearities limit usable spectral bands to <100nm range in common photonic substrates."], "distractors": [{"option": "Transformer-based photonic accelerator using microring weight banks. Attention mechanisms route signals through cascaded tunable resonators, with gradient descent optimizing ring coupling coefficients via microheaters.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Microring cascades require >50μm spacing for thermal isolation, exceeding compactness limits. Attention mechanisms demand reconfigurable routing incompatible with fixed in-memory structures."}, {"option": "ANN accelerator with photonic matrix multiplier and separate resistive RAM. Optical MZI arrays handle linear operations, while digital-electronic memory stores weights. Data shuttles via high-speed optical interconnects between units.", "label": "Naive Application", "analysis": "Violates Constraint 2: Physical separation between compute and memory units induces latency and heat dissipation exceeding thermal cross-talk thresholds. Electronic interfaces break all-optical processing."}, {"option": "Programmable photonic circuit with MEMS-actuated directional couplers. Reconfigurable waveguide meshes implement ANN layers through voltage-controlled beam splitting, with weights stored in external CMOS registers.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: MEMS actuators occupy >20μm² per coupler, breaching footprint limits. External memory access creates wavelength synchronization errors due to material dispersion in delay lines."}]}}
{"id": 276159829, "title": "Deep empirical neural network for optical phase retrieval over a scattering medium", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Deep Convolutional Neural Network (CNN)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Retrieving optical phase information from intensity measurements after light propagation through disordered scattering media, where wavefronts are distorted into complex speckle patterns.", "adaptation_ground_truth": "An end-to-end deep CNN trained on empirical speckle patterns to directly predict incident wavefront phase. The architecture incorporates physical priors of wave propagation and scattering, using convolutional layers to capture local speckle correlations while bypassing explicit transmission matrix inversion.", "ground_truth_reasoning": "The CNN's convolutional inductive bias efficiently models local speckle correlations (Constraint 1), while empirical training on experimental data overcomes limited datasets (Constraint 2). Physical priors embedded in the architecture ensure invertibility (Constraint 3), and noise robustness emerges from data-driven learning of scattering statistics (Constraint 4).", "atomic_constraints": ["Constraint 1: Speckle Correlation Complexity - Speckle patterns exhibit non-Gaussian spatial correlations requiring local feature extraction.", "Constraint 2: Limited Experimental Data - Scattering measurements are resource-intensive, restricting training samples.", "Constraint 3: Physical Invertibility - Phase retrieval requires implicit modeling of wave-equation constraints for stable inversion.", "Constraint 4: Noise Sensitivity - Experimental measurements contain Poisson shot noise and detector noise."], "distractors": [{"option": "A vision transformer pre-trained on natural images and fine-tuned with self-attention mechanisms to model global dependencies in speckle patterns. Transfer learning leverages large-scale datasets for feature extraction before phase regression.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformers require extensive data for pre-training and attention mechanisms underperform with limited experimental speckle samples."}, {"option": "Standard CNN architecture trained exclusively on simulated speckle data using random phase masks. Optimization minimizes mean squared error between predicted and ground-truth phases without physical regularization or experimental calibration.", "label": "Naive Application", "analysis": "Violates Constraint 4: Simulation-trained models ignore experimental noise statistics and fail to generalize to real detector noise in measurements."}, {"option": "Generative adversarial network with generator-discriminator pairs synthesizing speckle patterns from phase inputs. Adversarial training approximates the scattering transmission matrix distribution for iterative phase reconstruction.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: GANs lack explicit physical constraints for invertibility, causing instability in phase retrieval from single-shot measurements."}]}}
{"id": 276937568, "title": "Channel Estimation for Rydberg Atomic Receivers", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Bayesian Optimization"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurately estimating communication channels for Rydberg atomic receivers requires handling quantum-level uncertainties and experimental constraints while navigating high-dimensional parameter spaces with limited measurement opportunities.", "adaptation_ground_truth": "We develop a Bayesian optimization framework incorporating atomic Stark shift priors and quantum noise models. This customizes acquisition functions to prioritize physically feasible regions, reducing measurement rounds while respecting Rydberg energy level structures and decoherence constraints.", "ground_truth_reasoning": "Bayesian optimization is adapted by embedding domain knowledge of alkali atom quantum properties directly into the surrogate model. The physics-informed kernel structure accounts for Stark shift nonlinearities and quantum noise distributions, while the tailored acquisition function balances exploration with energy conservation constraints. This enables sample-efficient navigation of high-dimensional parameter spaces under experimental limitations.", "atomic_constraints": ["Constraint 1: Stark Shift Nonlinearity - Rydberg atoms exhibit complex nonlinear responses to electric fields due to quantum state mixing, requiring models that capture higher-order dependencies.", "Constraint 2: Quantum Decoherence Limits - Measurements must be completed within coherence timescales (typically microseconds), restricting data acquisition windows.", "Constraint 3: Sparse Experimental Sampling - Cryogenic setups and laser stabilization requirements limit the number of feasible experimental iterations.", "Constraint 4: Energy Level Conservation - Solutions must obey selection rules and preserve quantized energy level transitions during parameter optimization."], "distractors": [{"option": "Implement a vision transformer architecture pre-trained on simulated Rydberg spectra datasets. The self-attention mechanism processes multi-frequency microwave patterns through cascaded encoder blocks, outputting channel parameters via regression heads.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 (Sparse Experimental Sampling) as transformers require massive training data unavailable in atomic experiments, and Constraint 2 (Decoherence Limits) due to excessive computational latency during inference."}, {"option": "Apply standard Bayesian optimization with Matérn kernel and expected improvement acquisition. Systematically explore the parameter space through sequential measurements, updating Gaussian process surrogates after each experimental iteration.", "label": "Naive Application", "analysis": "Violates Constraint 1 (Stark Shift Nonlinearity) by using generic kernels that don't encode quantum response properties, and Constraint 4 (Energy Level Conservation) through unconstrained sampling of physically invalid configurations."}, {"option": "Design a first-order optimization scheme using stochastic gradients from RIS phase models. Adapt reconfigurable surface algorithms to tune microwave field parameters through backpropagation of receiver output errors.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 (Stark Shift Nonlinearity) as gradient methods assume differentiable landscapes incompatible with quantum discontinuities, and Constraint 2 (Decoherence Limits) due to iterative requirements exceeding atomic coherence times."}]}}
{"id": 278820908, "title": "Phase-Change Memory for In-Memory Computing", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Spiking Neural Networks (SNNs) / Neuromorphic Computing"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Implementing energy-efficient neuromorphic computing requires synaptic devices that emulate biological plasticity but face material-level stochasticity, endurance limits, and resistance drift in phase-change memory substrates.", "adaptation_ground_truth": "Spiking neural networks with spike-timing-dependent plasticity (STDP) implemented via phase-change memory devices, leveraging temporal sparsity and local weight updates to minimize device switching while exploiting crystallization dynamics for analog synaptic behavior.", "ground_truth_reasoning": "STDP aligns with PCM's physics: sparse spikes reduce write cycles (addressing endurance), amorphous-crystalline transitions naturally model synaptic weights, and event-driven updates accommodate resistance drift through dynamic renormalization without global recalibration.", "atomic_constraints": ["Constraint 1: Stochastic Switching - Phase transitions in chalcogenides exhibit probabilistic crystallization/amorphization kinetics incompatible with deterministic computing models.", "Constraint 2: Endurance Ceiling - Repeated phase transitions cause elemental segregation, limiting reliable device cycles below 10^9.", "Constraint 3: Resistance Drift - Amorphous phase resistance logarithmically increases post-programming, disrupting analog weight stability."], "distractors": [{"option": "A transformer-based architecture using phase-change memory crossbars for attention weight storage, trained via backpropagation to optimize in-memory matrix multiplications for large-scale language modeling tasks.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: Transformers require frequent dense weight updates during training, accelerating PCM endurance degradation. Stochastic switching disrupts precise attention score computation."}, {"option": "Standard spiking neural networks with backpropagation-trained weights mapped to phase-change memory arrays, using periodic conductance readouts and global weight recalibration to maintain inference accuracy.", "label": "Naive Application", "analysis": "Violates Constraint 2 and 3: Global recalibration necessitates excessive write cycles beyond endurance limits. Ignores resistance drift accumulation between calibrations."}, {"option": "Projected phase-change memory devices configured for multilevel storage, implementing convolutional neural networks with gradient descent optimization for pattern recognition via precise analog conductance tuning.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 and 3: Multilevel programming requires exact resistance targeting, undermined by stochastic switching. Resistance drift causes progressive loss of analog state fidelity."}]}}
{"id": 276250145, "title": "Is attention all you need to solve the correlated electron problem?", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Solving the quantum many-body problem for strongly correlated electrons, where wave functions exhibit complex entanglement and scale exponentially with system size.", "adaptation_ground_truth": "Transformer architecture with specialized positional encodings for lattice periodicity and an antisymmetrization layer enforcing fermionic constraints. Attention mechanisms capture long-range electron correlations while maintaining physical symmetries.", "ground_truth_reasoning": "The adaptation integrates periodic boundary conditions via positional encodings and guarantees fermionic antisymmetry through explicit architectural constraints. This allows efficient modeling of electron interactions in crystalline systems while preserving quantum mechanical symmetries essential for accuracy.", "atomic_constraints": ["Constraint 1: Fermionic antisymmetry - Wave functions must change sign under particle exchange to satisfy Pauli exclusion.", "Constraint 2: Lattice periodicity - Solutions must obey translational symmetry in crystalline materials.", "Constraint 3: Long-range correlations - Electron interactions span multiple lattice sites requiring non-local modeling.", "Constraint 4: Computational tractability - Methods must avoid exponential scaling with electron count."], "distractors": [{"option": "A foundation model pre-trained on diverse quantum datasets predicts electron densities via transfer learning. Fine-tuning leverages contextual embeddings to generalize across materials, utilizing large-scale pretraining for correlation mapping.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 and 4: Foundation models require excessive data for correlation accuracy and lack built-in periodicity/antisymmetry, leading to unphysical predictions without explicit constraints."}, {"option": "Standard Transformer processes electron coordinates through multi-head attention layers. Positional embeddings encode spatial relationships, with output heads predicting wave function values optimized via gradient descent.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Lacks enforced antisymmetry and periodic boundary handling, producing wave functions incompatible with fermionic systems or crystal symmetries."}, {"option": "Fermionic neural networks construct antisymmetric wave functions using Slater determinants. Continuous filters model local quantum interactions, with network parameters optimized via variational Monte Carlo sampling.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: While preserving antisymmetry, convolutional filters struggle with long-range electron correlations crucial in periodic lattices, unlike attention mechanisms."}]}}
{"id": 277104591, "title": "Toward Large-Scale Distributed Quantum Long Short-Term Memory with Modular Quantum Computers", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Long Short-Term Memory (QLSTM)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Implementing quantum recurrent neural networks for large-scale sequential data in condensed matter systems, constrained by current quantum hardware limitations.", "adaptation_ground_truth": "Distributed QLSTM implementation across modular quantum computers using photonic interconnects. This partitions the quantum circuit across specialized modules, synchronizing memory cells through entanglement-based communication protocols.", "ground_truth_reasoning": "The modular distribution addresses qubit scarcity by parallelizing operations across devices while photonic interconnects maintain coherence during inter-module communication. Atomic memory modules preserve quantum states locally, enabling longer sequence processing within decoherence limits.", "atomic_constraints": ["Constraint 1: Qubit Scarcity - Current quantum processors have limited physical qubits per module.", "Constraint 2: Coherence Time - Quantum states decohere before completing large sequential computations.", "Constraint 3: Connectivity Limits - Sparse qubit connectivity restricts complex gate operations.", "Constraint 4: Modular Architecture - Hardware requires distributed algorithms matching atomic memory/photonic link designs."], "distractors": [{"option": "Quantum transformer networks with multi-head attention mechanisms processing full sequences simultaneously. This architecture leverages parallel qubit operations for global context modeling in condensed matter systems.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: Transformers require excessive qubits for attention matrices and exceed coherence time with deep circuits."}, {"option": "Centralized QLSTM on monolithic quantum hardware using optimized gate sequences. Enhanced error mitigation techniques and qubit routing algorithms maximize circuit depth for temporal modeling.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Single-device execution hits qubit limits and connectivity bottlenecks for large-scale sequences."}, {"option": "Federated QLSTM with classical aggregation of quantum gradients. Local quantum processors compute partial gradients on data subsets, synchronized through classical parameter servers.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Classical synchronization bottlenecks prevent real-time quantum state transfer through photonic interconnects."}]}}
{"id": 276556955, "title": "High‐Fidelity Computational Microscopy via Feature‐Domain Phase Retrieval", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Deep Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Recovering phase information from intensity-only measurements in computational microscopy is ill-posed due to the loss of phase, noise, and the need for high-resolution reconstructions of complex samples.", "adaptation_ground_truth": "We propose a deep neural network that learns a feature representation from intensity patterns, embedding physical priors in the latent space. This enables end-to-end phase retrieval from single-shot measurements while maintaining consistency with wave optics constraints.", "ground_truth_reasoning": "The feature-domain approach addresses atomic constraints by learning noise-robust representations (Constraint 2) from limited measurements (Constraint 1), enforcing physical consistency through architecture design (Constraint 3), and enabling efficient large-scale reconstruction (Constraint 4) without iterative optimization.", "atomic_constraints": ["Constraint 1: Sample Sensitivity - Condensed matter samples degrade under prolonged exposure, requiring minimal measurements.", "Constraint 2: High Noise Environment - Low signal-to-noise ratios at high resolutions demand noise-robust reconstruction.", "Constraint 3: Physical Consistency - Reconstructions must satisfy wave optics constraints and non-negativity.", "Constraint 4: Scalability - Methods must handle gigapixel datasets from modern detectors efficiently."], "distractors": [{"option": "Employ a vision transformer pre-trained on natural images and fine-tuned for phase retrieval. The self-attention mechanism models global dependencies in diffraction patterns, leveraging transfer learning from large-scale vision datasets.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2 due to high data requirements and sensitivity to noise without embedded physical priors, increasing measurement needs and error propagation."}, {"option": "Use a standard U-Net architecture to directly map intensity measurements to phase images. Training incorporates mean-squared error loss and standard data augmentation techniques for improved generalization.", "label": "Naive Application", "analysis": "Lacks feature-domain physical priors, leading to inconsistencies with wave optics (Constraint 3) and suboptimal noise handling (Constraint 2) in complex samples."}, {"option": "Implement PhaseLift convex optimization with nuclear norm minimization for phase recovery. Multiple coded illuminations provide theoretical guarantees through semidefinite programming and lifted representations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 by requiring numerous measurements and Constraint 4 due to computational intractability for large-scale experimental data."}]}}
{"id": 276079503, "title": "Nonlinear inference capacity of fiber-optical extreme learning machines", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Reservoir Computing"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Implementing high-speed nonlinear inference in photonic systems constrained by optical hardware limitations and energy efficiency requirements.", "adaptation_ground_truth": "A fiber-optical extreme learning machine using a single time-delayed node with optical nonlinearity, where random projections are physically implemented through laser dynamics and delayed feedback.", "ground_truth_reasoning": "This approach satisfies optical constraints by leveraging inherent fiber nonlinearity for computation (Constraint 1), utilizing light-speed signal propagation (Constraint 2), maintaining hardware simplicity through single-node architecture (Constraint 3), and exploiting analog processing for energy efficiency (Constraint 4).", "atomic_constraints": ["Constraint 1: Optical Nonlinearity Requirement - Computation must exploit inherent material nonlinearities without external digital processing.", "Constraint 2: Light-Speed Propagation - Signal processing must occur at photon transit speeds without electronic conversion delays.", "Constraint 3: Hardware Simplicity - Physical implementation must avoid complex multi-node architectures due to alignment and stability challenges.", "Constraint 4: Analog Energy Efficiency - Solution must maintain ultra-low power consumption by avoiding digital-electronic components."], "distractors": [{"option": "A vision transformer pre-trained on optical transfer functions, integrated with digital signal processors to analyze fiber transmission patterns through attention mechanisms.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 (digital conversion breaks light-speed processing) and Constraint 4 (high-power electronic components required)"}, {"option": "Standard reservoir computing with multiple coupled semiconductor lasers, where each laser acts as a network node and output weights are tuned via offline optimization.", "label": "Naive Application", "analysis": "Violates Constraint 3 (multi-laser alignment complexity) and Constraint 1 (requires controlled inter-node couplings not naturally present in fibers)"}, {"option": "Optoelectronic reservoir computing using wavelength-division multiplexing in silicon photonics, with electronic feedback control for dynamic reconfiguration of node connections.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4 (electronic components increase power) and Constraint 2 (optoelectronic conversion limits processing speed)"}]}}
{"id": 277113094, "title": "Learning Chaos In A Linear Way", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Recurrent Orthogonal Networks"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurately modeling high-dimensional chaotic systems in condensed matter physics requires long-term stability and geometric structure preservation, which standard RNNs fail to achieve due to vanishing gradients and norm instability.", "adaptation_ground_truth": "Recurrent Orthogonal Networks parameterized via scaled Cayley transform enforce strict orthogonality in weight matrices. This maintains hidden state norm stability during recurrence, enabling robust long-horizon forecasting of chaotic trajectories while preserving dynamical symmetries.", "ground_truth_reasoning": "The orthogonal parameterization prevents gradient explosion/vanishing and preserves phase space volume—critical for chaotic systems obeying Liouville's theorem. It efficiently captures continuous spectra without spectral bias, satisfying condensed matter constraints of ergodicity and geometric consistency.", "atomic_constraints": ["Constraint 1: Norm Preservation - Chaotic systems require bounded state evolution to avoid artificial energy dissipation/growth in closed physical systems.", "Constraint 2: Continuous Spectrum Representation - Condensed matter chaos exhibits broadband frequency signatures demanding models without spectral distortion.", "Constraint 3: Long-Term Stability - Forecasting horizon must exceed Lyapunov timescales without trajectory divergence.", "Constraint 4: Volume-Preserving Dynamics - Hamiltonian systems mandate symplectic structure in learned flows per Liouville's theorem."], "distractors": [{"option": "Transformer models with self-attention mechanisms process chaotic time-series data. Positional encodings capture temporal dependencies while multi-head attention identifies cross-variable interactions for spatiotemporal forecasting.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Attention mechanisms induce spectral bias toward low frequencies, distorting continuous spectra critical for chaos. Data hunger exceeds typical condensed matter dataset scales."}, {"option": "Standard LSTM networks with forget gates regulate hidden state updates. Gradient clipping stabilizes training while layer normalization accelerates convergence for chaotic dynamics prediction tasks.", "label": "Naive Application", "analysis": "Violates Constraint 1: Non-orthogonal weights cause norm drift during recurrence, violating conservation laws. Vanishing gradients limit adherence to Constraint 3 for long horizons."}, {"option": "Reservoir Computing with fixed random recurrent connections and trainable readout layers. The echo state property provides nonlinear projection for model-free forecasting of high-dimensional chaotic attractors.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Random reservoirs lack enforced orthogonality, causing phase space volume contraction. Hyperparameter sensitivity conflicts with Constraint 2's spectral accuracy needs."}]}}
{"id": 277955917, "title": "Enhancing LLM-based Quantum Code Generation with Multi-Agent Optimization and Quantum Error Correction", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Multi-Agent Optimization"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Generating resource-efficient quantum simulation circuits for condensed matter systems while mitigating inherent quantum noise and gate errors.", "adaptation_ground_truth": "Multi-agent LLM framework with specialized roles: one agent generates base quantum circuits, another optimizes gate sequences for error reduction, and a third integrates topological error correction codes, validated through quantum circuit simulators.", "ground_truth_reasoning": "This approach addresses quantum constraints through role specialization: circuit optimization minimizes decoherence-sensitive operations, error-correction integration handles fault tolerance, and simulator validation ensures physical realizability. Multi-agent collaboration enables iterative refinement of circuit depth and qubit allocation under coherence time limits.", "atomic_constraints": ["Constraint: Decoherence Sensitivity - Quantum states degrade rapidly (<100μs coherence time), requiring minimal circuit depth.", "Constraint: Gate Fidelity Limits - Native quantum gates exhibit 99-99.9% fidelity, necessitating error-aware compilation.", "Constraint: Qubit Sparsity - NISQ devices have <100 physical qubits, demanding compact circuit designs."], "distractors": [{"option": "Directly applying GPT-4 with chain-of-thought prompting to generate quantum circuits, leveraging its reasoning capabilities for step-by-step gate sequence construction and parameter optimization.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Single-agent LLMs ignore coherence time limits, producing deep circuits vulnerable to decoherence without specialized error mitigation layers."}, {"option": "Fine-tuning a single LLM on quantum assembly datasets to output circuits, followed by standard transpilation passes for hardware compatibility and basic gate decomposition.", "label": "Naive Application", "analysis": "Violates Constraint 2: Lacks error-aware optimization, resulting in gate sequences exceeding fidelity thresholds and accumulating uncorrected noise."}, {"option": "Retrieval-augmented generation system indexing quantum algorithm templates, synthesizing circuits by adapting retrieved components to target condensed matter Hamiltonians.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Retrieved templates ignore qubit sparsity, producing non-compact circuits exceeding NISQ device capacity without resource optimization."}]}}
{"id": 277272156, "title": "Feedback-enhanced quantum reservoir computing with weak measurements", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Reservoir Computing"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Quantum measurement collapse disrupts reservoir dynamics and degrades coherence in condensed matter systems, limiting real-time information processing capabilities.", "adaptation_ground_truth": "Implementing weak measurements with real-time feedback loops preserves quantum coherence while extracting computational signals, enhancing reservoir performance through adaptive system tuning.", "ground_truth_reasoning": "Weak measurements minimize state disturbance in fragile quantum systems (Constraint 1), while feedback enables dynamic adjustment to input signals (Constraint 2) within coherence time limits (Constraint 3), maintaining non-equilibrium computational states.", "atomic_constraints": ["Constraint 1: Quantum State Preservation - Measurements must avoid wavefunction collapse to maintain coherent reservoir dynamics.", "Constraint 2: Non-Equilibrium Processing - Computation requires continuous system evolution without stable equilibria for time-series tasks.", "Constraint 3: Decoherence Timescales - Operations must complete within finite quantum coherence windows of condensed matter systems."], "distractors": [{"option": "Applying a transformer model with self-attention mechanisms to simulate quantum dynamics, leveraging pre-trained weights for predicting reservoir behavior through sequence modeling of measurement data.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 3: Transformers require discrete state sampling that collapses quantum coherence and exceed decoherence times through classical computation overhead."}, {"option": "Standard quantum reservoir computing with projective measurements after each input injection, using fixed Hamiltonian parameters and linear readout layers for signal processing without dynamical adjustments.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Projective measurements destroy quantum states, while lack of feedback prevents adaptation to input perturbations in non-equilibrium conditions."}, {"option": "Training a generative adversarial network to synthesize quantum reservoir states, where the generator produces entanglement patterns and the discriminator validates physical plausibility for dynamics forecasting.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 and 3: GANs operate offline with iterative training incompatible with real-time processing and exceed coherence times through multi-epoch optimization cycles."}]}}
{"id": 277104275, "title": "EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using Classical Data", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Amplitude Embedding"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Efficient quantum state preparation for amplitude embedding of high-dimensional classical data in noisy, resource-constrained NISQ devices.", "adaptation_ground_truth": "EnQode employs a divide-and-conquer strategy with sparse state preparation techniques, optimizing gate sequences by exploiting data locality and structural sparsity in condensed matter systems.", "ground_truth_reasoning": "This approach minimizes circuit depth by recursively decomposing high-dimensional data into localized subproblems, reducing gate count and coherence time requirements. Sparsity exploitation avoids unnecessary rotations for near-zero amplitudes, directly addressing NISQ constraints while maintaining embedding fidelity for condensed matter datasets.", "atomic_constraints": ["Constraint 1: Decoherence Time - Quantum operations must complete within the limited coherence window (typically microseconds) before qubit states decay.", "Constraint 2: Gate Fidelity - Each additional quantum gate introduces exponential error accumulation due to imperfect physical operations.", "Constraint 3: Data Sparsity - Condensed matter wavefunctions exhibit localized electron density distributions with many near-zero amplitude regions."], "distractors": [{"option": "A quantum transformer architecture processes classical data through attention-based feature maps, then encodes outputs via variational quantum circuits with parameterized rotational gates.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformer's high parameter count necessitates deep variational circuits with excessive gate sequences, amplifying cumulative errors beyond NISQ tolerance."}, {"option": "Standard amplitude embedding using Grover-Rudolph state preparation with uniformly applied controlled rotations across all qubits, optimized via gate compilation techniques.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Global rotations on full Hilbert space exceed coherence limits; ignores sparsity by processing zero-amplitude regions equally."}, {"option": "QuantumNAS noise-adaptive architecture search explores circuit topologies using reinforcement learning, selecting gates based on simulated hardware noise profiles.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Neural architecture search requires extensive circuit evaluations during training, exceeding coherence time constraints for real-time embedding."}]}}
{"id": 278000532, "title": "QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Designing efficient QAOA circuits for combinatorial optimization in condensed matter systems requires balancing expressiveness with hardware constraints, as manual tuning scales poorly with qubit count.", "adaptation_ground_truth": "A Transformer architecture fine-tuned on quantum circuit data generates adaptive QAOA ansätze with regularized gate sequences, dynamically adjusting depth and operators while maintaining hardware-compatible structures.", "ground_truth_reasoning": "The Transformer captures complex parameter patterns across problem instances through attention mechanisms, enforcing regularity via architectural priors. This adapts circuits to specific Hamiltonians while ensuring NISQ-compatible gate sequences and avoiding symmetry violations through learned constraints.", "atomic_constraints": ["Constraint 1: NISQ Hardware Limitations - Quantum circuits must minimize depth and two-qubit gates to respect coherence time and connectivity constraints of superconducting qubits.", "Constraint 2: Hamiltonian Symmetry Preservation - Circuit ansätze must conserve spin-inversion or lattice translation symmetries inherent in condensed matter systems to avoid unphysical solutions.", "Constraint 3: Instance-Specific Adaptivity - Circuit structures require dynamic adjustment to problem graph connectivity and energy landscapes without manual redesign per instance.", "Constraint 4: Parameter Concentration Regularity - Optimization landscapes demand smooth, correlation-preserving parameter initialization to avoid barren plateaus during variational loops."], "distractors": [{"option": "Apply GroverGPT's quantum search framework to optimize QAOA parameters through amplitude amplification. The model iteratively refines circuits by encoding cost Hamiltonians into its oracle layer.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 4: Grover-style amplification requires deep circuits exceeding NISQ coherence limits and disrupts parameter concentration through unstructured search, increasing optimization instability."}, {"option": "Use classical Bayesian optimization with Gaussian processes to tune QAOA parameters. The method models energy landscapes via kernel functions and sequentially queries quantum hardware for expectation values.", "label": "Naive Application", "analysis": "Violates Constraint 3: Lacks structural adaptivity, generating fixed-topology circuits. Sequential evaluation scales poorly with qubit count and ignores problem-specific symmetries in Constraint 2."}, {"option": "Implement the Generative Quantum Eigensolver (GQE) with quantum neural networks to construct QAOA ansätze. The variational generator learns circuit distributions through repeated Hamiltonian measurements.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Quantum generative training requires excessive circuit evaluations incompatible with NISQ constraints. Fails to enforce regularity in gate sequences (Constraint 4) for hardware efficiency."}]}}
{"id": 276742629, "title": "Scalable Connectivity for Ising Machines: Dense to Sparse", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Stochastic p-bits (Probabilistic Bits)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Physical Ising machines cannot implement dense all-to-all connectivity required for complex optimization problems due to hardware wiring limitations and scalability constraints.", "adaptation_ground_truth": "The paper maps dense Ising models to sparsely connected hardware using stochastic p-bits with reconfigurable time-division multiplexing, emulating dense interactions through probabilistic sampling and dynamic routing.", "ground_truth_reasoning": "This adaptation respects hardware wiring constraints by replacing physical dense connections with virtualized probabilistic links. It maintains thermal noise tolerance through stochastic bit flipping and ensures scalability via linear resource growth with problem size, while preserving solution accuracy via statistical convergence.", "atomic_constraints": ["Constraint 1: Wiring Density Limit - Hardware fabrication imposes a maximum fanout per node due to metal layer routing constraints and signal integrity decay.", "Constraint 2: Thermal Noise Tolerance - Room-temperature operation requires probabilistic bit designs resilient to thermal fluctuations without cryogenic cooling.", "Constraint 3: Area Scaling Law - Physical implementation must maintain sub-quadratic area growth with increasing node count to remain manufacturable."], "distractors": [{"option": "Employing a transformer-based architecture pre-trained on Ising instances to predict spin configurations, using attention mechanisms to model long-range correlations on fixed sparse hardware.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 due to quadratic memory scaling in attention layers, making hardware implementation impractical. Ignores thermal noise tolerance by assuming deterministic predictions."}, {"option": "Implementing direct dense p-bit connectivity via analog crossbar arrays with memristive synapses, enabling continuous conductance tuning for exact Ising model representation in hardware.", "label": "Naive Application", "analysis": "Violates Constraint 1 through O(N²) wiring requirements exceeding physical routing capacity. Contradicts Constraint 3 by necessitating exponentially growing chip area."}, {"option": "Applying adiabatic bifurcation in nonlinear Hamiltonian systems with parametric oscillators, gradually evolving couplings to embed dense problems in sparse physical networks.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 due to extreme sensitivity to thermal noise at room temperature. Incompatible with Constraint 1 as oscillator synchronization requires precise frequency control impractical in sparse topologies."}]}}
{"id": 276663583, "title": "Deep reinforcement cross-domain transfer learning of active flow control for three-dimensional bluff body flow", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Deep Reinforcement Learning (Soft Actor-Critic)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Active flow control for 3D bluff bodies requires policies that generalize across varying Reynolds numbers and experimental setups, where direct policy retraining is prohibitively expensive.", "adaptation_ground_truth": "Mutual information-driven transfer learning within Soft Actor-Critic framework, aligning source/task domains via shared latent representations to handle state-action dimension mismatches during cross-Reynolds number policy adaptation.", "ground_truth_reasoning": "The mutual information alignment respects multi-scale physics by preserving turbulence features during transfer, handles high dimensionality through learned latent spaces, and overcomes domain shift by extracting invariant features across flow regimes without full retraining.", "atomic_constraints": ["Constraint 1: High-dimensional state space - 3D bluff body flow requires processing volumetric velocity/pressure fields exceeding 10⁶ dimensions.", "Constraint 2: Multi-scale physics - Control must resolve interactions between large-scale vortex shedding and small-scale turbulent structures simultaneously.", "Constraint 3: Domain shift - Policies must adapt to unseen Reynolds numbers where flow separation characteristics nonlinearly change."], "distractors": [{"option": "Implement a Vision Transformer architecture processing raw 3D flow snapshots, leveraging pre-training on synthetic turbulence datasets to predict optimal control actions across varying Reynolds regimes.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by requiring impractical computational resources for high-dimensional 3D attention operations and Constraint 2 due to inadequate inductive biases for multi-scale physics."}, {"option": "Standard Soft Actor-Critic with convolutional encoders trained separately per Reynolds number, using wall shear stress measurements as state inputs and plasma actuators for real-time separation control.", "label": "Naive Application", "analysis": "Violates Constraint 3 by ignoring cross-domain transfer, requiring exhaustive retraining for each flow regime despite shared underlying physics."}, {"option": "Direct shape optimization via policy gradient methods, iteratively modifying bluff body geometry based on pressure distribution feedback to minimize drag without flow control actuators.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 by disregarding active control requirements and Constraint 3 due to geometric solutions lacking adaptability to dynamic flow changes."}]}}
{"id": 277940159, "title": "Robust Decentralized Quantum Kernel Learning for Noisy and Adversarial Environment", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Kernel Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Centralized quantum kernel learning fails in distributed condensed matter systems due to noise susceptibility and adversarial vulnerabilities during multi-site data integration.", "adaptation_ground_truth": "A decentralized framework combining local quantum kernel training with Byzantine-robust aggregation and error-mitigated quantum circuits for resilient distributed learning.", "ground_truth_reasoning": "This approach satisfies quantum noise constraints through circuit-level error suppression, adversarial constraints via Byzantine-resistant consensus, and data locality by processing sensitive condensed matter datasets on-site without centralization. Resource limits are addressed through optimized shallow circuits.", "atomic_constraints": ["Constraint 1: Quantum Noise Resilience - Near-term quantum devices exhibit high decoherence rates during kernel evaluations in condensed matter simulations.", "Constraint 2: Adversarial Vulnerability - Distributed data collection across laboratories risks malicious perturbations during model aggregation.", "Constraint 3: Data Locality - Experimental quantum state datasets cannot be centralized due to privacy and bandwidth limitations.", "Constraint 4: Resource Limitations - Limited qubit coherence times restrict quantum circuit depth for kernel estimation."], "distractors": [{"option": "Utilizing a centralized quantum transformer architecture with attention mechanisms trained on aggregated condensed matter data for global feature extraction.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 3: Transformers require centralized data processing, increasing adversarial attack surfaces and ignoring locality constraints."}, {"option": "Standard quantum kernel learning with a single quantum processor, using gradient-based optimization to maximize kernel alignment on preprocessed condensed matter datasets.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Centralized execution amplifies quantum noise impact and disregards data locality requirements in distributed environments."}, {"option": "Variational quantum circuits optimized via PennyLane's automatic differentiation for direct combinatorial optimization of lattice model parameters in a centralized server.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 and 4: Unmitigated variational circuits accumulate noise errors during optimization and exceed coherence time limits for complex lattices."}]}}
{"id": 277066665, "title": "A Comparative Study of Quantum Optimization Techniques for Solving Combinatorial Optimization Benchmark Problems", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Optimization Techniques (including Quantum Annealing, QAOA)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Solving combinatorial optimization problems relevant to condensed matter systems (e.g., spin glass ground states) with quantum methods constrained by NISQ hardware limitations.", "adaptation_ground_truth": "Hybrid variational optimization using parameterized quantum circuits with classical co-processors for iterative parameter tuning, benchmarking against quantum annealing on embedded problem instances.", "ground_truth_reasoning": "This approach combines shallow quantum circuits to respect coherence limits with classical optimization for noise-robust parameter tuning, while hardware-native embeddings address connectivity constraints in benchmark comparisons.", "atomic_constraints": ["Constraint 1: Coherence Time - Quantum operations must complete within sub-millisecond qubit coherence windows before decoherence.", "Constraint 2: Qubit Connectivity - Physical qubit topologies restrict direct interactions, requiring problem graph embedding.", "Constraint 3: Gate Fidelity - Limited quantum gate precision necessitates low-depth circuits to minimize error accumulation."], "distractors": [{"option": "Implementing a graph neural network with attention mechanisms trained on combinatorial problem datasets to predict optimal configurations using classical high-performance computing clusters.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 3: Ignores quantum coherence requirements and gate fidelity limitations by relying solely on classical computation without quantum circuit optimizations."}, {"option": "Direct quantum annealing execution with complete graph embeddings using SWAP gates for arbitrary connectivity, following standard minor-embedding protocols without variational tuning.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: SWAP gate chains exceed coherence time while ignoring hardware-native connectivity, increasing circuit depth and error rates."}, {"option": "Applying polynomial-time classical heuristics with guaranteed approximation bounds for subgraph optimization, as referenced in Cluster A literature, using branch-and-bound methods for solution verification.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Classical methods bypass quantum gate fidelity constraints entirely but fail to leverage quantum advantages for condensed matter problem structures."}]}}
{"id": 277272257, "title": "KGMM: A K-means Clustering Approach to Gaussian Mixture Modeling for Score Function Estimation", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "K-means Gaussian Mixture Modeling (KGMM)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Estimating score functions for high-dimensional condensed matter systems where traditional Gaussian mixture models suffer from computational inefficiency and poor interpretability of physical states.", "adaptation_ground_truth": "KGMM integrates K-means clustering as initialization for Gaussian mixture modeling, partitioning state space into physically meaningful regions before covariance optimization. This hybrid approach accelerates convergence while preserving cluster interpretability.", "ground_truth_reasoning": "K-means efficiently handles high-dimensional data partitioning (Constraint 1) with linear complexity, enabling scalable processing of large physics datasets (Constraint 2). The Voronoi tessellation structure provides transparent correspondence between clusters and physical states (Constraint 3), unlike black-box alternatives.", "atomic_constraints": ["Constraint 1: High-Dimensional State Space - Condensed matter systems exhibit configuration spaces with hundreds of dimensions, necessitating linear-scaling partitioning methods.", "Constraint 2: Computational Tractability - Molecular dynamics simulations generate terabyte-scale datasets, requiring O(n) algorithms for feasible analysis.", "Constraint 3: Physically Interpretable Clusters - Mixture components must correspond to distinct thermodynamic phases or metastable states for scientific utility."], "distractors": [{"option": "A transformer-based architecture processes condensed matter trajectories using self-attention mechanisms. Layer normalization and residual connections model global dependencies across particle configurations for score estimation.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Quadratic attention complexity becomes prohibitive for high-dimensional condensed matter trajectories, exceeding computational budgets."}, {"option": "Standard Gaussian Mixture Models with Expectation-Maximization optimization, initialized via random sampling. Full covariance matrices capture correlations across all degrees of freedom during iterative likelihood maximization.", "label": "Naive Application", "analysis": "Violates Constraints 1 and 2: Random initialization in high dimensions requires excessive EM iterations, while full covariance estimation scales poorly with dimensionality."}, {"option": "Dynamic Mode Decomposition constructs reduced-order Koopman operators from trajectory snapshots. Spectral analysis of linear approximations estimates score functions through eigenmode projections of system observables.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Linear approximations cannot capture non-Gaussian features of phase transitions, losing physical interpretability of clusters."}]}}
{"id": 280323241, "title": "Next-Generation Quantum Neural Networks: Enhancing Efficiency, Security, and Privacy", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Neural Networks (QNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Training quantum neural networks for condensed matter systems faces vanishing gradients, hardware noise, and privacy risks in distributed data environments.", "adaptation_ground_truth": "A federated QNN framework with fully homomorphic encryption and barren plateau-resistant ansatz design, enabling secure multi-institutional collaboration while maintaining trainability on NISQ devices.", "ground_truth_reasoning": "Federated learning preserves data locality for privacy, FHE secures gradient exchanges, and tailored ansatz mitigates barren plateaus—collectively addressing condensed matter's distributed data sensitivity and quantum hardware limitations.", "atomic_constraints": ["Constraint 1: Barren Plateaus - Gradient magnitudes decay exponentially with qubit count in unstructured circuits.", "Constraint 2: NISQ Noise - Limited coherence times restrict circuit depth and require noise-robust architectures.", "Constraint 3: Data Sensitivity - Condensed matter datasets contain proprietary experimental/simulation details requiring confidentiality.", "Constraint 4: Distributed Provenance - Material research data resides across multiple institutions with limited sharing permissions."], "distractors": [{"option": "Implementing a centralized quantum transformer model for condensed matter property prediction, leveraging self-attention to capture long-range correlations in quantum states across diverse material classes.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 4: Transformers require extensive data centralization incompatible with distributed provenance, and their deep architectures exceed NISQ coherence limits."}, {"option": "Standard quantum neural network training with hardware-efficient ansatz on aggregated condensed matter datasets, optimized via parameter-shift rules and mini-batch processing for material property regression.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Centralized data aggregation ignores sensitivity requirements, and generic hardware-efficient ansatz suffers barren plateaus in high-dimensional material systems."}, {"option": "Quantum federated learning for financial fraud detection adapted to condensed matter, using weight averaging and differential privacy on transaction-style data to predict material behaviors without specialized encryption.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 and 4: Financial methods lack FHE-grade security for scientific IP, and transaction data structures mismatch condensed matter's quantum state representations."}]}}
{"id": 277452298, "title": "Force-Free Molecular Dynamics Through Autoregressive Equivariant Networks", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Autoregressive Equivariant Graph Neural Network"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate molecular dynamics requires force computation, which amplifies noise in learned potentials and violates physical symmetries during trajectory generation.", "adaptation_ground_truth": "Autoregressive SE(3)-equivariant graph network directly predicts atomic displacements without force intermediates, ensuring symmetry-preserving trajectory generation.", "ground_truth_reasoning": "The autoregressive approach conditions each atomic displacement on prior predictions, avoiding noisy force derivatives. SE(3)-equivariant operations intrinsically conserve angular momentum and rotational symmetries during position updates, satisfying fundamental physics constraints.", "atomic_constraints": ["Constraint 1: SE(3) Equivariance - Dynamics must be invariant to global rotations/translations to conserve angular momentum.", "Constraint 2: Force Noise Immunity - Avoid amplifying errors through numerical force differentiation.", "Constraint 3: Causality - Position updates must respect lightcone constraints of atomic interactions.", "Constraint 4: Energy Conservation - Trajectories must obey Hamiltonian/Lagrangian structure without explicit force computation."], "distractors": [{"option": "Transformer-based force predictor with attention over atomic coordinates. Forces are integrated via velocity Verlet algorithm, using gradient clipping and layer normalization for stability during dynamics simulation.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Transformers lack inherent SE(3)-equivariance, requiring exhaustive rotation augmentation. Violates Constraint 2: Force prediction and differentiation compound energy surface errors."}, {"option": "Equivariant GNN learns potential energy surface with higher-order B-spline basis. Forces derived via automatic differentiation support NPT ensemble integration using Langevin thermostats and periodic boundary conditions.", "label": "Naive Application", "analysis": "Violates Constraint 2: Explicit force computation propagates energy surface inaccuracies. Violates Constraint 4: Numerical integration errors accumulate without Hamiltonian structure enforcement."}, {"option": "3D Steerable CNNs process voxelized electron density maps to predict forces. Spatial convolutions with spherical harmonics ensure rotational equivariance, coupled with Nosé-Hoover chains for canonical sampling.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Voxelization breaks continuous atomic coordinates. Violates Constraint 4: Force dependence contradicts force-free dynamics; unsuitable for direct position prediction."}]}}
{"id": 280748960, "title": "A small set of critical hyper-motifs governs heterogeneous flow-weighted network resilience", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Hypergraph Analysis / Network Motif Detection"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Understanding mechanisms of cascading failures in flow-weighted networks and resolving discrepancies between global and local resilience during structural degradation.", "adaptation_ground_truth": "We model flow-weighted networks as hypergraphs encoding cascading failure pathways via hyperedges. Percolation theory identifies stable hyper-motifs during degradation, capturing heterogeneous flow and higher-order interactions for comprehensive resilience assessment.", "ground_truth_reasoning": "Hypergraphs inherently represent multi-node failure dependencies (Constraint 3), while percolation theory quantifies phase transitions (Constraint 4). Flow heterogeneity is embedded in hyperedge weights (Constraint 2), and cascading pathways ensure flow redistribution adheres to conservation (Constraint 1).", "atomic_constraints": ["Constraint 1: Flow Conservation - Flow redistribution during failures must obey continuity laws at nodes.", "Constraint 2: Heterogeneous Flow - Models must incorporate non-uniform flow capacities across network components.", "Constraint 3: Higher-order Interactions - Cascading failures require multi-node dependency modeling beyond pairwise edges.", "Constraint 4: Percolation Thresholds - Resilience frameworks must detect critical phase transitions in network connectivity."], "distractors": [{"option": "A graph neural network processes node features and flow weights via message passing. Training on cascade simulations, it predicts failure propagation using attention mechanisms to weight edge importance for resilience estimation.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 by relying on pairwise interactions, ignoring multi-node failure pathways. Violates Constraint 4 as data-driven approaches lack inherent percolation phase detection."}, {"option": "Standard weighted graph percolation removes edges probabilistically based on flow capacity. Global resilience is measured via giant component size decay; local resilience uses node betweenness centrality under flow constraints.", "label": "Naive Application", "analysis": "Violates Constraint 3 by treating failures as independent edge removals, not capturing hyperedge-dependent cascades. Violates Constraint 2 as flow heterogeneity reduces to edge weights without multi-node flow redistribution."}, {"option": "Optimal percolation identifies critical nodes using non-backtracking centrality. By sequentially removing high-impact nodes, we minimize the percolation threshold to control cascade spread and optimize network robustness.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 by ignoring flow conservation during targeted removals. Violates Constraint 2 as centrality measures overlook heterogeneous flow capacities in failure propagation."}]}}
{"id": 274693438, "title": "Quantum Distributed Event-Triggered Frequency Control for AC Microgrids Under FDIAs", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Distributed Optimization"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Classical microgrid frequency control is vulnerable to FDIAs, while quantum alternatives incur excessive communication overhead from continuous monitoring.", "adaptation_ground_truth": "Quantum event-triggered control with Zeno-free triggers and FDIA isolation via non-periodic communication analysis.", "ground_truth_reasoning": "Event-triggering reduces quantum channel usage by transmitting data only during significant deviations, satisfying quantum bandwidth limits. Non-periodic communication patterns enable direct FDIA detection by identifying anomalous link behavior, leveraging quantum state uniqueness without continuous measurement.", "atomic_constraints": ["Constraint 1: Quantum Measurement Collapse - Continuous monitoring collapses quantum states, disrupting coherence.", "Constraint 2: Quantum Channel Saturation - High-frequency communication exceeds qubit transmission capacity.", "Constraint 3: Non-Markovian Attack Dynamics - FDIAs exploit temporal correlations in periodic control signals."], "distractors": [{"option": "Quantum transformers process all DG frequency deviations via continuous attention mechanisms, enabling real-time FDIA detection through entangled qubit networks.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Continuous attention requires persistent quantum measurements, inducing decoherence and state collapse."}, {"option": "Standard quantum consensus synchronizes DG frequencies using continuous Lamport timestamps over quantum channels, with Gaussian filters smoothing measurement noise.", "label": "Naive Application", "analysis": "Violates Constraint 2: Continuous timestamp exchange saturates quantum channels, exceeding qubit transmission thresholds during transients."}, {"option": "Wireless load-sharing control with quantum key distribution (QKD) secures inverter output impedance signals using BB84 protocol during scheduled periodic updates.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Periodic updates create predictable communication windows, enabling FDIAs to inject correlated false data masked as legitimate signals."}]}}
{"id": 276329821, "title": "Defect modeling in semiconductors: the role of first principles simulations and machine learning", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Machine Learning Integration"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate prediction of defect properties in semiconductors requires computationally intensive first-principles calculations, limiting high-throughput screening and design optimization.", "adaptation_ground_truth": "The paper integrates graph neural networks with automated first-principles workflows to predict defect formation energies and charge transition levels. By leveraging symmetry-aware descriptors and transfer learning from high-throughput datasets, it captures local atomic environments while reducing computational costs.", "ground_truth_reasoning": "GNNs inherently respect SE(3) equivariance (Constraint 4) through message passing, capturing local bonding environments (Constraint 1). Transfer learning from Materials Project datasets addresses data scarcity (Constraint 3), while explicit charge-state encoding handles electronic variability (Constraint 2).", "atomic_constraints": ["Constraint 1: Local Structure Sensitivity - Defect properties depend critically on sub-nanometer atomic arrangements and bond distortions.", "Constraint 2: Charge State Variability - Defect formation energies shift nonlinearly with Fermi level and require explicit electron-counting.", "Constraint 3: Data Scarcity - High-fidelity defect calculations are computationally prohibitive, limiting training datasets.", "Constraint 4: Symmetry Equivariance - Predictions must remain invariant to rotation/translation operations (SE(3)) due to crystal periodicity."], "distractors": [{"option": "Implementing a vision transformer pre-trained on crystal structure images to classify defect configurations. The model leverages attention mechanisms across atomic positions, utilizing transfer learning from large-scale image datasets for property prediction.", "label": "SOTA Bias", "analysis": "Violates Constraint 4: Transformers lack inherent SE(3) equivariance, causing rotational variance in predictions. Ignores Constraint 2: Image representations cannot explicitly encode charge states or Fermi-level dependence."}, {"option": "Training a gradient-boosted decision tree ensemble on DScribe-generated atomic descriptors. Feature engineering includes Coulomb matrices and radial distribution functions, with hyperparameter optimization maximizing accuracy on formation energy benchmarks.", "label": "Naive Application", "analysis": "Violates Constraint 1: Fixed descriptors cannot adapt to unique defect-induced bond distortions. Ignores Constraint 4: Descriptor-based methods require explicit symmetry augmentation, increasing computational overhead."}, {"option": "Deploying AiiDA workflows for high-throughput DFT defect calculations across semiconductor libraries. Automated structure generation and simulation management enable systematic mapping of formation energies without machine learning acceleration.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Pure DFT approaches remain computationally prohibitive for large-scale screening. Lacks adaptation to Constraint 2: Requires manual configuration of charge-state calculations for each Fermi-level condition."}]}}
{"id": 277452202, "title": "Quantum approximate multi-objective optimization.", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Quantum Approximate Optimization Algorithm (QAOA)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Simultaneously optimizing competing quantum observables (e.g., energy gap and magnetization) in many-body systems where objectives exhibit complex trade-offs.", "adaptation_ground_truth": "Modified QAOA with adaptive weight vectors and entanglement-sharing ansatz to explore Pareto-optimal states while preserving quantum correlations.", "ground_truth_reasoning": "The adaptation dynamically adjusts objective weights during quantum evolution to maintain superposition of trade-off solutions. Shared entanglement layers preserve non-local correlations critical for condensed matter systems, satisfying symmetry constraints while enabling multi-objective exploration within NISQ device limitations.", "atomic_constraints": ["Constraint 1: Observable Commutation - Target observables may not commute, requiring simultaneous measurement strategies without state collapse.", "Constraint 2: Symmetry Preservation - Solutions must respect SU(2) spin rotation symmetry inherent in quantum magnets.", "Constraint 3: Resource Efficiency - Circuit depth must scale sub-linearly with objective count to avoid decoherence."], "distractors": [{"option": "Fine-tuning a pre-trained transformer model to predict optimal quantum circuit parameters by learning from simulation data of Ising spin chains.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Transformers cannot guarantee preservation of non-commuting observable relationships without explicit physical constraints, leading to measurement incompatibility."}, {"option": "Standard QAOA with weighted sum scalarization: combining objectives classically before optimization using fixed preference coefficients.", "label": "Naive Application", "analysis": "Violates Constraint 2: Fixed weights ignore dynamic trade-off exploration and break rotational symmetry through artificial bias, collapsing superposition of Pareto solutions."}, {"option": "Hypervolume indicator algorithm from multi-objective optimization, adapted to select quantum circuit parameters maximizing solution diversity in objective space.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Classical hypervolume calculations require exponential Pareto front evaluations, exceeding NISQ coherence times and ignoring entanglement resource constraints."}]}}
{"id": 277784174, "title": "Physical twinning for joint encoding-decoding optimization in computational optics: a review", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Convolutional Neural Networks (CNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Separate optimization of optical encoding hardware and computational decoding algorithms leads to suboptimal image reconstruction quality in resource-constrained microscopy systems.", "adaptation_ground_truth": "A differentiable CNN framework co-optimizes programmable optical encoder parameters (e.g., mask patterns) and decoder reconstruction networks through end-to-end learning, enforcing physical realizability constraints via backpropagation-compatible layers.", "ground_truth_reasoning": "This joint optimization respects fabrication limits through hardware-in-the-loop training, maximizes photon efficiency via learned encoding, leverages shift-equivariance through convolutional layers, and enables real-time reconstruction critical for dynamic condensed matter observations.", "atomic_constraints": ["Constraint 1: Photon Efficiency - Must maximize information per photon in low-dose imaging to prevent sample damage.", "Constraint 2: Fabrication Limits - Modulation patterns must be binary/grayscale-realizable with existing spatial light modulator technology.", "Constraint 3: Reconstruction Latency - Decoding must operate at video rates for in situ dynamics observation.", "Constraint 4: Shift-Equivariance - Optical point spread functions exhibit translation symmetry requiring convolutional processing."], "distractors": [{"option": "A vision transformer processes raw sensor measurements using multi-head self-attention across spatial-frequency domains. Pre-training on ImageNet leverages transfer learning for generalized feature extraction, with fine-tuning on specific optical forward models.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 and 4: Transformers lack native shift-equivariance and have quadratic complexity incompatible with real-time requirements. Pre-training data distribution mismatch reduces photon efficiency."}, {"option": "Standard CNN reconstruction applied to measurements from pre-optimized random binary masks. The U-Net architecture with residual blocks performs iterative deconvolution, incorporating total variation regularization for noise suppression.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Fixed masks cause photon inefficiency. No co-optimization prevents adaptation to hardware constraints like binarization requirements during encoding design."}, {"option": "Adaptive foveated single-pixel imaging with dynamic supersampling allocates measurements based on spatial importance. Compressed sensing reconstruction using learned dictionaries recovers high-frequency details in regions of interest.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 and 4: Dictionary learning lacks shift-equivariance and has high computational latency. Adaptive sampling introduces synchronization overhead incompatible with real-time dynamics."}]}}
{"id": 277490233, "title": "Hybrid Optimization of Phase Masks: Integrating Non-Iterative Methods with Simulated Annealing and Validation via Tomographic Measurements", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Simulated Annealing"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Non-iterative phase mask methods lack precision and adaptability in complex holographic scenarios, requiring globally optimized solutions for high-fidelity light field symmetry.", "adaptation_ground_truth": "Hybrid optimization using non-iterative methods to generate initial phase masks, enhanced by simulated annealing with adaptive cooling strategies for symmetry-aware refinement.", "ground_truth_reasoning": "This approach satisfies atomic constraints: NI initialization accelerates convergence (Constraint 3), adaptive SA cooling preserves optical symmetries (Constraint 2), and MUB validation ensures tomographic precision (Constraint 1). The hybrid design balances computational efficiency with physical accuracy.", "atomic_constraints": ["Constraint 1: Tomographic Fidelity - Phase masks must achieve >0.99 similarity in mutually unbiased basis measurements for quantum-validated light fields.", "Constraint 2: Symmetry Preservation - Optimized masks must perfectly reproduce rotational/reflective symmetries in Laguerre-Gaussian optical modes.", "Constraint 3: Non-Iterative Bootstrapping - Computational resources require initial solutions from fast, non-iterative algorithms before refinement.", "Constraint 4: Cooling Sensitivity - Annealing parameters must dynamically adapt to avoid quenched defects in phase transition landscapes."], "distractors": [{"option": "Vision transformers pre-trained on optical mode datasets directly predict phase masks via attention mechanisms, leveraging transfer learning for holographic optimization tasks.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformers lack inherent symmetry preservation for Laguerre-Gaussian modes, causing rotational asymmetry in output light fields."}, {"option": "Standard simulated annealing with random initialization and fixed geometric cooling, iteratively minimizing intensity error via pixel-wise phase perturbations.", "label": "Naive Application", "analysis": "Violates Constraints 3 & 4: Random starts increase convergence time 5x versus NI bootstrapping; fixed cooling induces symmetry-breaking defects in phase transitions."}, {"option": "Surrogate-assisted grey wolf optimization using Gaussian process models to approximate optical responses, guiding swarm search for phase mask parameters.", "label": "Cluster Competitor", "analysis": "Violates Constraints 1 & 2: Surrogate inaccuracies in high-dimensional MUB space reduce tomographic fidelity; bio-inspired heuristics ignore structured symmetry constraints."}]}}
{"id": 277667067, "title": "Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Graph Neural Networks (GNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Standard GNNs fail to respect fundamental symmetries in turbulence physics, leading to poor generalization across reference frames and scales.", "adaptation_ground_truth": "We develop an SE(3)-equivariant GNN architecture using irreducible representations and steerable kernels. This ensures predictions transform consistently under 3D rotations/translations while preserving conservation laws through symmetry-constrained message passing.", "ground_truth_reasoning": "The method satisfies turbulence constraints by embedding SE(3)-equivariance directly into the architecture, ensuring predictions are invariant to reference frame rotations/translations. Conservation laws are enforced via symmetry-derived weight constraints, and scale interactions are preserved through equivariant aggregation across dynamically generated graphs.", "atomic_constraints": ["Constraint 1: SE(3) Equivariance - Predictions must transform covariantly with 3D rotations/translations to maintain consistency across reference frames.", "Constraint 2: Galilean Invariance - Closure terms must remain unchanged under constant-velocity boosts to satisfy fluid dynamics principles.", "Constraint 3: Local Momentum Conservation - Subgrid stress tensors must satisfy divergence theorems to prevent unphysical energy accumulation."], "distractors": [{"option": "A volumetric transformer processes turbulence data using 3D patch embeddings and multi-head attention. The model incorporates positional encodings and learns spatiotemporal correlations through stacked transformer blocks trained on high-resolution DNS datasets.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Transformers lack inherent SE(3) equivariance, requiring exhaustive data augmentation to approximate rotation/translation consistency. Attention mechanisms also struggle with local conservation laws."}, {"option": "A standard message-passing GNN predicts turbulence fields using edge features and node updates. The architecture includes residual connections and layer normalization, trained via supervised learning on particle trajectories from direct numerical simulations.", "label": "Naive Application", "analysis": "Violates Constraints 1-3: Without equivariance layers, predictions depend on arbitrary coordinate systems. Galilean invariance is broken by velocity-dependent features, and message aggregation lacks built-in conservation guarantees."}, {"option": "Semi-supervised graph convolutional networks classify turbulent structures using labeled/unlabeled nodes. The model combines Laplacian smoothing with a physics-informed loss function, leveraging sparse experimental measurements to regularize velocity field reconstructions.", "label": "Cluster Competitor", "analysis": "Violates Constraints 1-2: GCNs assume graph isomorphism instead of continuous symmetries. Classification objectives ignore conservation constraints, and frame-dependent convolutions compromise Galilean invariance."}]}}
{"id": 279243133, "title": "Transient dynamics of associative memory models", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Associative Memory Models (specifically Hopfield-like networks)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Understanding the time-dependent evolution of neural network states toward memory attractors in large-scale systems with asymmetric connections and continuous neuronal responses.", "adaptation_ground_truth": "A dynamical mean-field theory framework analyzing order parameter evolution for continuous neurons with asymmetric couplings, capturing transient state trajectories through self-consistent stochastic differential equations.", "ground_truth_reasoning": "This approach addresses non-equilibrium dynamics (Constraint 1) through time-dependent order parameters, scales to large systems (Constraint 2) via mean-field reduction, incorporates asymmetric interactions (Constraint 3) through non-Hermitian operators, and handles continuous states (Constraint 4) via graded response functions.", "atomic_constraints": ["Constraint 1: Non-equilibrium dynamics - Systems evolve through time-dependent pathways before reaching attractors, requiring trajectory analysis.", "Constraint 2: Large system size - Collective behavior emerges only in thermodynamic limit (N→∞), necessitating scalable analytical methods.", "Constraint 3: Asymmetric interactions - Biological synapses lack symmetry, breaking detailed balance and preventing equilibrium analysis.", "Constraint 4: Continuous state variables - Neurons exhibit graded membrane potentials rather than binary states, requiring continuous dynamics modeling."], "distractors": [{"option": "A transformer architecture processes neural activity sequences using multi-head attention layers. Positional encoding captures temporal dependencies, while fine-tuning on spike-train data optimizes memory retrieval accuracy through gradient descent.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 3: Transformers require discrete tokenization unsuitable for continuous dynamics, and their data hunger conflicts with theoretical large-system analysis of asymmetric couplings."}, {"option": "Symmetric Hopfield networks with binary neurons and Hebbian storage. Energy landscape minimization identifies fixed-point attractors, with storage capacity calculated via replica method for equilibrium states under synchronous updates.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Assumes symmetric weights and equilibrium analysis, ignoring transient trajectories and asymmetric connections fundamental to biological systems."}, {"option": "High-order correlation models with polynomial interactions encode memory patterns. Tensor decomposition extracts multi-neuron dependencies, and Monte Carlo sampling simulates pattern retrieval dynamics under Langevin equations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: High-order interactions become computationally intractable for large N, lacking mean-field scalability to thermodynamic limit required for collective phenomena."}]}}
{"id": 279291394, "title": "Memristor-Based Artificial Neural Networks for Hardware Neuromorphic Computing", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "Artificial Neural Networks (ANNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Implementing multi-layer ANNs on memristor hardware faces challenges due to non-ideal device characteristics that degrade weight precision and inference accuracy during in-situ training.", "adaptation_ground_truth": "Ex-situ training with offline weight optimization using conventional computers, followed by calibrated mapping to memristor crossbars. Post-programming conductance adjustment compensates for device-level variations while preserving endurance.", "ground_truth_reasoning": "This approach separates training from physical implementation, avoiding destructive in-situ updates constrained by memristor endurance limits. Calibration mitigates filamentary switching variability and conductance nonlinearity by measuring actual device responses after programming, enabling accurate analog weight representation without real-time tuning.", "atomic_constraints": ["Constraint 1: Filamentary Stochasticity - Nanoscale conductive filament formation/rupture exhibits cycle-to-cycle and device-to-device variability.", "Constraint 2: Endurance Limitation - Oxide-based memristors tolerate only 10^4-10^6 write cycles before irreversible degradation.", "Constraint 3: Non-linear Conductance Response - Analog weight updates show asymmetric and non-linear voltage-pulse dependence."], "distractors": [{"option": "Transformer-based in-situ training using memristor crossbars for attention weight storage. Self-attention layers receive direct conductance updates via backpropagation signals, enabling adaptive on-device learning for sequential tasks.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Frequent attention weight updates during training exceed memristor endurance limits. Violates Constraint 1: Stochastic filament changes corrupt high-precision attention matrices."}, {"option": "Standard backpropagation implemented on memristor crossbars with pulse-based gradient descent. Synaptic weights receive identical voltage pulses proportional to error gradients during end-to-end in-situ training cycles.", "label": "Naive Application", "analysis": "Ignores Constraint 3: Identical pulses cause divergent conductance states due to nonlinear switching. Ignores Constraint 1: Uncompensated variability accumulates errors across layers."}, {"option": "Reservoir computing with fixed random memristor reservoir and trained readout layer. Unsupervised feature extraction leverages intrinsic device dynamics while avoiding weight programming through passive signal propagation.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Fixed reservoir performance degrades with filament variability over cycles. Ignores multi-layer requirement: Single readout layer limits complex pattern recognition achievable with deep ANNs."}]}}
{"id": 275531678, "title": "An Airborne Gravity Gradient Compensation Method Based on Convolutional and Long Short-Term Memory Neural Networks", "taxonomy": {"domain": "Physical Sciences", "sub": "Condensed matter physics", "method": "CNN-LSTM"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate airborne gravity gradient measurement requires compensating for aircraft-induced noise that obscures weak geophysical signals during dynamic motion.", "adaptation_ground_truth": "A hybrid CNN-LSTM network where convolutional layers extract spatial gravity tensor patterns and LSTM layers model temporal aircraft dynamics, enabling joint compensation of motion artifacts.", "ground_truth_reasoning": "The CNN captures local spatial correlations in gravity tensor components while LSTM processes sequential flight trajectory data. This fusion addresses both spatial continuity and time-dependent noise constraints inherent to moving sensor platforms.", "atomic_constraints": ["Constraint 1: Temporal Dynamics - Aircraft motion creates time-varying noise that must be modeled as sequential dependencies.", "Constraint 2: Spatial Continuity - Gravity tensors exhibit strong local correlations requiring neighborhood feature extraction.", "Constraint 3: Weak Signal Sensitivity - Nanoscale gravity variations demand noise suppression without signal degradation."], "distractors": [{"option": "A Transformer model processes gravity time-series with self-attention mechanisms, capturing long-range dependencies in sensor trajectories. Positional encoding tracks aircraft movement for noise compensation across extended survey paths.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 by inadequately capturing local spatial correlations without convolutional inductive bias, reducing precision for tensor relationships."}, {"option": "A standard CNN architecture processes gravity tensor grids through convolutional blocks for spatial feature extraction. Training uses paired clean/noisy data with residual connections to predict compensation values across survey areas.", "label": "Naive Application", "analysis": "Violates Constraint 1 by ignoring temporal sequencing of aircraft motion, leaving time-variant noise uncompensated."}, {"option": "A ResNet-based denoising architecture with skip connections processes gravity grids. Residual blocks learn noise mappings from training data, progressively refining output through convolutional layers for spatial artifact removal.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 due to absence of recurrent components for temporal modeling, making it ineffective for motion-dependent disturbances."}]}}
