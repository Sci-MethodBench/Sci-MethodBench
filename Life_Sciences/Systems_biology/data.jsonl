{"id": 277940653, "title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Single-cell RNA sequencing generates ultra-high-dimensional, sparse data with complex biological noise, requiring scalable models to capture gene-gene interactions across millions of cells while preserving cell-type specificity.", "adaptation_ground_truth": "A domain-specific transformer architecture with gene-aware sparse attention mechanisms, pre-trained on large-scale single-cell datasets using masked gene modeling, then fine-tuned for downstream tasks like cell-type annotation.", "ground_truth_reasoning": "The sparse attention reduces computational complexity for high-dimensional gene expression vectors while capturing long-range dependencies. Pre-training leverages unlabeled data to handle biological noise, and gene-aware mechanisms incorporate prior biological knowledge about gene interactions.", "atomic_constraints": ["Constraint 1: Ultra-high dimensionality - scRNA-seq measures 20,000+ genes per cell, demanding efficient feature compression.", "Constraint 2: Extreme data sparsity - >90% zero values in expression matrices require specialized handling of null signals.", "Constraint 3: Biological heterogeneity - Models must resolve subtle transcriptional differences between cell states amid technical noise.", "Constraint 4: Scalability requirement - Analysis must process datasets exceeding 1 million cells with feasible compute resources."], "distractors": [{"option": "Implementing a BERT model pre-trained on PubMed abstracts, then fine-tuning its token embeddings on gene expression vectors for cell classification using standard dense attention layers.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Text-optimized BERT cannot handle extreme sparsity patterns in scRNA-seq, wasting capacity on zero-inflated inputs. Violates Constraint 1: Dense attention scales quadratically with gene dimensions."}, {"option": "Direct application of vanilla transformer architecture treating genes as tokens, with positional encoding based on genomic coordinates and standard cross-entropy loss for cell-type prediction tasks.", "label": "Naive Application", "analysis": "Violates Constraint 3: Lacks biological noise mitigation and gene relationship priors. Violates Constraint 4: Full attention on 20k genes causes O(n²) memory explosion at million-cell scale."}, {"option": "A question-entailment framework where gene expression profiles are converted into natural language queries against a knowledge base of curated cell-type descriptions to infer annotations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Loses high-dimensional signal through text conversion. Violates Constraint 3: Fails to model continuous gene interactions. Violates Constraint 4: Knowledge base curation doesn't scale to novel cell states."}]}}
{"id": 277545899, "title": "SpaMask: Dual masking graph autoencoder with contrastive learning for spatial transcriptomics", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Graph Neural Networks (GNNs) with Contrastive Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "High sparsity in spatial transcriptomics data causes instability in graph-based models, hindering accurate spatial domain characterization and batch integration.", "adaptation_ground_truth": "SpaMask introduces dual masking: node masking in graph autoencoders leverages spatial neighbors for robust feature reconstruction, while edge masking in contrastive learning tightens embeddings of adjacent spots based on spatial proximity and feature similarity.", "ground_truth_reasoning": "Node masking addresses data sparsity by forcing reliance on spatial neighbors for reconstruction, satisfying locality constraints. Edge masking enforces feature similarity between proximate spots through contrastive loss. Dual masking jointly improves clustering accuracy and batch correction by handling structural instability.", "atomic_constraints": ["Constraint 1: Spatial Locality - Gene expression patterns depend on immediate cellular neighborhoods due to diffusion limits and signaling gradients.", "Constraint 2: Feature Sparsity - Technical limitations cause over 90% zero values in transcript counts, creating unstable graph topologies.", "Constraint 3: Neighborhood Homophily - Adjacent spots sharing similar expression profiles indicate functional tissue domains requiring cohesive embeddings.", "Constraint 4: Platform Variance - Multi-batch data exhibits systematic technical noise from diverse sequencing platforms."], "distractors": [{"option": "A Graph Transformer reconstructs masked node features using global self-attention over all spots. Positional encodings incorporate spatial coordinates, and a cross-entropy loss trains the model to predict gene expression patterns from partial inputs.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 by relying on dense global attention, amplifying noise from sparse inputs. Ignores Constraint 1 through isotropic attention weights that dilute local spatial dependencies."}, {"option": "A standard graph autoencoder encodes spot features and adjacency matrices into latent embeddings. The decoder reconstructs original gene expressions via multilayer perceptrons, optimized with mean squared error loss without masking mechanisms.", "label": "Naive Application", "analysis": "Violates Constraint 2 by directly reconstructing sparse inputs, propagating instability. Fails Constraint 3 by lacking explicit mechanisms to enforce neighborhood similarity in embeddings."}, {"option": "Deep Graph Infomax maximizes mutual information between patch-level and global summaries. Local spot embeddings are contrasted against corrupted graphs through discriminator networks, preserving high-level spatial relationships without masking.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 by using global summaries that obscure micro-scale locality. Ignores Constraint 2 as corruption strategies may not simulate sparsity-driven instability in graph topology."}]}}
{"id": 276317838, "title": "Inverse problems with experiment-guided AlphaFold", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "AlphaFold predicts static protein structures, but biological function requires dynamic ensembles. Experimental data (e.g., NMR) capture ensemble-averaged measurements that single structures cannot satisfy.", "adaptation_ground_truth": "Integrate NMR residual dipolar couplings (RDCs) as physical constraints during AlphaFold's transformer-based structure refinement. This guides conformational sampling to generate ensembles matching experimental observables.", "ground_truth_reasoning": "The transformer architecture incorporates RDCs as auxiliary inputs during attention-based refinement. This satisfies ensemble dynamics by biasing sampling toward conformations consistent with NMR data while maintaining SE(3)-equivariant transformations essential for structural integrity.", "atomic_constraints": ["Constraint 1: Ensemble Dynamics - Proteins exist as conformational ensembles in solution, requiring multi-state representations.", "Constraint 2: Sparse Experimental Data - NMR RDCs provide limited (<0.1% atomic coverage) but high-precision ensemble-averaged measurements.", "Constraint 3: SE(3) Equivariance - Structural predictions must respect rotational/translational invariance of molecular energy landscapes.", "Constraint 4: Data Consistency - Generated ensembles must satisfy experimental measurements within quantum mechanical error bounds."], "distractors": [{"option": "Apply denoising diffusion probabilistic models (DDPMs) trained on PDB structures to generate diverse conformations. Experimental data is incorporated via post-sampling filtering based on maximum likelihood estimation.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 4: DDPMs require dense data coverage and lack built-in physics constraints, causing statistical deviations from NMR measurements due to insufficient conditioning on sparse experimental inputs."}, {"option": "Use standard AlphaFold without modifications to predict a single structure. Perform standard gradient descent with PyTorch and Adam optimizer for energy minimization, applying gradient clipping for stability.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Single-point prediction ignores ensemble dynamics, and unconstrained optimization violates SE(3) equivariance by converging to arbitrary local minima inconsistent with solution-state conformations."}, {"option": "Employ molecular dynamics simulations with AMBER force fields, integrating RDCs as harmonic restraints. Use NMR-derived atomic motions to initialize simulations over microsecond timescales.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 and 4: Force-field inaccuracies and sampling limitations produce conformational drift exceeding RDC error tolerances, with computational costs scaling prohibitively for ensemble generation."}]}}
{"id": 274117280, "title": "GPU-accelerated homology search with MMseqs2", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "GPU Acceleration (Parallel Computing)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Exponential growth of protein sequence databases necessitates ultra-fast homology searches to handle billions of comparisons efficiently.", "adaptation_ground_truth": "MMseqs2 is restructured to offload parallelizable alignment computations to GPUs, leveraging CUDA cores for batched processing of sequence comparisons. This exploits fine-grained parallelism while optimizing memory transfers between host and device.", "ground_truth_reasoning": "GPU acceleration directly addresses atomic constraints: massive parallelism handles independent comparisons at scale, high memory bandwidth supports rapid data access for billions of sequences, and energy-efficient processing reduces computational costs for large databases.", "atomic_constraints": ["Massive Parallelism Demand: Billions of independent sequence comparisons require concurrent processing capabilities.", "High Memory Throughput: Rapid access to vast sequence databases necessitates exceptional data transfer bandwidth.", "Energy-Efficient Scaling: Computational costs must remain feasible despite exponential database growth."], "distractors": [{"option": "Employing a transformer-based protein language model to generate sequence embeddings, followed by approximate nearest-neighbor search in latent space. This leverages pre-trained representations for functional similarity detection across large datasets.", "label": "SOTA Bias", "analysis": "Violates High Memory Throughput: Embedding storage and similarity calculations for billion-scale databases exceed GPU memory limits, causing frequent data swapping and latency."}, {"option": "Optimizing MMseqs2 for multi-core CPUs using AVX-512 vectorization and thread parallelism. Database chunks are processed via shared-memory architectures with efficient cache utilization for alignment calculations.", "label": "Naive Application", "analysis": "Violates Massive Parallelism Demand: Limited CPU cores cannot concurrently process billions of comparisons, creating bottlenecks for large-scale homology searches."}, {"option": "Implementing inter-sequence SIMD parallelization for Smith-Waterman alignments on CPUs. This aligns multiple database sequences simultaneously with one query using vectorized instructions, enhancing throughput for pairwise comparisons.", "label": "Cluster Competitor", "analysis": "Violates Energy-Efficient Scaling: CPU-based SIMD lacks the parallel compute density of GPUs, increasing power consumption and runtime for terabyte-scale databases."}]}}
{"id": 276213794, "title": "Are protein language models the new universal key?", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting protein function and interactions from sequence data is challenged by evolutionary constraints, sparse structural annotations, and non-local residue dependencies.", "adaptation_ground_truth": "A Transformer-based protein language model pre-trained on evolutionary sequence alignments captures residue co-dependencies through self-attention. Contextual embeddings are fine-tuned for downstream tasks like mutation effect prediction, leveraging unsupervised learning from massive unlabeled protein databases.", "ground_truth_reasoning": "The MSA Transformer architecture processes multiple sequence alignments as input, using column-wise attention to model evolutionary constraints. Self-attention handles long-range residue interactions critical for folding, while pre-training overcomes data sparsity by learning from unannotated sequences. Embeddings implicitly encode structural regularities through co-evolution patterns.", "atomic_constraints": ["Constraint 1: Evolutionary conservation - Protein functions depend on conserved residues requiring models to capture phylogenetic relationships.", "Constraint 2: Long-range dependencies - Non-local residue interactions in 3D structures necessitate modeling sequence positions beyond local windows.", "Constraint 3: Data sparsity - Experimentally validated structural/functional annotations are scarce, demanding unsupervised learning from sequence databases.", "Constraint 4: Co-evolutionary signals - Functionally critical residue pairs evolve correlatively, requiring models to detect inter-residue dependencies."], "distractors": [{"option": "A vision transformer processes protein contact maps as 2D images, using patch embeddings to detect structural motifs. Hierarchical attention layers extract global features for function prediction, leveraging transfer learning from image classification benchmarks.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by ignoring evolutionary sequence context and Constraint 4 due to inability to model co-evolution without alignment inputs."}, {"option": "A standard Transformer encodes individual protein sequences as token embeddings. Positional encoding captures residue order, while multi-head attention models pairwise dependencies. Supervised training uses annotated datasets for specific prediction tasks.", "label": "Naive Application", "analysis": "Inadequate for Constraint 3 (requires large labeled data) and Constraint 1 (lacks evolutionary context from MSAs), reducing generalization to rare proteins."}, {"option": "A Markov model estimates residue transition probabilities from aligned protein families. Emission states represent conserved positions, while Viterbi decoding predicts functional sites. Bayesian inference incorporates prior structural knowledge.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 (models only local dependencies) and Constraint 4 (fails to capture non-linear co-evolution patterns), limiting accuracy for complex interactions."}]}}
{"id": 275605147, "title": "Challenges and opportunities for digital twins in precision medicine from a complex systems perspective", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Graph Neural Networks (GNNs) / Hypergraph Neural Networks"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Modeling multiscale biological networks for precision medicine digital twins requires capturing dynamic, heterogeneous interactions across molecular to organismal scales while integrating mechanistic hypotheses.", "adaptation_ground_truth": "Hypergraph Neural Networks incorporating explicit biological mechanisms and multiscale simulations, enabling dynamic exploration of therapeutic strategies through higher-order network representations.", "ground_truth_reasoning": "Hypergraphs natively model multiway biological interactions (e.g., protein complexes), while embedded mechanistic simulations enforce domain hypotheses. Multiscale architecture integrates molecular-cellular-tissue dynamics, satisfying biological hierarchy and noise robustness needs.", "atomic_constraints": ["Constraint 1: Multiscale Integration - Must bridge molecular, cellular, and tissue-level dynamics without scale-separation assumptions.", "Constraint 2: Higher-Order Interactions - Requires modeling n-ary biological relationships (e.g., protein complexes) beyond pairwise connections.", "Constraint 3: Hypothesis-Driven Mechanisms - Simulations must enforce causal biological principles rather than purely data-driven patterns.", "Constraint 4: Dynamic Adaptation - Systems must continuously update states from sparse/noisy clinical data streams."], "distractors": [{"option": "Transformer-based foundation models pre-trained on biomedical literature, fine-tuned with protein interaction data. Self-attention layers capture global network dependencies, while transfer learning leverages semantic knowledge from diverse biological contexts.", "label": "SOTA Bias", "analysis": "Violates Constraint 3: Lacks explicit mechanistic simulations, relying on statistical correlations without enforcing causal biological hypotheses. Also violates Constraint 4 due to static architecture unsuited for continuous clinical data streams."}, {"option": "Standard Graph Neural Networks with message-passing layers applied to protein-protein interaction networks. Node features include expression levels, with graph convolutions aggregating neighbor information for phenotype prediction tasks.", "label": "Naive Application", "analysis": "Violates Constraint 2: Cannot represent higher-order interactions (e.g., metabolic complexes) due to pairwise edge limitations. Also violates Constraint 1 by operating at a single biological scale without multiscale integration."}, {"option": "Agent-based modeling of cellular networks with stochastic differential equations. Agents represent cells with rule-based behaviors, simulating emergent tissue dynamics through local interaction rules calibrated to omics data.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Lacks efficient parameterization for continuous clinical updates. Also violates Constraint 2 by inadequately capturing system-wide higher-order dependencies compared to hypergraph representations."}]}}
{"id": 278824639, "title": "Revolutionizing Drug Discovery: Integrating Spatial Transcriptomics with Advanced Computer Vision Techniques", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Computer Vision"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Extracting actionable insights from noisy, high-dimensional spatial transcriptomics data to identify disease biomarkers and therapeutic targets.", "adaptation_ground_truth": "Integration of deep learning-based segmentation with graph neural networks (GNNs) to model spatial relationships in tissue data, enhancing biomarker identification and interpretability.", "ground_truth_reasoning": "Segmentation captures cellular structures from spatial transcriptomics images, while GNNs explicitly encode tissue topology and cell interactions. This dual approach respects spatial dependencies, handles noise through hierarchical feature learning, and provides interpretable graph-based representations for biological insights.", "atomic_constraints": ["Constraint 1: Spatial Topology Preservation - Gene expression patterns depend on precise 2D/3D tissue architecture, requiring methods that encode neighborhood relationships.", "Constraint 2: Multi-Scale Heterogeneity - Data exhibits variability across cellular, subcellular, and tissue scales, demanding hierarchical feature extraction.", "Constraint 3: Interpretable Biomarker Mapping - Drug discovery necessitates traceable feature attribution to specific tissue regions and genes."], "distractors": [{"option": "A vision transformer processes spatial transcriptomics as image patches, using self-attention to model global dependencies. Layer normalization and multi-head attention capture gene expression correlations across tissue sections for target identification.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Transformers ignore local spatial hierarchies critical for tissue topology, treating patches as unordered tokens. Lacks inductive biases for neighborhood relationships, reducing biological plausibility."}, {"option": "A standard U-Net architecture segments tissue structures using convolutional layers. Skip connections merge encoder-decoder features, and data augmentation handles noise. Output masks classify disease regions for biomarker extraction.", "label": "Naive Application", "analysis": "Violates Constraint 3: Pure CNNs provide pixel-level segmentation but fail to explicitly model cell-cell interactions as graphs. Lacks relational reasoning for interpretable biomarker mapping across discontinuous tissue regions."}, {"option": "Contrastive learning (SimCLR framework) pre-trains embeddings from augmented spatial transcriptomics views. A linear probe then identifies biomarkers by maximizing agreement between positive pairs of tissue subregions.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: Instance discrimination in contrastive learning homogenizes multi-scale heterogeneity. Augmentations may disrupt spatial continuity, failing to preserve hierarchical tissue structures essential for biomarker discovery."}]}}
{"id": 277996196, "title": "Custom CRISPR–Cas9 PAM variants via scalable engineering and machine learning", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Supervised Machine Learning (with Feature Engineering & Interpretability)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Expanding CRISPR-Cas9 targeting range by engineering variants that recognize diverse non-canonical PAM sequences beyond natural constraints.", "adaptation_ground_truth": "Supervised ML with engineered biophysical features (e.g., residue charge, evolutionary conservation) predicts Cas9-PAM activity. SHAP interpretability identifies critical mutations, enabling rational design of functional variants with expanded targeting.", "ground_truth_reasoning": "The method addresses combinatorial explosion through feature-based dimensionality reduction, overcomes data sparsity via engineered structural proxies, captures epistasis via non-linear ML, and provides biological interpretability for actionable designs.", "atomic_constraints": ["Constraint 1: Combinatorial Explosion - Vast mutation-PAM combinations preclude exhaustive experimental screening.", "Constraint 2: Data Sparsity - Limited functional measurements for mutant-PAM pairs.", "Constraint 3: Epistatic Interactions - Non-additive effects of multiple mutations on PAM recognition.", "Constraint 4: Interpretability Demand - Requires residue-level insights for biological engineering."], "distractors": [{"option": "Using a protein language transformer (e.g., ESM-2) fine-tuned on Cas9 sequences to generate novel variants. The model leverages unsupervised pre-training on evolutionary data, followed by supervised adjustment for PAM recognition tasks.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformers require massive datasets but experimental PAM activity data is sparse, leading to poor generalization on unseen mutations."}, {"option": "Standard random forest regression using one-hot encoded protein sequences and PAM motifs. Hyperparameters are optimized via grid search, and top features are ranked by Gini importance for variant screening.", "label": "Naive Application", "analysis": "Violates Constraint 4: Raw sequence encoding lacks biophysical context; Gini importance fails to capture directional epistasis needed for engineering."}, {"option": "LSTM-based generative model trained on phage display libraries to design Cas9 variants. Sequences are optimized for structural stability using coevolutionary couplings from EVcouplings, followed by PAM activity assays.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Generative LSTMs prioritize sequence likelihood over functional PAM interactions, ignoring mutation epistasis critical for DNA recognition."}]}}
{"id": 269773431, "title": "Gauge fixing for sequence-function relationships", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformers"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Non-unique sequence-function mappings in biological systems due to gauge freedom, complicating predictive modeling from sparse experimental data.", "adaptation_ground_truth": "Transformer architecture with gauge-fixing layers that enforce reference-sequence invariance, enabling consistent functional predictions across equivalent sequence representations.", "ground_truth_reasoning": "Gauge fixing resolves representation ambiguity by anchoring predictions to a biological reference frame. This satisfies gauge invariance constraints while leveraging transformers' attention mechanisms to handle epistatic interactions and sparse data via contextual sequence modeling.", "atomic_constraints": ["Constraint 1: Gauge Invariance - Sequence-function mappings exhibit non-uniqueness; equivalent sequences must yield identical functional outputs regardless of representation.", "Constraint 2: Data Sparsity - Experimental variant-effect measurements cover <0.01% of sequence space, demanding sample-efficient generalization.", "Constraint 3: Epistatic Coupling - Non-additive interactions between distant residues require modeling long-range dependencies in sequences."], "distractors": [{"option": "Fine-tuning a large pre-trained genomic transformer (e.g., DNABert) on variant-effect data, utilizing its full parameter set and transfer learning capabilities for functional prediction.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Pre-trained models lack gauge-fixing mechanisms, causing inconsistent predictions across equivalent sequence representations due to reference dependence."}, {"option": "Standard transformer with positional encodings and multi-head attention trained directly on one-hot encoded sequences, optimizing cross-entropy loss via gradient descent.", "label": "Naive Application", "analysis": "Violates Constraint 1: Without gauge fixing, predictions fluctuate with arbitrary reference choices, and Constraint 2: Vanilla transformers overfit sparse data due to excessive parameters."}, {"option": "Convolutional neural network with motif-scanning filters and max-pooling layers to detect local sequence features, followed by fully connected layers for regression.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: CNNs capture local dependencies but ignore long-range epistasis; Constraint 1: No mechanism to enforce gauge invariance across sequence representations."}]}}
{"id": 273324021, "title": "Exploring Log-Likelihood Scores for Ranking Antibody Sequence Designs", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Diffusion Models"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Ranking antibody sequence designs requires evaluating structural viability, functional diversity, and naturalness to ensure antigen-binding capability and manufacturability.", "adaptation_ground_truth": "Using diffusion models to generate antibody sequences and ranking them via log-likelihood scores, which implicitly encode structural and functional constraints learned from natural sequences.", "ground_truth_reasoning": "Diffusion models capture the joint distribution of sequence-structure relationships in antibodies. Log-likelihood scores provide a computationally efficient proxy for structural stability and functional fitness, leveraging natural sequence data without explicit structural simulations.", "atomic_constraints": ["Constraint 1: Structural Viability - Antibody sequences must fold into stable 3D conformations with functional complementarity-determining regions (CDRs).", "Constraint 2: Functional Diversity - CDR3 regions require high sequence variability to recognize diverse antigens while maintaining structural integrity.", "Constraint 3: Naturalness Constraint - Designed sequences must resemble natural antibodies to ensure proper folding and low immunogenicity."], "distractors": [{"option": "Leveraging a large protein language model (e.g., BERT) pre-trained on antibody sequences to generate CDR variants and rank them by perplexity scores.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Language models lack explicit structural modeling, generating sequences with plausible local patterns but unstable global folds."}, {"option": "Applying standard diffusion models for antibody sequence generation, then ranking designs using Rosetta energy minimization scores for structural stability assessment.", "label": "Naive Application", "analysis": "Violates Constraint 3: Reliance on Rosetta introduces computational bottlenecks and overlooks natural sequence preferences encoded in biological data."}, {"option": "Using inverse folding models (e.g., AlphaFold-based) to design sequences for fixed antibody backbones, ranking by structural recovery metrics.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: Inverse folding restricts CDR3 diversity by optimizing for fixed scaffolds, limiting antigen-binding variability."}]}}
{"id": 277449820, "title": "N6-methyladenine identification using deep learning and discriminative feature integration", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Deep Learning with Discriminative Feature Integration"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate identification of N6-methyladenine (6mA) epigenetic modifications in DNA, which regulate gene expression but are challenging to detect due to subtle genomic signatures and data limitations.", "adaptation_ground_truth": "An attention-enhanced deep neural network integrating nucleotide sequence features, structural properties, and physicochemical indices. The model dynamically weights discriminative features through learned attention mechanisms to optimize 6mA site prediction.", "ground_truth_reasoning": "The multi-feature integration addresses DNA's heterogeneous biochemical properties, while attention mechanisms prioritize context-dependent signals. This handles sparse positive samples through feature-informed regularization and captures local sequence dependencies via convolutional filters adapted for genomic data.", "atomic_constraints": ["Constraint 1: Sequence Context Dependency - Methylation likelihood depends on flanking nucleotide patterns requiring models to capture position-specific motifs.", "Constraint 2: Feature Heterogeneity - Accurate prediction necessitates integrating complementary data types (sequence/structure/physicochemistry) with varying discriminative power.", "Constraint 3: Sparse Positive Samples - Extremely low 6mA occurrence (~0.1% of adenines) demands robustness to class imbalance."], "distractors": [{"option": "A genomic foundation model pre-trained on billion-scale nucleotide datasets using masked language modeling. The transformer architecture processes DNA sequences through self-attention layers, with fine-tuning on 6mA labels for epigenetic site prediction.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 3: Transformers require massive labeled data unavailable for 6mA, and pure sequence modeling ignores structural/physicochemical features. Self-attention dilutes sparse signal among irrelevant nucleotides."}, {"option": "A standard convolutional neural network using one-hot encoded DNA sequences. The architecture includes three convolutional layers for motif extraction, max-pooling operations, and dense layers with dropout regularization for binary classification of methylation sites.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Relies solely on sequence data without structural/physicochemical integration. Fixed convolutional kernels cannot adaptively weight context-dependent features critical for sparse 6mA signals."}, {"option": "A graph convolutional network representing DNA nucleotides as nodes with chemical property embeddings. Edges connect spatially proximal bases, and message-passing layers aggregate neighborhood features for 6mA classification using multi-source information integration.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Graph convolutions prioritize spatial proximity over sequential context, disrupting position-dependent motif recognition. Chemical embeddings lack nucleotide-specific resolution for methylation biochemistry."}]}}
{"id": 280967453, "title": "Critical Assessment of Protein Intrinsic Disorder Round 3 - Predicting Disorder in the Era of Protein Language Models.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Protein Language Models (Transformer-based)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting protein intrinsic disorder is challenged by its continuum nature—ranging from fully disordered to context-dependent ordered states—and the absence of comprehensive experimental ground truth due to conditional structural transitions.", "adaptation_ground_truth": "Top methods leverage embeddings from protein language models (pLMs) pretrained on evolutionary protein sequence data, capturing contextual patterns for disorder prediction without relying on explicit structural templates.", "ground_truth_reasoning": "pLMs address constraints by learning co-evolutionary patterns across 250+ million sequences, implicitly modeling disorder continuum and environmental dependencies. This avoids reliance on sparse experimental data while enabling scalable prediction of ambiguous regions.", "atomic_constraints": ["Constraint 1: Disorder Continuum - Intrinsic disorder spans a spectrum rather than binary states, requiring probabilistic modeling.", "Constraint 2: Context-Dependent Transitions - Structural order/disorder shifts under specific conditions (e.g., binding), demanding environment-aware predictions.", "Constraint 3: Sparse Experimental Ground Truth - Manually curated annotations (e.g., DisProt) cover limited proteins/conditions, necessitating extrapolation beyond direct evidence.", "Constraint 4: Binding Region Complexity - Identifying Molecular Recognition Features (MoRFs) within disordered regions requires resolving transient structure-formation signals."], "distractors": [{"option": "Apply a general protein structure prediction transformer (e.g., AlphaFold 2) fine-tuned on DisProt annotations. The model predicts disorder from predicted structural confidence scores and residue flexibility metrics.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: AlphaFold assumes structural determinism and cannot model context-dependent disorder transitions, as it is optimized for stable folds, not conditional disorder."}, {"option": "Use a convolutional neural network (CNN) with handcrafted features like amino acid composition and physicochemical properties. Train on DisProt using standard cross-validation and optimize hyperparameters via grid search.", "label": "Naive Application", "analysis": "Violates Constraint 1: CNNs with engineered features cannot capture the disorder continuum or evolutionary context, reducing sensitivity to ambiguous states between order and disorder."}, {"option": "Employ OpenMM 7 for all-atom molecular dynamics simulations across diverse cellular conditions. Predict disorder by quantifying residue RMSF fluctuations in simulated ensembles over microsecond trajectories.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: MD simulations require atomic-level parameters unavailable for most proteins in DisProt, making them infeasible for proteome-scale predictions due to computational costs and force field limitations."}]}}
{"id": 274610971, "title": "SubCell: Proteome-aware vision foundation models for microscopy capture single-cell biology", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Attention-based Deep Multiple Instance Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting cell states from microscopy images requires integrating heterogeneous subcellular features while handling weak, cell-level labels without patch annotations.", "adaptation_ground_truth": "Attention-based deep MIL aggregates proteome-aware features from image patches. Vision transformers process patches as instances, with attention weights highlighting biologically relevant regions for cell-level predictions.", "ground_truth_reasoning": "The attention mechanism dynamically weights patches by biological significance, accommodating spatial heterogeneity and weak supervision. It integrates local features into global proteome context while filtering noisy/irrelevant regions through instance-aware learning.", "atomic_constraints": ["Constraint 1: Spatial Heterogeneity - Molecular distributions vary across subcellular regions, demanding adaptive feature weighting.", "Constraint 2: Instance Ambiguity - Only sparse patches contain phenotype-relevant signals amidst cellular noise.", "Constraint 3: Weak Supervision - Annotations exist only at cell level, not for individual subcellular components.", "Constraint 4: Multi-scale Integration - Models must resolve local (organelle) and global (cell) biological interactions."], "distractors": [{"option": "A Vision Transformer (ViT) processes entire microscopy images via standard 16x16 patch encoding. Global pooled features from ImageNet-pretrained layers predict cell states using cross-entropy loss.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 by treating all patches equally, ignoring sparse informative regions. Lacks instance weighting for ambiguous signals, reducing sensitivity to rare subcellular events."}, {"option": "Deep MIL with max-pooling: ResNet-50 extracts patch features from segmented cells. The highest-activation patch determines cell labels via fully connected layers and standard backpropagation.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 4. Max-pooling oversimplifies spatial heterogeneity by ignoring multi-region contributions, failing to integrate distributed proteome context."}, {"option": "Self-supervised contrastive learning (BYOL) pretrains on unlabeled images. Augmented views train a Siamese network, then global average-pooled features classify cell states with logistic regression.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3. Whole-image contrastive learning disregards weak supervision needs, losing patch-to-cell relationships critical for morphological profiling."}]}}
{"id": 275406062, "title": "Simple controls exceed best deep learning algorithms and reveal foundation model effectiveness for predicting genetic perturbations", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting gene expression changes from genetic perturbations in single-cell RNA-seq data, which involves high-dimensional sparse outputs, nonlinear biological interactions, and limited labeled data.", "adaptation_ground_truth": "A pre-trained transformer foundation model fine-tuned on perturbation data, leveraging self-supervised learning on large-scale unlabeled scRNA-seq datasets to capture gene-gene interactions and biological context.", "ground_truth_reasoning": "The pre-training addresses data sparsity by learning latent biological representations from abundant unlabeled data. The transformer architecture captures long-range gene dependencies, while fine-tuning adapts to perturbation-specific responses with limited labeled examples.", "atomic_constraints": ["Constraint 1: High-Dimensional Sparsity - Gene expression data has thousands of dimensions with zero-inflated distributions, requiring efficient representation learning.", "Constraint 2: Nonlinear Biological Interactions - Perturbation effects propagate through complex regulatory networks demanding models capturing higher-order dependencies.", "Constraint 3: Limited Labeled Data - Sparse perturbation labels necessitate leveraging unlabeled data via self-supervised paradigms.", "Constraint 4: Contextual Variability - Cell-type-specific responses require models that condition predictions on transcriptional context."], "distractors": [{"option": "A vision transformer (ViT) adapted for gene expression by treating genes as image patches. Pre-trained on ImageNet, then fine-tuned with genetic perturbation data using cross-attention layers to integrate sequence features.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 4: ViT's grid-based processing ignores gene interaction topology and biological sparsity patterns. ImageNet pre-training introduces domain-irrelevant features, worsening data efficiency."}, {"option": "A standard transformer trained exclusively on perturbation labels with positional encodings for gene order. Includes 12 layers, 8 attention heads, and dropout regularization, optimized via AdamW for expression prediction.", "label": "Naive Application", "analysis": "Violates Constraint 3: Lacks pre-training, causing overfitting due to limited perturbation labels. Fails to leverage unlabeled data for biological context, reducing generalization to rare cell types."}, {"option": "Graph neural networks with protein-protein interaction priors. Nodes represent genes, edges denote known biological interactions. Trained via contrastive learning on unlabeled data, then fine-tuned for perturbation response.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: Relies on incomplete interaction graphs, missing long-range regulatory dependencies. Struggles with unseen gene relationships beyond predefined edges, unlike transformers' global attention."}]}}
{"id": 277917889, "title": "Genomic language models could transform medicine but not yet", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Standard language models struggle with genomic sequences due to biological constraints like long-range dependencies and sparse functional signals, limiting medical applicability.", "adaptation_ground_truth": "A bidirectional Transformer pre-trained on genomic sequences using k-mer tokenization and masked language modeling objectives to capture nucleotide context and biological patterns.", "ground_truth_reasoning": "This adaptation addresses genomic constraints by using k-mer tokenization to preserve sequence motifs, bidirectional attention for regulatory element context, and pre-training on massive DNA datasets to overcome functional signal sparsity through transfer learning.", "atomic_constraints": ["Long-range dependencies - Regulatory elements influence genes across thousands of base pairs, requiring sequence-wide context modeling.", "Sparse functional signals - Only ~2% of human DNA encodes functional elements, demanding high sensitivity to rare patterns.", "Bidirectional context - Nucleotide function depends on both upstream and downstream sequences (e.g., promoter-enhancer interactions).", "Discrete symbolic grammar - DNA follows combinatorial rules (e.g., codon usage, splice sites) distinct from natural language."], "distractors": [{"option": "A GPT-style autoregressive Transformer generates genomic sequences by predicting next nucleotides. Trained on chromosome-scale data, it captures sequential patterns for gene synthesis applications.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 (bidirectional context) due to unidirectional attention, missing regulatory relationships. Also struggles with Constraint 2 (sparse signals) without explicit masking objectives."}, {"option": "Standard BERT architecture applied directly to nucleotide sequences using single-character tokens. Fine-tuned on target genomic tasks with standard positional embeddings and classification layers.", "label": "Naive Application", "analysis": "Violates Constraint 1 (long-range dependencies) with limited context window, and Constraint 4 (discrete grammar) by ignoring k-mer semantics. Single-character tokens lose motif information."}, {"option": "Evolutionary algorithm-based sequence design (Evo) optimized for functional genomics. Uses fitness landscapes and mutation operators to generate novel DNA structures with desired biological properties.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 (long-range dependencies) by lacking attention mechanisms. Struggles with Constraint 2 (sparse signals) due to reliance on iterative optimization rather than contextual learning."}]}}
{"id": 276408054, "title": "HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer-Mamba2 Hybrid"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurately modeling enhancer-promoter interactions in DNA requires capturing ultra-long-range dependencies (up to 1 million base pairs) while resolving fine-grained local motifs, a challenge for existing architectures due to computational limits and contextual granularity.", "adaptation_ground_truth": "Develop a hybrid architecture integrating Transformer layers for local motif extraction and Mamba2 state-space blocks for efficient long-range dependency modeling, enabling linear-time processing of megabase-scale sequences while preserving nucleotide-level sensitivity.", "ground_truth_reasoning": "The hybrid leverages Transformers' strength in local pattern recognition and Mamba2's linear-scaling state-space formulation, addressing DNA's dual needs: high-resolution motif analysis (Constraint 3) and computational tractability for chromosome-length sequences (Constraint 2) without sacrificing biological accuracy.", "atomic_constraints": ["Constraint 1: Ultra-long-range dependencies - Regulatory elements interact across megabase-scale genomic distances requiring sub-quadratic sequence modeling.", "Constraint 2: Computational tractability - Full-chromosome modeling demands O(n) complexity to handle millions of nucleotides efficiently.", "Constraint 3: Local-context sensitivity - Precise nucleotide motifs (e.g., TF binding sites) necessitate fine-grained, position-aware feature extraction.", "Constraint 4: Data sparsity - Limited experimental annotations require sample-efficient architectures avoiding overparameterization."], "distractors": [{"option": "Deploy a pure sparse-attention Transformer with global-local attention patterns, optimizing memory usage through block-sparse operations for genomic sequences while maintaining full context awareness via hierarchical attention mechanisms.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 by retaining near-quadratic complexity through sparse approximations and Constraint 4 due to high parameter counts requiring excessive data."}, {"option": "Implement a standalone Mamba2 model with state-space transitions across all sequence positions, utilizing hardware-optimized scans for linear-time processing of DNA while capturing global interactions through recurrent dynamics.", "label": "Naive Application", "analysis": "Violates Constraint 3 by inadequately resolving local nucleotide motifs due to inherent smoothing in state-space representations and Constraint 1 through limited position-specific context."}, {"option": "Adapt a ProteinBERT-style bidirectional Transformer pretrained on protein sequences, fine-tuning it for DNA tasks with rotary position embeddings and masked language modeling to transfer cross-domain biological knowledge.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 due to quadratic attention limiting genomic-scale context and Constraint 3 from domain mismatch in tokenization strategies between amino acids and nucleotides."}]}}
{"id": 274763471, "title": "Democratizing Protein Language Model Training, Sharing and Collaboration", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "High computational costs and data privacy barriers prevent widespread access to protein language model development, limiting collaborative biological discovery.", "adaptation_ground_truth": "A federated learning framework with parameter-efficient fine-tuning enables decentralized training across institutions while preserving data privacy, coupled with model compression for resource-constrained deployment.", "ground_truth_reasoning": "This approach addresses protein data sensitivity by avoiding raw data sharing, reduces computational barriers through distributed training and lightweight fine-tuning, and accommodates hardware heterogeneity via compression—aligning with biological collaboration needs.", "atomic_constraints": ["Constraint 1: Resource Intensity - Training billion-parameter models requires months of specialized hardware access, excluding under-resourced labs.", "Constraint 2: Data Sensitivity - Experimental protein sequences often contain private human genomic information restricting centralized data pooling.", "Constraint 3: Hardware Heterogeneity - Global collaborators possess varying compute infrastructure (from laptops to clusters), necessitating flexible deployment."], "distractors": [{"option": "Centralized training of a 100-billion parameter transformer on all public protein databases using 1024 TPUs, with cloud-based inference APIs for global access.", "label": "SOTA Bias", "analysis": "Violates Resource Intensity and Data Sensitivity constraints by requiring prohibitive computational resources and ignoring private data exclusion from public databases."}, {"option": "Standard transformer architecture trained end-to-end on aggregated protein sequence datasets using 256 GPUs, followed by public release of full model weights for community use.", "label": "Naive Application", "analysis": "Violates Data Sensitivity and Hardware Heterogeneity constraints through centralized sensitive data aggregation and unoptimized weights requiring high-end inference resources."}, {"option": "Distributed graph convolutional networks for collaborative protein structure prediction, sharing structural embeddings across labs using synchronized weight updates.", "label": "Cluster Competitor", "analysis": "Violates Resource Intensity constraint as GCNs require expensive structural data preprocessing and lack parameter efficiency of sequence-based transformers for large-scale deployment."}]}}
{"id": 277855235, "title": "AlphaFold3 at CASP16", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate joint prediction of protein and non-protein biomolecular structures, including complexes, requires modeling diverse atomic interactions while ensuring computational scalability and reliable model selection.", "adaptation_ground_truth": "AlphaFold3 integrates a transformer architecture with multi-molecule training data and automated decoy generation, enabling unified prediction of proteins and non-proteins via its web server without manual refinement.", "ground_truth_reasoning": "The transformer's attention mechanisms capture long-range atomic interactions across biomolecule types, while automated decoy generation addresses model diversity needs. Unified training on diverse structures satisfies multi-molecule constraints, and server deployment ensures scalability without performance loss from manual intervention.", "atomic_constraints": ["Constraint 1: Multi-molecule interactions - Must model atomic-level forces between proteins, RNA, and ligands with varying chemical properties.", "Constraint 2: Structural stoichiometry - Requires accurate prediction of subunit composition and copy numbers in heteromeric complexes.", "Constraint 3: Decoy diversity - Demands generation of multiple structurally distinct conformations for effective model selection.", "Constraint 4: Computational scalability - Needs efficient processing of large macromolecular assemblies without manual tuning."], "distractors": [{"option": "A protein language foundation model pre-trained on evolutionary sequences predicts structures via zero-shot inference. It leverages unsupervised embeddings for protein and RNA folding, using attention heads to infer atomic coordinates without complex-specific training.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 by relying solely on sequence statistics, ignoring stoichiometric dependencies in heteromeric complexes. Lacks explicit physics-based interaction modeling for Constraint 1."}, {"option": "Standard transformer architecture trained exclusively on monomeric proteins, extended to complexes via sequence concatenation. Uses residue-level confidence metrics and iterative refinement cycles, with five models generated per target using default hyperparameters.", "label": "Naive Application", "analysis": "Violates Constraint 1 due to absence of non-protein training data. Fails Constraint 2 by not modeling stoichiometric variations, and Constraint 3 with limited decoy diversity from homogeneous inputs."}, {"option": "Vfold-Pipeline for RNA structure prediction combined with AlphaFold2 for proteins, assembled using rigid-body docking. DockQ scoring evaluates complex quality, with Monte Carlo sampling to explore conformational space for interface refinement.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 by treating molecules separately, disrupting joint atomic interactions. Rigid docking ignores dynamic flexibility (Constraint 1), and modular design hinders stoichiometric accuracy (Constraint 2)."}]}}
{"id": 277042992, "title": "Integrating Dynamical Systems Modeling with Spatiotemporal scRNA-Seq Data Analysis", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Neural Ordinary Differential Equations"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Inferring continuous cell-state trajectories from sparse, high-dimensional scRNA-Seq snapshots that capture epigenetic landscape transitions and cell-fate decisions.", "adaptation_ground_truth": "Neural ODEs model gene expression dynamics by learning a latent-space vector field from sparse temporal snapshots. The neural network parameterizes ODE derivatives, enabling continuous trajectory inference and prediction of unobserved states through adaptive numerical integration.", "ground_truth_reasoning": "Neural ODEs address scRNA-Seq constraints by learning continuous dynamics from discrete observations, handling irregular time spacing through ODE solvers, reducing dimensionality via latent space, and capturing nonlinear transitions without predefined mechanistic equations.", "atomic_constraints": ["Constraint 1: High-dimensional sparsity - scRNA-Seq measures 10k+ genes per cell but with low sampling density across timepoints.", "Constraint 2: Continuous state transitions - Cell differentiation occurs through continuous epigenetic landscape traversals, not discrete jumps.", "Constraint 3: Temporal irregularity - Experimental sampling has non-uniform time intervals with varying cell counts per snapshot.", "Constraint 4: Multi-stable dynamics - Systems exhibit coexisting attractors (cell fates) separated by low-probability transition states."], "distractors": [{"option": "Transformer-based sequence modeling of scRNA-Seq time series, using self-attention to capture gene interactions across timesteps. Positional encoding handles temporal ordering while masked language modeling imputes missing expression values.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Discrete attention mechanisms cannot model infinitesimal continuous transitions between cell states, losing dynamical continuity essential for landscape reconstruction."}, {"option": "Standard Neural ODE implementation with fixed-step Runge-Kutta solver on raw gene expression space. Optimization minimizes mean squared error between predicted and observed expression matrices at measurement times.", "label": "Naive Application", "analysis": "Violates Constraint 1: Operating directly in high-dimensional space ignores data sparsity, causing overfitting; lacks latent compression essential for biological generalization."}, {"option": "Wasserstein geodesic interpolation between cell populations using unbalanced optimal transport. Entropy-regularized couplings align consecutive timepoints, constructing trajectories through barycentric projection in expression space.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Pure transport methods interpolate between densities but cannot infer underlying vector fields or multi-stable basin structures governing fate decisions."}]}}
{"id": 275850396, "title": "Consensus representation of multiple cell–cell graphs from gene signaling pathways for cell type annotation", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Similarity Network Fusion (SNF)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Robust cell type annotation in scRNA-seq data is hindered by marker ambiguity, batch effects, and incomplete representation of cellular interactions and regulatory networks.", "adaptation_ground_truth": "scMCGraph integrates multiple pathway-specific graphs (from databases like KEGG/Reactome) via Similarity Network Fusion into a consensus graph. This graph reconstructs pathway views, capturing complementary signaling activities to enhance cell-cell relationships and annotation accuracy.", "ground_truth_reasoning": "SNF fusion addresses pathway complementarity by aggregating diverse database views into a unified representation. Consensus graph reconstruction mitigates data sparsity through cross-pathway information sharing. Multi-view consistency is enforced by mutual optimization, while scalability accommodates increasing pathways without performance degradation.", "atomic_constraints": ["Constraint 1: Pathway Complementarity - Different databases capture non-overlapping signaling aspects, requiring integration for comprehensive cellular state representation.", "Constraint 2: Data Sparsity - Single-cell RNA-seq exhibits high dropout rates, necessitating aggregation of multiple pathway signals for robust feature learning.", "Constraint 3: Multi-view Consistency - Cellular states must exhibit coherent patterns across divergent pathway activity views despite technical noise."], "distractors": [{"option": "scBERT, a pretrained gene-sequence language model, generates cell embeddings from expression profiles. Fine-tuning on annotated datasets enables direct cell type classification using contextual gene relationships.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Ignores pathway-specific signaling hierarchies, treating genes as isolated tokens without biochemical pathway context. Lacks explicit multi-database fusion, reducing biological interpretability."}, {"option": "A single pathway graph (e.g., KEGG) is constructed from gene expressions. Graph convolutional networks process this view with neighbor aggregation and ReLU activations for cell classification, using standard dropout for regularization.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Relies on one database, missing complementary pathway signals. Single-view approach amplifies sparsity issues without cross-pathway consensus to impute missing interactions."}, {"option": "Adversarial graph convolutional networks process each pathway graph separately. A discriminator aligns feature distributions across views, and the concatenated embeddings feed a classifier for cell annotation.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Adversarial alignment cannot enforce explicit inter-pathway consistency. Feature concatenation without structured fusion dilutes complementary signaling relationships, reducing cross-dataset robustness."}]}}
{"id": 277233468, "title": "RNAmigos2: accelerated structure-based RNA virtual screening with deep graph learning", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Graph Neural Networks (GNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Traditional molecular docking is computationally prohibitive for large-scale RNA virtual screening due to RNA's structural complexity and limited experimental data, hindering drug discovery for RNA targets.", "adaptation_ground_truth": "RNAmigos2 integrates coarse-grained 3D RNA modeling with graph neural networks, augmented by synthetic data generation and RNA-specific self-supervised pretraining. This pipeline accelerates screening while capturing RNA structural motifs and overcoming data scarcity.", "ground_truth_reasoning": "Coarse-grained modeling enables scalability by reducing computational complexity. Synthetic data and self-supervision address data scarcity by generating diverse training examples. The GNN inherently respects RNA's 3D spatial constraints and structural motifs through graph representations.", "atomic_constraints": ["Constraint 1: Scalability - Must process millions of compounds against large RNA structures within feasible computational time.", "Constraint 2: Data Efficiency - Must operate effectively with limited experimentally validated RNA-ligand interaction data.", "Constraint 3: Structural Specificity - Must capture RNA's unique 3D motifs (e.g., loops, pseudoknots) and binding site geometries.", "Constraint 4: Conformational Robustness - Must maintain accuracy across variations in RNA binding site conformations."], "distractors": [{"option": "Apply a transformer-based foundation model pretrained on protein-ligand datasets to RNA targets. The architecture uses attention mechanisms to process sequence and structural embeddings, leveraging transfer learning for binding affinity predictions.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 (Data Efficiency) due to RNA-specific data scarcity limiting transfer learning. Also struggles with Constraint 3 (Structural Specificity) as transformers lack inherent 3D geometric reasoning for RNA motifs."}, {"option": "Use a standard graph neural network with atomic-resolution RNA and ligand graphs. Node features include element types and coordinates, with edge updates via message passing. Training employs supervised learning on curated docking scores.", "label": "Naive Application", "analysis": "Violates Constraint 1 (Scalability) due to high computational load from atomic graphs. Fails Constraint 2 (Data Efficiency) without data augmentation or self-supervision for limited labeled data."}, {"option": "Implement an optimized docking pipeline using rDock with GPU parallelization and machine learning-enhanced scoring. Integrates ChEMBL bioactivity data for pose refinement and ZINC library pre-filtering to accelerate screening.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 (Scalability) as docking remains computationally intensive despite optimizations. Lacks Constraint 3 (Structural Specificity) due to rigid scoring functions not adapting to RNA's flexible motifs."}]}}
{"id": 278324255, "title": "Automatic biomarker discovery and enrichment with BRAD", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Retrieval-Augmented Generation"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Automating biomarker discovery requires integrating fragmented biological knowledge from heterogeneous sources (literature, databases) while maintaining precision and adaptability to evolving data.", "adaptation_ground_truth": "BRAD integrates structured biomedical knowledge graphs with RAG, enabling context-aware retrieval from curated databases and literature for precise biomarker-disease association mapping and enrichment analysis.", "ground_truth_reasoning": "Knowledge graphs resolve data heterogeneity by structuring entities/relationships, ensuring precise retrieval critical for biomarker validity. Dynamic updating accommodates new findings without retraining. Combined with RAG, it maintains interpretability through traceable reasoning paths.", "atomic_constraints": ["Heterogeneous Data Integration - Biomarker information spans unstructured literature and structured databases requiring unified semantic access.", "Dynamic Knowledge Evolution - Continuous biomarker discoveries necessitate systems that update without full model retraining.", "Precision-Specificity Tradeoff - High-recall retrieval risks false positives; biomarker validation demands entity-level precision.", "Interpretability Mandate - Biologists require transparent evidence chains for biomarker-disease associations."], "distractors": [{"option": "A foundation model like GPT-4 fine-tuned on biomedical corpora generates biomarker hypotheses directly. It leverages emergent reasoning capabilities and parameterized knowledge for end-to-end discovery without external retrieval modules.", "label": "SOTA Bias", "analysis": "Violates Dynamic Knowledge Evolution: Static training data cannot incorporate new biomarker findings post-deployment. Lacks Precision-Specificity control, increasing hallucination risks for rare entities."}, {"option": "Standard RAG queries PubMed/PMC using BM25 retrieval, feeding top passages to an LLM for biomarker summarization. Query expansion handles terminology variations, and result ranking prioritizes recent publications.", "label": "Naive Application", "analysis": "Violates Heterogeneous Data Integration: Unstructured retrieval misses structured database relationships critical for biomarker validation. Suffers Precision-Specificity issues from keyword-based document retrieval."}, {"option": "Chain-of-thought prompting guides an LLM through multi-step biomarker inference. Carefully engineered prompts decompose tasks into biological reasoning steps, leveraging the model's parametric knowledge for enrichment analysis without retrieval.", "label": "Cluster Competitor", "analysis": "Violates Interpretability Mandate: Opaque reasoning traces obscure evidence sources. Fails Dynamic Knowledge Evolution as prompts cannot dynamically incorporate new database entries or literature."}]}}
{"id": 276669854, "title": "Comparative Assessment of Protein Large Language Models for Enzyme Commission Number Prediction", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer-based Large Language Models (LLMs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate prediction of hierarchical Enzyme Commission (EC) numbers from protein sequences requires modeling multi-label dependencies and long-range functional interactions while handling extreme class imbalance.", "adaptation_ground_truth": "Fine-tuning transformer-based protein language models (e.g., ProtBERT, ESM) pre-trained on massive sequence corpora with hierarchical multi-label classification heads, leveraging self-attention to capture non-local residue interactions and mitigate data sparsity.", "ground_truth_reasoning": "Transformers capture long-range dependencies (Constraint 3) via self-attention. Protein-specific pre-training provides evolutionary context for rare EC classes (Constraint 4). Hierarchical multi-label heads explicitly model functional dependencies across EC levels (Constraints 1-2).", "atomic_constraints": ["Constraint 1: Hierarchical Label Dependencies - EC numbers form a 4-level taxonomy requiring joint modeling of parent-child relationships.", "Constraint 2: Multi-label Functional Redundancy - Single enzymes often catalyze multiple reactions necessitating simultaneous EC predictions.", "Constraint 3: Long-range Residue Interactions - Catalytic function emerges from non-local amino acid interactions across sequences.", "Constraint 4: Extreme Class Sparsity - Over 40% of EC classes have <10 training examples, demanding robust representation learning."], "distractors": [{"option": "Implementing a vision transformer pre-trained on ImageNet, converting protein sequences to 2D contact maps. Uses standard cross-entropy classification with transfer learning from visual patterns for EC prediction.", "label": "SOTA Bias", "analysis": "Violates Constraint 3: Contact maps lose sequential context and evolutionary signals. Lacks protein-specific pretraining, worsening performance on sparse classes (Constraint 4)."}, {"option": "Applying standard BERT architecture with standard tokenization and flat classification layers. Uses standard pretrained weights from general text corpora without hierarchical output adjustments.", "label": "Naive Application", "analysis": "Violates Constraints 1-2: Flat classifiers ignore hierarchical dependencies and multi-label requirements. General-domain pretraining misses biophysical context (Constraint 3)."}, {"option": "Training convolutional neural networks with multiple kernel sizes to capture local motifs. Uses max-pooling layers and dense classifiers similar to DEEPre's architecture for EC classification.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: CNNs struggle with long-range residue interactions. Pooling operations discard positional information critical for EC hierarchy (Constraint 1)."}]}}
{"id": 277772673, "title": "DeepHeteroCDA: circRNA–drug sensitivity associations prediction via multi-scale heterogeneous network and graph attention mechanism", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Graph Attention Networks (GATs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting circRNA-drug sensitivity associations to advance personalized oncology, challenged by biological heterogeneity, sparse labeled data, and multi-scale interactions in molecular networks.", "adaptation_ground_truth": "DeepHeteroCDA integrates multi-scale heterogeneous networks with graph attention mechanisms. It models circRNAs, drugs, and diseases as distinct node types, using attention layers to dynamically weigh neighborhood features across diverse biological relationships for association prediction.", "ground_truth_reasoning": "The multi-scale heterogeneous network handles biological heterogeneity (Constraint 1) and multi-level interactions (Constraint 2). Graph attention adaptively prioritizes relevant neighbors, overcoming data sparsity (Constraint 3) and feature imbalance (Constraint 4) through context-dependent weighting.", "atomic_constraints": ["Constraint 1: Biological Heterogeneity - Must process diverse entity types (circRNAs, drugs, diseases) with distinct relational semantics.", "Constraint 2: Multi-scale Interactions - Requires simultaneous modeling of molecular-level and pathway-level biological relationships.", "Constraint 3: Association Sparsity - Must operate with limited known circRNA-drug sensitivity pairs due to experimental costs.", "Constraint 4: Feature Imbalance - Needs adaptive weighting of node features with varying biological significance across contexts."], "distractors": [{"option": "Implementing a transformer-based foundation model pre-trained on biological sequences. We encode circRNA and drug structures as token sequences, then apply multi-head self-attention to predict associations through cross-modal fusion layers.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: Transformers process sequential tokens, failing to explicitly model heterogeneous graph structures or multi-scale biological relationships."}, {"option": "Using standard graph attention networks on a homogeneous circRNA-drug interaction graph. Node features include sequence embeddings, with GAT layers performing neighborhood aggregation. A fully connected layer outputs sensitivity scores.", "label": "Naive Application", "analysis": "Violates Constraint 1: Homogeneous graphs ignore node-type diversity and relationship semantics, losing critical biological context."}, {"option": "Adopting graph-based multi-label learning from Cluster A. Construct a circRNA-drug graph with multi-label propagation, where each node's labels represent sensitivity associations. Incorporate biological features via Laplacian regularization.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Multi-label propagation assigns fixed weights to neighbors, lacking adaptive attention to contextually relevant features."}]}}
{"id": 279071625, "title": "Improving AlphaFold2- and AlphaFold3-Based Protein Complex Structure Prediction With MULTICOM4 in CASP16.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer-based Deep Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate prediction of multichain protein complexes remains challenging due to dynamic inter-chain interactions, stoichiometric variability, and sparse experimental data for quaternary structures.", "adaptation_ground_truth": "Integration of AlphaFold2 and AlphaFold3 with in-house stoichiometry prediction, structure-enhanced MSA generation, exception handling, and deep learning-based model quality assessment to improve complex structure accuracy.", "ground_truth_reasoning": "This approach addresses atomic constraints by predicting chain counts (stoichiometry), enriching evolutionary signals via structural MSA comparisons, mitigating modeling failures, and selecting physically plausible models through quality assessment, ensuring stable interfaces and conformational accuracy.", "atomic_constraints": ["Constraint 1: Quaternary Symmetry - Protein complexes require precise geometric arrangements of chains to maintain symmetry and avoid steric clashes at interfaces.", "Constraint 2: Interface Complementarity - Atomic-level interactions (e.g., hydrogen bonds, hydrophobic packing) demand exact spatial alignment between chain surfaces for stability.", "Constraint 3: Conformational Dynamics - Binding-induced structural changes necessitate modeling flexibility to prevent unrealistic rigid-body docking artifacts.", "Constraint 4: Stoichiometric Specificity - Variable chain counts in complexes require accurate prediction of subunit composition prior to assembly."], "distractors": [{"option": "Using AlphaFold3's diffusion model alone for complex prediction, leveraging its generative capabilities without stoichiometry guidance or MSA enhancements, assuming inherent chain-assembly proficiency.", "label": "SOTA Bias", "analysis": "Violates Constraint 4: Lacks stoichiometry prediction, leading to incorrect chain counts; sparse complex data limits interface accuracy without structure-augmented MSAs."}, {"option": "Standard AlphaFold2 deployment with default MSA generation and no exception handling, directly applied to complex sequences without model ranking or stoichiometry adjustments.", "label": "Naive Application", "analysis": "Violates Constraint 2 and 3: Default MSAs miss interface-specific evolutionary signals; rigidity causes clashes in flexible regions without conformational refinement."}, {"option": "Homology-based complex modeling with MODELLER using oligomeric templates from PDB, aligning target sequences without deep learning enhancements or quality assessment.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 and 4: Template absence for novel complexes yields incorrect symmetry; cannot predict variable stoichiometry or adaptive interfaces."}]}}
{"id": 279074059, "title": "Parameter-Efficient Fine-Tuning of a Supervised Regulatory Sequence Model", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Prefix-Tuning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Adapting large regulatory sequence models to tissue-specific tasks with limited data while preserving general biological knowledge and computational feasibility.", "adaptation_ground_truth": "We implement prefix-tuning by prepending trainable continuous vectors to input sequences. Only these prefix parameters are optimized during fine-tuning, keeping the pre-trained model frozen. This enables tissue-specific adaptation with minimal new parameters.", "ground_truth_reasoning": "Prefix-tuning addresses tissue-specific data scarcity (Constraint 1) by reducing trainable parameters, preventing overfitting. It maintains computational efficiency (Constraint 2) by freezing the base model and avoids catastrophic forgetting (Constraint 3) by preserving pre-trained weights. The input-level adaptation captures tissue-specific regulatory contexts without altering core biological knowledge.", "atomic_constraints": ["Constraint 1: Data Scarcity - Tissue-specific transcription factor binding datasets are extremely limited, necessitating low-parameter adaptation to prevent overfitting.", "Constraint 2: Computational Efficiency - Adapting large regulatory models for multiple tissues requires minimal computational overhead per task.", "Constraint 3: Knowledge Retention - General regulatory principles from genome-scale pre-training must be preserved during tissue-specific specialization."], "distractors": [{"option": "We apply full fine-tuning of a foundation transformer model using tissue-specific data. Hyperparameter optimization and gradient clipping ensure stable convergence while leveraging the model's full capacity for regulatory pattern recognition.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: Full fine-tuning requires substantial tissue-specific data to avoid overfitting and incurs high computational costs per task, making it impractical for resource-limited biological settings."}, {"option": "The base model undergoes standard fine-tuning where all weights are updated with tissue-specific data. Learning rate scheduling and layer normalization maintain stability while adapting the entire parameter set to new regulatory contexts.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Updating all parameters demands large tissue-specific datasets to generalize and requires significant computational resources, conflicting with sparse biological data and efficiency needs."}, {"option": "We utilize Low-Rank Adaptation (LoRA) by injecting trainable rank-decomposition matrices into attention layers. Only these low-rank projections are updated, allowing efficient adaptation while retaining original model weights.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: LoRA modifies internal weight matrices, potentially disrupting learned biological representations. Unlike input-level prefix-tuning, weight alterations risk erasing critical regulatory knowledge encoded in the original parameters."}]}}
{"id": 279168739, "title": "Iterative deep learning design of human enhancers exploits condensed sequence grammar to achieve cell-type specificity.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Deep Convolutional Neural Networks (CNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Designing human enhancers requires capturing condensed combinatorial grammar of transcription factor binding sites for precise cell-type specificity, which exceeds simple motif-summing approaches.", "adaptation_ground_truth": "Iterative CNN optimization with activation maximization refines synthetic sequences through cyclic prediction and redesign, enforcing grammar-aware constraints for maximal target-cell specificity and minimal off-target activity.", "ground_truth_reasoning": "The iterative CNN approach addresses combinatorial grammar by progressively optimizing sequences through feedback loops, leveraging convolutional filters to detect spatial motif arrangements while maintaining biological validity through nucleotide-level constraints.", "atomic_constraints": ["Constraint 1: Combinatorial Grammar - TFBS interactions require non-additive spatial arrangements beyond isolated motifs.", "Constraint 2: Cell-Type Specificity - Enhancers must activate exclusively in target cells via differential TF availability.", "Constraint 3: Sequence Validity - Designs must maintain nucleotide composition and length of functional enhancers.", "Constraint 4: Data Efficiency - Models must generalize from limited epigenomic datasets without exhaustive experimental validation."], "distractors": [{"option": "A transformer model pretrained on genomic corpora generates enhancers via masked language modeling. Fine-tuning on accessibility data produces sequences capturing long-range dependencies through self-attention mechanisms.", "label": "SOTA Bias", "analysis": "Violates Constraint 4: Transformers require massive pretraining data unavailable for cell-type-specific grammar, lacking optimization for compact motif arrangements."}, {"option": "Standard CNNs predict chromatin accessibility from sequence. One-step activation maximization designs enhancers by gradient ascent on target-cell predictions using fixed convolutional filters without redesign cycles.", "label": "Naive Application", "analysis": "Violates Constraint 1: Single-pass optimization ignores cooperative TFBS interactions, producing sequences with motif collisions violating condensed grammar."}, {"option": "Generative adversarial networks create synthetic enhancers by sampling latent vectors. A discriminator evaluates sequence fitness while adversarial training maximizes diversity across cell-type-specific candidates.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: GANs lack explicit differential optimization, failing to suppress off-target activity through iterative specificity refinement."}]}}
{"id": 281496907, "title": "SimpleFold: Folding Proteins is Simpler than You Think", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Denoising Diffusion Probabilistic Models"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate and efficient generation of protein tertiary structures from amino acid sequences, critical for understanding biological functions and drug design.", "adaptation_ground_truth": "SimpleFold employs a diffusion model operating on internal coordinates (torsion angles) rather than Cartesian coordinates, with equivariant networks ensuring SE(3)-invariant denoising. This enforces physical constraints during generation while reducing computational complexity.", "ground_truth_reasoning": "The method uses torsion angles as a reduced representation that intrinsically preserves bond lengths/angles. Equivariant networks maintain rotational symmetry, and the diffusion process iteratively refines structures to avoid steric clashes. This balances physical accuracy with scalability.", "atomic_constraints": ["Constraint 1: SE(3) Invariance - Generated protein structures must be independent of global rotation/translation in 3D space.", "Constraint 2: Local Geometry Feasibility - Bond lengths and angles must adhere to biophysically plausible ranges.", "Constraint 3: Steric Clash Avoidance - Atomic van der Waals radii require minimum separation distances between non-bonded atoms."], "distractors": [{"option": "Leverage a large protein language model (e.g., ESM-2) to autoregressively predict atomic coordinates. This capitalizes on sequence-structure correlations but relies on implicit geometric reasoning without explicit 3D constraints.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 3: Autoregressive coordinate prediction often generates infeasible bond lengths and steric clashes due to lack of explicit physical enforcement."}, {"option": "Implement a standard diffusion model on Cartesian coordinates using isotropic Gaussian noise. The denoising network processes atom positions directly, with standard U-Net architectures for 3D data.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 2: Cartesian diffusion breaks SE(3) symmetry and produces distorted local geometries without angle/length constraints."}, {"option": "Apply rectified flow to generate protein structures via straight-line trajectories in data space. This deterministic method accelerates convergence by minimizing transport costs between noise and native conformations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Linear interpolations cause atomic overlaps during generation, as rectified flow lacks explicit clash-avoidance mechanisms."}]}}
{"id": 276758817, "title": "Analysis of the subtype I amidohydrolase responsible for Ochratoxin A degradation in the Sphingomonas genus.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Molecular Docking"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Characterizing how Sphingomonas subtype I amidohydrolase selectively binds and hydrolyzes Ochratoxin A's amide bond, given structural heterogeneity in enzyme-substrate complexes and scarcity of experimental structures for this specific system.", "adaptation_ground_truth": "Hybrid docking integrating homology modeling of the amidohydrolase active site with flexible side-chain sampling during OTA binding simulations, calibrated against kinetic data from related hydrolases to resolve catalytic residue positioning.", "ground_truth_reasoning": "This approach addresses the need for precise active-site modeling amid limited structural data. Flexibility sampling handles OTA's conformational variability, while kinetic calibration ensures catalytic geometry respects hydrolysis requirements. Homology modeling bridges sequence-function gaps specific to subtype I amidohydrolases.", "atomic_constraints": ["Constraint 1: Catalytic Distance - The scissile amide bond in OTA must be positioned within 3.5Å of nucleophilic residues for hydrolysis.", "Constraint 2: Conformational Plasticity - OTA's isocoumarin and phenylalanine moieties exhibit torsional flexibility requiring multi-conformer docking.", "Constraint 3: Metal Coordination - Active-site zinc ions impose strict tetrahedral geometry on coordinating residues and substrate orientation.", "Constraint 4: Solvent Exposure - Hydrolytic water access necessitates binding poses avoiding deep hydrophobic pockets."], "distractors": [{"option": "Using a pretrained ESM-2 protein language model, we predict the amidohydrolase structure and perform rigid docking with OTA. The transformer architecture leverages evolutionary patterns to infer binding pockets, with scoring based on residue coevolution probabilities across homologs.", "label": "SOTA Bias", "analysis": "Violates Constraint 3: Ignores metal coordination geometry as language models lack explicit atomic coordinate optimization, risking misalignment of catalytic zinc ligands."}, {"option": "Standard AutoDock Vina docking with Lamarckian GA, using a static crystal structure of a homologous amidohydrolase. Grid parameters centered on conserved residues, exhaustiveness=32, with scoring based on the default Vina force field for pose ranking.", "label": "Naive Application", "analysis": "Violates Constraint 2: Static protein structure fails to sample side-chain rearrangements needed for OTA's flexible moieties, producing non-bioactive conformations."}, {"option": "AlphaFold-predicted amidohydrolase structure docked with OTA using default protocols. High pLDDT confidence regions guide binding site selection, with multimer modeling to simulate enzyme-substrate complexes based on predicted interface probabilities.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: AlphaFold's static prediction cannot dynamically adjust catalytic residue distances during docking, risking sub-angstrom errors in hydrolysis positioning."}]}}
{"id": 278741044, "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting fine-grained protein functions requires modeling interdependent residue interactions and 3D structural constraints that determine biological activity, where sequence alone is insufficient.", "adaptation_ground_truth": "VenusX integrates pairwise residue distances and angular orientations into transformer attention layers, enabling geometric-aware feature learning while preserving global sequence context for functional site prediction.", "ground_truth_reasoning": "This adaptation respects protein folding physics by encoding spatial relationships directly into attention weights, satisfying constraints of SE(3)-invariant function determination and long-range residue dependencies while leveraging transformer strength in contextual modeling.", "atomic_constraints": ["Long-range residue dependencies: Functional sites involve non-adjacent residues interacting through 3D folding, requiring non-local interaction modeling.", "SE(3)-invariant function: Protein functions remain unchanged under rotation/translation, demanding geometric equivariance in feature extraction.", "Sparse functional annotations: Limited experimental data necessitates efficient knowledge transfer from unlabeled structural data.", "Multi-scale interactions: Functional understanding requires simultaneous modeling of atomic contacts and domain-level motifs."], "distractors": [{"option": "Employ a pure sequence-based ESM-2 foundation model pre-trained on 250M proteins, using its contextual embeddings with task-specific heads for functional prediction without structural inputs.", "label": "SOTA Bias", "analysis": "Violates Constraints 1 and 2 by ignoring 3D spatial relationships and SE(3) invariance, relying solely on evolutionary patterns from sequence data."}, {"option": "Implement a standard transformer with positional encodings on amino acid sequences, adding convolutional layers for local feature extraction and multi-task learning for auxiliary predictions.", "label": "Naive Application", "analysis": "Violates Constraints 1 and 2 due to absence of explicit geometric constraints and inability to model non-Euclidean residue relationships in 3D space."}, {"option": "Use geometric deep learning with graph neural networks on residue-level protein structures, incorporating edge features based on atomic distances and surface curvature for functional site classification.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 by potentially overlooking global sequence context and long-range dependencies beyond local neighborhoods, which transformers capture effectively."}]}}
{"id": 276228783, "title": "Trans-MoRFs: A Disordered Protein Predictor Based on the Transformer Architecture", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate computational identification of molecular recognition features (MoRFs) within intrinsically disordered protein regions is challenged by their dynamic disorder-to-order transitions and limitations of existing tools in handling variable sequence lengths.", "adaptation_ground_truth": "Trans-MoRFs leverages transformer self-attention mechanisms to capture long-range residue interactions in protein sequences, enabling robust prediction of MoRFs across diverse sequence lengths with high AUC performance.", "ground_truth_reasoning": "The self-attention mechanism dynamically weights all residue interactions regardless of sequence position, addressing long-range dependencies critical for MoRF transitions. Transformers inherently process variable-length inputs without architectural modifications, satisfying length adaptability constraints. Pre-training on protein sequences provides transferable representations that mitigate data scarcity issues.", "atomic_constraints": ["Constraint 1: Long-range residue dependencies - Functional residues in disordered regions interact across distant sequence positions during folding transitions.", "Constraint 2: Sequence length variability - Protein chains range from short peptides to multi-domain structures (>5,000 residues), requiring length-agnostic processing.", "Constraint 3: Data scarcity - Experimentally validated MoRFs are limited (<0.1% of known proteins), demanding data-efficient learning."], "distractors": [{"option": "Applying a pre-trained protein language model like ProtBERT for MoRF prediction. This approach transfers knowledge from large unlabeled sequence corpora through fine-tuning on MoRF annotations, leveraging evolutionary patterns captured during self-supervised training.", "label": "SOTA Bias", "analysis": "Violates Constraint 3: Foundation models require massive training data unavailable for MoRFs, leading to overfitting on limited annotations and poor generalization to rare transition patterns."}, {"option": "Implementing a convolutional neural network with fixed-size sliding windows to scan protein sequences. Local residue patterns are extracted through multiple filter layers, followed by fully connected layers for MoRF probability estimation at each position.", "label": "Naive Application", "analysis": "Violates Constraint 1: Fixed receptive fields cannot model distant residue interactions essential for disorder transitions. Also violates Constraint 2: Inefficient for long sequences due to window-size limitations and computational overhead."}, {"option": "Using BioSeq-Analysis2.0's feature engineering pipeline to compute physicochemical descriptors like hydrophobicity indexes and secondary structure propensities. These features train a gradient boosting classifier for residue-level MoRF prediction.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Handcrafted features cannot capture complex long-range dependencies between residues. Violates Constraint 3: Manual feature engineering performs poorly with limited training data due to information loss in descriptor abstraction."}]}}
{"id": 280292315, "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Variational Autoencoder (VAE) with Euclidean Latent Space Enforcement"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Single-cell gene expression data forms complex, high-dimensional manifolds where nonlinear biological trajectories (e.g., cell differentiation) must be interpolated for dynamic analysis.", "adaptation_ground_truth": "A VAE with Euclidean latent space regularization, enforcing flat geometry via a penalty on Riemannian metric deviations from identity during manifold learning.", "ground_truth_reasoning": "Euclidean enforcement ensures linear interpolations approximate true biological trajectories by aligning latent paths with geodesics. It addresses single-cell data sparsity through probabilistic modeling while maintaining computational tractability for large-scale genomics.", "atomic_constraints": ["Constraint 1: Continuous Trajectory Preservation - Cell differentiation forms smooth paths requiring linear-interpolatable latent spaces.", "Constraint 2: High-Dimensional Sparsity - Gene expression matrices have 10K+ dimensions but low per-cell observations.", "Constraint 3: Local Euclidean Geometry - Microscopic cellular state transitions exhibit locally flat manifold structure."], "distractors": [{"option": "A transformer-based autoencoder processes single-cell sequences using self-attention layers. Pre-training on gene tokens captures global dependencies, and latent interpolations use attention-weighted averaging across cell states.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Transformers prioritize discrete token relationships over continuous geometric structure, distorting trajectory smoothness. Attention mechanisms also demand excessive data, conflicting with Constraint 2 sparsity."}, {"option": "Standard VAE with Gaussian latent prior trains on gene expression data using reconstruction loss and KL divergence. Latent interpolations linearly connect points without geometric constraints.", "label": "Naive Application", "analysis": "Violates Constraint 3: Unconstrained latent space curves nonlinearly under data density variations, causing interpolated paths to deviate from true biological trajectories on the manifold."}, {"option": "Riemannian autoencoder with learnable metric tensor models latent space curvature. Geodesic interpolation via exponential maps captures nonlinear cell dynamics, regularized by sectional curvature penalties.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: Complex curvature estimation requires dense data samples unavailable in sparse single-cell genomics. Computational overhead also scales poorly with gene dimensionality."}]}}
{"id": 277939292, "title": "EnsembleSE: identification of super-enhancers based on ensemble learning", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Ensemble Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate super-enhancer identification from genomic sequences is challenged by biological noise, heterogeneous regulatory patterns, and sparse functional signals in high-dimensional DNA data.", "adaptation_ground_truth": "EnsembleSE integrates multiple heterogeneous base learners (e.g., CNNs, SVMs) through weighted voting, combining their diverse feature representations to improve robustness against genomic variability and sparse signal detection.", "ground_truth_reasoning": "Ensemble learning mitigates individual model biases by aggregating predictions, addressing DNA sparsity through complementary feature extraction and reducing noise sensitivity via consensus mechanisms, which aligns with genomic data constraints.", "atomic_constraints": ["Constraint 1: Sequence Sparsity - Functional genomic elements occupy minimal segments within extensive non-coding regions, demanding focused feature extraction.", "Constraint 2: Regulatory Heterogeneity - Super-enhancers involve diverse transcription factor combinations requiring multi-pattern recognition capabilities.", "Constraint 3: Experimental Noise Variability - ChIP-seq data contains technical artifacts necessitating noise-invariant modeling approaches."], "distractors": [{"option": "A BERT-based architecture pre-trained on genome-scale sequences uses self-attention layers to contextualize nucleotide dependencies for super-enhancer classification, capturing long-range regulatory syntax.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 (Experimental Noise Variability) due to high parameter sensitivity amplifying technical artifacts in limited training data."}, {"option": "A single convolutional neural network with optimized kernel sizes scans DNA sequences for motif patterns, using max-pooling layers and dense connections to predict super-enhancer regions from local features.", "label": "Naive Application", "analysis": "Inadequate for Constraint 2 (Regulatory Heterogeneity) as monolithic architectures cannot capture diverse TF binding combinations."}, {"option": "Leveraging dna2vec embeddings, a bidirectional LSTM with attention mechanisms processes k-mer sequences to identify enhancer-promoter interactions critical for super-enhancer formation.", "label": "Cluster Competitor", "analysis": "Overlooks Constraint 1 (Sequence Sparsity) by treating entire regions uniformly, diluting sparse functional signals in fixed-length representations."}]}}
{"id": 280007211, "title": "A hybrid YOLO-UNet3D framework for automated protein particle annotation in Cryo-ET images", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Hybrid YOLO-UNet3D"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Automated detection of protein particles in Cryo-ET tomograms is challenged by extreme noise, low contrast, and complex 3D structural variations requiring precise volumetric localization.", "adaptation_ground_truth": "A hybrid framework where YOLO rapidly localizes candidate particles in 3D volumes, followed by UNet3D refining segmentation within localized regions to balance speed and precision.", "ground_truth_reasoning": "YOLO efficiently handles sparse particle distribution in large volumes, while UNet3D leverages local contextual details for accurate segmentation. This cascaded design optimizes computational resources and overcomes noise limitations inherent to Cryo-ET imaging.", "atomic_constraints": ["Constraint 1: Low Signal-to-Noise Ratio - Cryo-ET data exhibits extreme noise from low electron doses, necessitating robust feature extraction.", "Constraint 2: Volumetric Sparsity - Protein particles occupy <5% of cellular tomograms, demanding efficient search strategies.", "Constraint 3: Anisotropic Resolution - Z-axis resolution is 3-5× lower than XY, requiring asymmetric processing kernels.", "Constraint 4: Structural Heterogeneity - Particles exhibit variable conformations, needing flexible shape modeling."], "distractors": [{"option": "End-to-end SwinVFTR transformer processes entire tomograms using self-attention mechanisms to capture long-range 3D dependencies for unified detection and segmentation.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Full-volume processing ignores volumetric sparsity, incurring prohibitive computational costs (~8× memory overhead) for terabyte-scale datasets."}, {"option": "Standard 3D U-Net with dilated convolutions and residual blocks processes full tomograms at multiple scales, optimized via focal loss and extensive data augmentation.", "label": "Naive Application", "analysis": "Violates Constraint 1: Uniform processing amplifies noise in empty regions, reducing precision for low-contrast particles without localization pre-filtering."}, {"option": "scDFC clustering adapted for Cryo-ET: Jointly embeds voxel features and geometric relationships via deep fusion, then partitions volumes using self-supervised clustering objectives.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Untrained clustering cannot distinguish heterogeneous particle morphologies from noise artifacts without annotated supervision."}]}}
{"id": 277218713, "title": "High intra-laboratory reproducibility of nanopore sequencing in bacterial species underscores advances in its accuracy", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Sequence Alignment (specifically using Edit Distance)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate alignment of error-prone nanopore sequencing reads for bacterial genomics, where high error rates (10-15%) challenge reproducibility in clinical and epidemiological applications.", "adaptation_ground_truth": "Implementation of Edlib for exact edit distance alignment, enabling precise nucleotide-level matching of nanopore reads to reference genomes. This accounts for high indel error rates while maintaining computational efficiency for bacterial whole-genome analysis.", "ground_truth_reasoning": "Edlib's exact edit distance algorithm directly addresses nanopore error profiles (Constraint 1) by quantifying minimal mutations without heuristics. Its C/C++ optimization handles large bacterial genomes efficiently (Constraint 3), while deterministic outputs ensure reproducibility (Constraint 2). The method's sensitivity to single-base variations supports strain discrimination (Constraint 4).", "atomic_constraints": ["Constraint 1: Error Profile Specificity - Nanopore sequencing generates predominantly indel errors in homopolymer regions, requiring alignment robust to gap formations.", "Constraint 2: Reproducibility Mandate - Clinical bacterial typing demands identical variant calls across repeated runs for diagnostic reliability.", "Constraint 3: Genomic-Scale Efficiency - Bacterial genomes (2-10 Mbp) require O(n) alignment algorithms to process multi-sample datasets feasibly.", "Constraint 4: Variant Sensitivity - Strain identification necessitates detection of single-nucleotide polymorphisms in conserved genomic regions."], "distractors": [{"option": "Applying a pre-trained genomic transformer model to embed nanopore reads, then using attention-based similarity scoring for alignment. This leverages state-of-the-art contextual representations for sequence matching.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 (Genomic-Scale Efficiency) due to GPU memory limitations with long reads and Constraint 2 (Reproducibility Mandate) as stochastic attention mechanisms yield variable alignments across runs."}, {"option": "Standard BLAST alignment with optimized scoring matrices for bacterial genomes, including gap-open penalties adjusted for nanopore characteristics. Followed by SAMtools processing for variant calling consistency.", "label": "Naive Application", "analysis": "Violates Constraint 1 (Error Profile Specificity) due to heuristic seed-and-extend approaches missing indels in homopolymers and Constraint 4 (Variant Sensitivity) from reduced SNP detection in low-complexity regions."}, {"option": "Prodigal-based gene prediction followed by chewBBACA allele alignment, creating a pangenome schema for strain typing. This leverages gene-level homology detection for taxonomic classification.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4 (Variant Sensitivity) as gene-centric approaches overlook intergenic SNPs and Constraint 1 (Error Profile Specificity) since frameshift errors from indels disrupt gene prediction accuracy."}]}}
{"id": 275978337, "title": "Caps-ac4C: an effective computational framework for identifying N4-acetylcytidine sites in human mRNA based on deep learning.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Capsule Network"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Identifying N4-acetylcytidine (ac4C) sites in human mRNA requires modeling spatial hierarchies in nucleotide sequences, as acetylation depends on precise positional relationships within RNA secondary structures.", "adaptation_ground_truth": "Caps-ac4C employs capsule networks with dynamic routing to hierarchically model nucleotide spatial relationships. Capsules preserve positional information through vector outputs and routing-by-agreement, capturing motif constellations critical for ac4C site recognition.", "ground_truth_reasoning": "Capsule networks address spatial hierarchy constraints by representing motifs as instantiated parameters in vector capsules. Dynamic routing maintains equivariance to positional shifts, while vector outputs preserve spatial relationships lost in scalar pooling. This architecture captures nested biochemical context with limited data by avoiding destructive feature compression.", "atomic_constraints": ["Constraint 1: Spatial Hierarchy - Nucleotide motifs form position-dependent constellations where sub-motif arrangements determine acetylation likelihood.", "Constraint 2: Limited Data - Experimentally confirmed ac4C sites are sparse, necessitating parameter-efficient models that avoid overfitting.", "Constraint 3: Positional Equivariance - Predictive features must remain detectable regardless of motif position in variable-length sequences."], "distractors": [{"option": "A BERT-based transformer fine-tuned on mRNA sequences predicts ac4C sites using self-attention mechanisms. This captures global dependencies through multi-head attention layers and contextual embeddings of nucleotide tokens.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformers require massive datasets to converge, while limited ac4C annotations cause overfitting. Self-attention also ignores precise spatial hierarchies critical for motif recognition."}, {"option": "Standard CNNs process one-hot encoded sequences with convolutional filters and max-pooling layers. Multiple filter sizes extract local motifs, followed by fully connected layers for classification using ReLU activations.", "label": "Naive Application", "analysis": "Violates Constraint 1: Max-pooling destroys spatial relationships between motifs. Fixed filter sizes cannot capture hierarchical part-whole dependencies essential for ac4C context modeling."}, {"option": "Chaos Game Representation (CGR) encodes sequences into fractal images. An ensemble of convolutional networks processes CGR inputs with varying kernel sizes, combining predictions through weighted averaging.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: CGR spatializes sequence positions non-linearly, breaking translational equivariance. Ensemble CNNs compound positional sensitivity, struggling with motif shifts in mRNA regions."}]}}
{"id": 276637091, "title": "Towards multi-fusion graph neural network for single-cell RNA sequence clustering", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Graph Neural Network (GNN)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Single-cell RNA sequencing data exhibits high dimensionality, sparsity, and complex cell-type relationships that challenge traditional clustering methods to capture non-linear biological structures effectively.", "adaptation_ground_truth": "Multi-fusion GNN integrates gene expression, regulatory networks, and protein interactions through attention-based graph fusion. This constructs a unified cell-gene graph where GNN layers propagate information across biological hierarchies, optimizing embeddings for clustering via joint reconstruction and self-supervised losses.", "ground_truth_reasoning": "The fusion mechanism addresses scRNA-seq sparsity by combining complementary data sources, while GNNs capture non-linear cell relationships. Attention weights adapt to varying feature reliability, and joint optimization preserves both local graph structures and global cluster separability.", "atomic_constraints": ["Constraint 1: High-dimensional sparsity - Gene expression matrices contain >20k dimensions with >90% zero values, requiring robust feature integration.", "Constraint 2: Multi-modal relationships - Biological meaning emerges from simultaneous cell-gene-regulator interactions across hierarchical scales.", "Constraint 3: Non-linear manifolds - Cell-type transitions form continuous, curved trajectories in latent space.", "Constraint 4: Biological hierarchy preservation - Methods must maintain gene regulatory dependencies during dimensionality reduction."], "distractors": [{"option": "A transformer model pre-trained on gene sequences processes expression profiles as token sets. Multi-head attention layers capture global dependencies, with k-means clustering on final embeddings. Leverages large-scale biological pretraining for contextual gene relationships.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformers process genes as independent tokens, disregarding regulatory network structures and hierarchical biological dependencies essential for cell identity."}, {"option": "Standard GCN applied to k-nearest neighbor graph from PCA-reduced expression. Two convolutional layers with ReLU activation produce embeddings. Spectral clustering partitions cells using cosine similarity in embedding space. Includes batch normalization and dropout for regularization.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Single-graph input amplifies sparsity noise; linear PCA preprocessing collapses non-linear manifolds critical for rare cell-type discrimination."}, {"option": "Ultra-scalable spectral clustering with ensemble similarity matrices. Combines gene co-expression, pathway cosine similarity, and epigenetic Jaccard indices. Approximated eigen decomposition enables handling 1M+ cells. Cluster assignments refined via iterative subspace optimization.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Matrix factorization decouples biological hierarchies, losing regulatory dependencies between gene modules and cellular phenotypes during embedding."}]}}
{"id": 274060053, "title": "Enhancing gene set overrepresentation analysis with large language models", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Traditional gene set overrepresentation analysis relies on static databases that lack real-time biological context and cannot interpret complex gene-phenotype relationships from emerging literature.", "adaptation_ground_truth": "Integrating a biomedical-domain fine-tuned transformer LLM that dynamically generates context-aware gene sets by interpreting literature, then applies statistical enrichment analysis to capture nuanced biological relationships beyond predefined databases.", "ground_truth_reasoning": "The fine-tuned transformer respects Biological Knowledge Dynamism by processing current literature, addresses Contextual Variability through semantic understanding of gene interactions, and overcomes Data Sparsity via few-shot learning from limited domain examples while maintaining computational tractability.", "atomic_constraints": ["Constraint 1: Biological Knowledge Dynamism - Biomedical knowledge evolves rapidly through new publications, requiring continuous updates beyond static databases.", "Constraint 2: Contextual Variability - Gene functions exhibit tissue/disease-specific behaviors demanding adaptive interpretation unavailable in fixed ontologies.", "Constraint 3: Data Sparsity - Novel gene-phenotype relationships have minimal curated examples, necessitating inference from unstructured text."], "distractors": [{"option": "Apply a generic foundation model like GPT-4 with standard prompting to directly output gene set associations, leveraging its broad pretraining without domain-specific adjustments for enrichment calculations.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: Lacks biomedical fine-tuning, generating inaccurate gene sets from outdated or non-specific knowledge while ignoring contextual biological nuances."}, {"option": "Use predefined KEGG pathways with Camera's competitive statistical framework, incorporating inter-gene correlation adjustments through established covariance matrices for enrichment significance testing.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Static databases miss recent discoveries and cannot infer novel relationships, failing when gene sets lack experimental curation."}, {"option": "Employ retrieval-augmented generation by querying PubMed with user genes, synthesizing gene sets from top abstracts using a language model, followed by standard overrepresentation analysis.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 and 3: Retrieved snippets provide fragmented context, missing implicit gene interactions and performing poorly for sparse or emerging phenotypes without structured reasoning."}]}}
{"id": 279452528, "title": "From Code to Life: The AI‐Driven Revolution in Genome Editing", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Deep Convolutional Neural Network (CNN)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate prediction of CRISPR guide RNA activity requires modeling local nucleotide dependencies and spatial patterns in DNA sequences that determine editing efficiency and specificity.", "adaptation_ground_truth": "A deep convolutional neural network processes one-hot encoded gRNA-DNA sequences as 2D spatial maps. Convolutional filters scan for position-invariant local motifs and nucleotide dependencies, while hierarchical layers capture complex patterns. Regularization handles limited data.", "ground_truth_reasoning": "CNNs excel at detecting local spatial patterns through convolutional filters, directly addressing motif position invariance and nucleotide context constraints. Their parameter-sharing design manages data scarcity, while hierarchical processing captures multi-scale sequence determinants essential for activity prediction.", "atomic_constraints": ["Motif Position Invariance - Functional nucleotide motifs (e.g., PAM-proximal regions) must be recognized regardless of exact sequence position.", "Nucleotide Context Dependency - Editing outcomes depend on local nucleotide neighborhoods (3-5 base windows), not isolated bases.", "Data Sparsity - Experimentally validated gRNA activity datasets are limited (<10^4 samples), requiring sample-efficient architectures.", "Spatial Hierarchy - Predictive features exist at multiple scales: single nucleotides, trinucleotide motifs, and full gRNA-DNA interactions."], "distractors": [{"option": "A transformer model pre-trained on genomic sequences uses self-attention to capture global dependencies in gRNA-DNA pairs. Fine-tuning predicts activity by weighting all nucleotide interactions across the full sequence length.", "label": "SOTA Bias", "analysis": "Violates Data Sparsity and Spatial Hierarchy constraints: transformers require massive datasets and emphasize global over local patterns, overlooking critical short-range motifs."}, {"option": "A standard CNN with fixed-size filters processes flattened sequence vectors. Multiple convolutional layers extract features followed by fully connected layers for regression output using standard pooling and activation functions.", "label": "Naive Application", "analysis": "Overlooks Nucleotide Context Dependency: fixed filters ignore variable motif lengths. Fully connected layers lose spatial hierarchy and require excessive parameters, conflicting with Data Sparsity."}, {"option": "A hybrid CNN-SVR model where convolutional layers extract sequence features, which are then fed into a support vector regressor. The SVR maps high-dimensional CNN outputs to activity scores using kernel methods.", "label": "Cluster Competitor", "analysis": "Violates Spatial Hierarchy: decoupling feature extraction and regression fragments end-to-end learning. SVRs struggle with hierarchical biological constraints and scale poorly with feature dimensions."}]}}
{"id": 279990489, "title": "Generation of protein dynamics by machine learning.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Denoising Diffusion Probabilistic Models"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Modeling the dynamic conformational ensembles of intrinsically disordered proteins, which cannot be captured by static structure prediction methods like AlphaFold due to their lack of fixed tertiary structure.", "adaptation_ground_truth": "Denoising Diffusion Probabilistic Models (DDPM) adapted with physics-guided constraints to generate Boltzmann-distributed conformational ensembles. Energy-based guidance during denoising ensures physical realism in bond geometries and steric avoidance while sampling diverse states.", "ground_truth_reasoning": "DDPMs naturally handle multimodal distributions and gradual state transitions. Incorporating energy-based constraints during denoising enforces atomic-level physical plausibility and thermodynamic consistency, making them ideal for disordered protein ensembles where traditional methods fail.", "atomic_constraints": ["Constraint 1: Boltzmann Sampling - Generated conformations must obey equilibrium thermodynamic distributions.", "Constraint 2: Steric Viability - Atomic clashes must be avoided in all generated structures.", "Constraint 3: Bond Geometry Preservation - Bond lengths and angles must remain within biophysically feasible ranges.", "Constraint 4: Conformational Diversity - Models must capture the full breadth of disordered protein ensembles."], "distractors": [{"option": "A transformer-based autoregressive model generates protein sequences stepwise, followed by AlphaFold2 structure prediction. This leverages large-scale pretraining on protein databases to infer dynamic states from evolutionary patterns.", "label": "SOTA Bias", "analysis": "Violates Constraint 4: Transformers prioritize sequence covariation over physical dynamics, producing biased ensembles that overlook non-evolutionary conformational states."}, {"option": "Standard DDPM applied directly to protein Cartesian coordinates with fixed Gaussian noise schedules. Training uses maximum likelihood estimation on MD trajectory datasets without energy-based regularization.", "label": "Naive Application", "analysis": "Violates Constraint 2 and 3: Lacks physics-based guidance, frequently generating steric clashes and distorted bond geometries due to unconstrained denoising."}, {"option": "Flow Matching generative models simulate dynamics by learning velocity fields between conformational states. Neural networks regress continuous-time paths using optimal transport principles on coarse-grained protein representations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Flow Matching assumes predefined intermediate distributions, struggling to enforce Boltzmann equilibrium across diverse disordered states without explicit energy terms."}]}}
{"id": 276778119, "title": "The effectiveness of large language models with RAG for auto-annotating trait and phenotype descriptions", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "RAG (Retrieval-Augmented Generation)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurate auto-annotation of biological traits requires precise mapping of variable natural language descriptions to standardized ontology terms, overcoming terminology ambiguity and sparse domain-specific training data.", "adaptation_ground_truth": "A RAG framework retrieves relevant ontology definitions from curated databases like Ontobee, then conditions an LLM to generate context-aware annotations. This combines real-time knowledge access with semantic understanding for phenotype descriptions.", "ground_truth_reasoning": "RAG addresses ontological precision by retrieving authoritative definitions, mitigates data sparsity through external knowledge bases, handles terminology variability via contextual generation, and supports dynamic updates without model retraining.", "atomic_constraints": ["Constraint 1: Ontological Precision - Annotations must map to exact ontology terms with strict hierarchical relationships to maintain biological consistency.", "Constraint 2: Terminology Variability - Identical traits exhibit diverse linguistic descriptions (synonyms, abbreviations) requiring context-aware normalization.", "Constraint 3: Data Sparsity - Rare phenotypes lack sufficient labeled examples for training data-hungry models.", "Constraint 4: Dynamic Knowledge - Evolving biological ontologies necessitate annotation systems that incorporate updates without full retraining."], "distractors": [{"option": "Directly applying GPT-4 for zero-shot ontology term prediction using its internal knowledge. The model generates annotations based solely on pre-trained biological understanding without external data retrieval.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 4: Lacks ontological precision due to potential term hallucination and cannot dynamically integrate updated ontology definitions without retraining."}, {"option": "Training a BERT model on labeled phenotype descriptions to classify ontology terms. The system uses token-level embeddings and a classification layer, optimized via cross-entropy loss on existing datasets.", "label": "Naive Application", "analysis": "Violates Constraint 2 and 3: Struggles with terminology variability due to fixed embeddings and exhibits poor generalization for rare traits from sparse training examples."}, {"option": "Implementing ontologyX for rule-based term matching through syntactic pattern recognition. The R package processes text inputs using predefined lexical rules and ontology hierarchies to assign annotations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2 and 4: Fails to handle complex terminology variations requiring contextual disambiguation and requires manual rule updates for new ontology versions."}]}}
{"id": 276104838, "title": "Deep learning powered single-cell clustering framework with enhanced accuracy and stability", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Structural Deep Clustering Network"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Graph-based clustering methods for scRNA-seq data neglect node distribution and suffer from GCN oversmoothing, limiting cell-type discrimination accuracy.", "adaptation_ground_truth": "scG-cluster integrates dual-topology adjacency graphs capturing spatial node distribution and TAGCN with attention-weighted features plus residual connections to prevent oversmoothing, iteratively refining cluster centers.", "ground_truth_reasoning": "The dual-topology graph satisfies spatial distribution constraints by enriching structural relationships. TAGCN's attention mechanism prioritizes discriminative features while residual connections counteract oversmoothing, preserving subtle expression differences. Iterative refinement ensures stability against data variability.", "atomic_constraints": ["Constraint 1: Spatial Distribution Preservation - Must encode spatial relationships between cells beyond pairwise similarities to avoid incomplete population representations.", "Constraint 2: Discriminative Feature Retention - Must prevent oversmoothing in deep architectures to maintain sensitivity to minor expression profile variations.", "Constraint 3: Iterative Stability - Requires progressive optimization of cluster centroids to ensure consistent assignments across biological replicates."], "distractors": [{"option": "We developed scTransformer, using a hierarchical vision transformer with patch embedding of expression matrices. Multi-head self-attention captures global dependencies, and contrastive learning refines embeddings. Clustering is performed via spectral partitioning on attention-weighted node features.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Transformers lack inherent mechanisms to prevent oversmoothing in graph contexts and ignore spatial distribution (Constraint 1), focusing only on global dependencies irrelevant to local cell neighborhoods."}, {"option": "Our framework applies a standard 3-layer GCN on k-NN similarity graphs. We use symmetric normalization and ReLU activations, optimizing with KL-divergence clustering loss. Embeddings are clustered via Leiden algorithm, with hyperparameters tuned on validation data.", "label": "Naive Application", "analysis": "Violates Constraints 1 and 2: Standard GCNs cannot capture spatial node distributions and inherently suffer from oversmoothing, losing discriminative power for similar cells. Lacks iterative refinement (Constraint 3)."}, {"option": "scVAE-Kmeans combines variational autoencoders with deep k-means. The VAE compresses data preserving neighborhood topology, while self-training k-means with student-t weighting assigns clusters. Cluster centers are updated via momentum-based optimization.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: Autoencoders capture pairwise similarities but fail to explicitly model spatial node distributions. Momentum updates partially address stability (Constraint 3) but offer no solution to GCN-specific oversmoothing (Constraint 2)."}]}}
{"id": 266054855, "title": "Controllable Protein Design by Prefix-Tuning Protein Language Models", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Controllable generation of functional protein sequences that satisfy structural stability and specific biochemical properties.", "adaptation_ground_truth": "Prefix-tuning protein language models by prepending trainable continuous vectors to input embeddings. This conditions sequence generation on desired properties while leveraging evolutionary knowledge from pre-trained weights without full fine-tuning.", "ground_truth_reasoning": "Prefix-tuning maintains the frozen PLM's understanding of evolutionary constraints (atomic stability, folding rules) while enabling precise control via lightweight trainable parameters. This balances structural viability with property-specific generation, avoiding destabilizing mutations.", "atomic_constraints": ["Constraint 1: Structural Foldability - Protein sequences must adopt stable tertiary structures with specific secondary elements (α-helices/β-sheets).", "Constraint 2: Evolutionary Plausibility - Designed sequences require amino acid compositions observed in natural proteins to ensure biological functionality.", "Constraint 3: Property-Specific Control - Generation must precisely tune biochemical properties (e.g., binding affinity) without compromising structural integrity."], "distractors": [{"option": "Fine-tuning a transformer language model with property-specific tokens concatenated to input sequences. The model adjusts all weights during training on target datasets to optimize functional objectives.", "label": "SOTA Bias", "analysis": "Violates Constraint 2: Full fine-tuning destabilizes evolutionary knowledge in pre-trained weights, causing non-viable sequences. Property tokens lack continuous latent steering for precise control."}, {"option": "Directly sampling sequences from a pre-trained protein language model using temperature-controlled decoding. Generated outputs are filtered by structure prediction tools to select stable designs.", "label": "Naive Application", "analysis": "Violates Constraint 3: Uncontrolled sampling cannot target specific properties. Post-hoc filtering fails to enforce biochemical requirements during generation."}, {"option": "Training a GAN with a generator creating sequences and a discriminator evaluating structural stability. Feedback from TM-align scores refines adversarial training for foldable proteins.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: GANs ignore co-evolutionary patterns critical for foldability. Adversarial instability produces non-convergent designs violating atomic packing constraints."}]}}
{"id": 278095384, "title": "SCassist: An AI Based Workflow Assistant for Single-Cell Analysis", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Single-cell analysis requires complex, multi-step workflows with interdependent parameter choices that demand domain expertise, creating accessibility barriers for non-specialists.", "adaptation_ground_truth": "SCassist integrates a Transformer-based language model with domain-specific tools and structured workflow knowledge. It interprets natural language queries to generate context-aware analysis pipelines, dynamically adjusting steps based on biological constraints.", "ground_truth_reasoning": "The Transformer architecture handles sequential dependencies in workflow steps while domain fine-tuning captures biological context. Tool integration ensures parameter choices respect data sparsity and batch effects, enabling non-expert accessibility without sacrificing analytical rigor.", "atomic_constraints": ["Constraint 1: Data Sparsity - Single-cell RNA-seq exhibits high zero-inflation due to dropout effects, requiring specialized normalization.", "Constraint 2: Batch Effects - Technical variations between experiments necessitate explicit correction before cross-dataset analysis.", "Constraint 3: Workflow Interdependence - Parameter choices in early steps (e.g., dimensionality reduction) constrain downstream tool compatibility."], "distractors": [{"option": "A foundation model like GPT-4 processes natural language queries about single-cell analysis using its broad knowledge base. It suggests comprehensive workflow steps by retrieving patterns from diverse scientific literature without explicit tool integration.", "label": "SOTA Bias", "analysis": "Violates Constraint 3 by ignoring workflow interdependence; lacks domain-specific corrections for batch effects and sparsity, leading to incompatible tool sequences."}, {"option": "A standard BERT architecture processes user queries and outputs workflow recommendations based on semantic similarity to published protocols. It includes standard hyperparameter tuning and attention mechanisms for general biomedical text understanding.", "label": "Naive Application", "analysis": "Violates Constraints 1-2: Generic text processing fails to encode data sparsity characteristics or batch effect constraints, producing biologically invalid parameter sets."}, {"option": "Using divide-and-conquer contrastive learning from Cluster A, this method clusters similar experimental designs. Workflows are recommended by matching query embeddings to cluster centroids, leveraging cell representation similarities.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Reliance on data patterns overlooks workflow logic dependencies and parameter sensitivities, causing tool incompatibility in multi-step analyses."}]}}
{"id": 281746022, "title": "A trimodal protein language model enables advanced protein searches.", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Existing protein language models rely primarily on sequence data, neglecting structural and functional information critical for comprehensive protein understanding and precise biological searches.", "adaptation_ground_truth": "A transformer architecture integrating sequence, structure, and function modalities. It employs cross-modal attention to align representations from amino acid sequences, predicted 3D structures, and functional annotations, enabling holistic protein similarity searches.", "ground_truth_reasoning": "The trimodal design addresses atomic constraints by: 1) Capturing interdependent sequence-structure-function relationships via attention mechanisms, 2) Handling structural complexity through precomputed embeddings, 3) Mitigating functional sparsity by fusing heterogeneous databases, and 4) Enabling joint queries across modalities.", "atomic_constraints": ["Constraint 1: Multimodal Interdependence - Protein function emerges from interdependent sequence-structure-context relationships.", "Constraint 2: Structural Complexity - 3D protein folds require geometric representations incompatible with sequence-only models.", "Constraint 3: Functional Sparsity - Biological annotations are fragmented across databases with varying coverage and noise.", "Constraint 4: Cross-Modal Alignment - Protein searches demand joint similarity assessment across sequence, structure, and function."], "distractors": [{"option": "A foundation transformer pretrained on billions of protein sequences using unsupervised learning. It generates novel protein variants and infers evolutionary patterns through deep contextual embeddings for sequence-based functional annotation.", "label": "SOTA Bias", "analysis": "Violates Constraints 1 and 2 by ignoring structural data and functional context, relying solely on sequences despite their inability to fully encode 3D folding or biological roles."}, {"option": "A standard protein language model processing amino acid sequences with transformer layers. It outputs residue-level embeddings for similarity searches using k-nearest neighbors, optimized via masked language modeling on sequence corpora.", "label": "Naive Application", "analysis": "Violates Constraints 1 and 4 by lacking structural/functional integration, limiting searches to sequence parallels without capturing higher-order biological characteristics."}, {"option": "A graph convolutional network analyzing protein structures as residue graphs. It propagates spatial features through message-passing layers to predict functional sites, using AlphaFold-predicted structures as input for annotation tasks.", "label": "Cluster Competitor", "analysis": "Violates Constraints 3 and 4 by focusing exclusively on structural data, neglecting sequence evolution and functional context needed for comprehensive searches."}]}}
{"id": 277781462, "title": "Causal integration of chemical structures improves representations of microscopy images for morphological profiling", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Graph Neural Networks (GNNs)"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Current morphological profiling methods treat chemical compounds as black-box inputs, ignoring how atomic-level structural properties causally influence cellular phenotypes in microscopy images, limiting biological interpretability.", "adaptation_ground_truth": "A graph neural network integrates microscopy images with molecular graph representations of chemical compounds through causal feature fusion. This explicitly models atomic bonds and functional groups as determinants of cellular morphology, enforcing structure-phenotype causality in joint embeddings.", "ground_truth_reasoning": "GNNs inherently process graph-structured chemical data (Constraint 1) while causal fusion captures directional structure-phenotype relationships (Constraint 2). The joint embedding preserves spatial image features and molecular topology (Constraint 3), avoiding confounding variables in phenotype prediction.", "atomic_constraints": ["Constraint 1: Molecular Topology - Chemical properties emerge from graph-structured atomic connections requiring explicit bond and node representations.", "Constraint 2: Causal Directionality - Compound structures determine cellular phenotypes, not vice versa; models must preserve this asymmetric causality.", "Constraint 3: Multimodal Alignment - Representations must bridge spatial image features (micrometers) and molecular features (angstroms) without information loss."], "distractors": [{"option": "A vision-language foundation model processes microscopy images and SMILES string embeddings of compounds. Cross-attention layers align image patches with chemical tokens, leveraging pretrained knowledge from natural image-text pairs for zero-shot morphological profiling.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 and 2: SMILES strings discard molecular topology, while attention mechanisms model correlations rather than causal structure-phenotype relationships."}, {"option": "Standard GNN processes molecular graphs into embeddings, while a CNN extracts image features. These vectors are concatenated before a fully connected layer predicts morphological profiles, using batch normalization and dropout for regularization.", "label": "Naive Application", "analysis": "Violates Constraint 2: Concatenation fails to model causal dependencies between chemical structures and cellular phenotypes, allowing confounding factors in joint representations."}, {"option": "ResNet extracts features from microscopy images, while molecular fingerprints encode chemical structures. A multilayer perceptron combines these inputs with metadata to predict morphological profiles, using transfer learning from ImageNet initialization.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1 and 3: Fingerprints lose graph topology, and MLP fusion cannot align spatial image hierarchies with molecular substructures across scales."}]}}
{"id": 275442913, "title": "Two decades of advances in sequence-based prediction of MoRFs, disorder-to-order transitioning binding regions", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Bayesian Inference"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting short disordered protein regions (MoRFs) that fold upon binding to partners, despite sparse experimental data and heterogeneous binding mechanisms.", "adaptation_ground_truth": "Hierarchical Bayesian model integrating evolutionary profiles, disorder predictions, and physicochemical features to quantify uncertainty in MoRF identification.", "ground_truth_reasoning": "The hierarchical structure combines weak signals from sparse MoRF data through probabilistic priors, addressing data scarcity while modeling heterogeneous binding behaviors via feature integration. Bayesian inference quantifies prediction confidence, essential for transient interactions.", "atomic_constraints": ["Constraint 1: Data Scarcity - Limited experimental annotations for transient MoRF complexes due to structural instability.", "Constraint 2: Binding Heterogeneity - MoRFs interact with diverse partners (proteins/peptides/lipids) requiring multi-feature integration.", "Constraint 3: Disorder-Order Transition - Sequences must exhibit latent folding propensity upon binding despite intrinsic disorder."], "distractors": [{"option": "Fine-tune ESM-2 transformer on MoRF sequences to predict binding regions using self-attention over residue embeddings, leveraging transfer learning from large protein language models.", "label": "SOTA Bias", "analysis": "Violates Constraint 1 by requiring extensive training data unavailable for rare MoRF interactions, ignoring uncertainty quantification crucial for sparse annotations."}, {"option": "Apply standard Bayesian networks with position-specific scoring matrices (PSSMs) as input features, implementing expectation-maximization for parameter optimization on sequence windows.", "label": "Naive Application", "analysis": "Violates Constraint 2 by omitting hierarchical integration of disorder scores and physicochemical features needed for partner-specific binding heterogeneity."}, {"option": "Develop HMM profiles capturing consensus MoRF sequences, trained on aligned binding motifs with Viterbi decoding for state transitions across protein domains.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3 by assuming rigid contiguous motifs, disregarding context-dependent disorder-to-order transitions and non-local interactions."}]}}
{"id": 275368630, "title": "Ensemble learning-based predictor for driver synonymous mutation with sequence representation", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Ensemble Learning"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Accurately identifying driver synonymous mutations that impact disease progression despite not altering amino acid sequences, requiring nuanced sequence-context analysis beyond protein-level effects.", "adaptation_ground_truth": "An ensemble framework integrating multiple base models (e.g., SVM, RF) with diverse sequence-derived feature representations (k-mer frequencies, conservation scores, structural motifs) to capture complementary biological signals and improve prediction robustness.", "ground_truth_reasoning": "The ensemble approach addresses heterogeneous biological constraints by combining specialized models: each base learner targets distinct sequence features (e.g., local nucleotide dependencies, evolutionary conservation), compensating for individual model limitations. This integration mitigates data sparsity issues and captures non-linear feature interactions inherent in genomic contexts, enhancing sensitivity to subtle functional impacts of synonymous variants.", "atomic_constraints": ["Constraint 1: Sequence Context Sensitivity - Synonymous mutation effects depend on precise local nucleotide environment (e.g., splicing regulators, RNA stability motifs), requiring position-aware feature extraction.", "Constraint 2: Feature Heterogeneity - Predictive signals span orthogonal biological layers (conservation, structural dynamics, k-mer statistics), demanding multi-source integration.", "Constraint 3: Data Sparsity - Clinically validated driver mutations are extremely rare, necessitating robustness to class imbalance and noise.", "Constraint 4: Non-linear Epistasis - Biological impacts arise from complex interactions between distant sequence elements, requiring high-order pattern recognition."], "distractors": [{"option": "A DNABERT transformer model pre-trained on genome-scale sequences and fine-tuned with task-specific labels, leveraging self-attention mechanisms to capture long-range nucleotide dependencies for mutation impact prediction.", "label": "SOTA Bias", "analysis": "Violates Constraint 3: Transformers require massive labeled data for effective fine-tuning, but driver synonymous mutations are extremely sparse, leading to overfitting on limited positive samples despite theoretical context-awareness."}, {"option": "A single random forest classifier trained on consolidated sequence features (including conservation profiles and codon usage indices), with feature importance weighting and stratified sampling to handle class imbalance.", "label": "Naive Application", "analysis": "Violates Constraint 2: A monolithic model cannot adequately resolve conflicts between heterogeneous feature types (e.g., structural vs. evolutionary signals), causing information loss when diverse biological constraints compete."}, {"option": "A predictor using DNAshapeR-derived structural descriptors (minor groove width, helix twist) as input to a gradient-boosted tree model, correlating DNA shape perturbations with functional impacts of mutations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 1: DNAshapeR focuses on local structural geometry but overlooks distal regulatory motifs (e.g., exonic splicing enhancers), missing critical context-dependent effects of synonymous variants."}]}}
{"id": 270766548, "title": "TarDis: Achieving Robust and Structured Disentanglement of Multiple Covariates", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Variational Autoencoder (VAE) / beta-VAE variant"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Disentangling overlapping biological and technical covariates (e.g., cell types, batch effects) in sparse single-cell RNA-seq data where factors exhibit hierarchical dependencies.", "adaptation_ground_truth": "TarDis introduces covariate-specific latent subspaces with hierarchical regularization, using a β-VAE framework with targeted KL divergences per covariate to enforce structured disentanglement.", "ground_truth_reasoning": "Hierarchical subspaces accommodate biological dependencies between covariates (Constraint 3), targeted KL terms handle sparse data by preventing factor collapse (Constraint 1), and covariate-specific separation resolves entanglement without losing interpretability (Constraint 2).", "atomic_constraints": ["Constraint 1: High-dimensional sparsity - Single-cell RNA-seq data exhibits extreme zero-inflation and low counts per gene, necessitating robustness to dropout noise.", "Constraint 2: Covariate entanglement - Biological factors (e.g., cell state) and technical artifacts (e.g., batch) exhibit non-linear interactions in high-dimensional space.", "Constraint 3: Biological hierarchy - Covariates exist in dependency structures (e.g., cell type → subpopulation) requiring non-independent latent representations."], "distractors": [{"option": "A vision transformer architecture pre-trained on ImageNet is fine-tuned using gene expression patches. Attention maps are used to isolate covariate influences via gradient-based attribution, leveraging transfer learning for feature extraction.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Transformers require dense data, collapsing under single-cell sparsity; attention mechanisms misinterpret gene-gene interactions as spatial relationships."}, {"option": "Standard β-VAE with isotropic Gaussian prior trains on gene expression data. KL divergence weight is increased to enhance factor separation, and a multilayer perceptron decoder handles non-linear reconstruction.", "label": "Naive Application", "analysis": "Ignores Constraint 3: Isotropic latent space cannot capture hierarchical covariate dependencies, mixing cell types and technical factors due to unconstrained interactions."}, {"option": "Invariant Risk Minimization learns representations invariant to batches by optimizing across multiple experimental environments. Domain classifiers enforce batch-agnostic features while preserving biological variations.", "label": "Cluster Competitor", "analysis": "Violates Constraint 2: Invariance collapses entangled technical/biological factors; single representation cannot hierarchically separate covariates like cell subtypes."}]}}
{"id": 276450303, "title": "On inputs to deep learning for RNA 3D structure prediction", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Transformer"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "RNA 3D structure prediction requires modeling intricate spatial dependencies and non-local interactions that sequence alone cannot capture.", "adaptation_ground_truth": "Transformer architecture modified to incorporate geometric and chemical priors of RNA base pairs as attention biases, with inputs encoding torsion angles and spatial relationships.", "ground_truth_reasoning": "The adaptation integrates domain-specific constraints (base pair geometry, electrostatic forces) directly into attention mechanisms, enabling the model to respect RNA's 3D biophysics while leveraging sequence patterns.", "atomic_constraints": ["Constraint 1: Base Pair Geometry - RNA structures rely on specific hydrogen-bonding patterns and base stacking orientations defined by geometric nomenclature.", "Constraint 2: Electrostatic Repulsion - Negatively charged phosphate backbones require explicit distance constraints to avoid steric clashes.", "Constraint 3: Torsion Angle Limits - Backbone flexibility is bounded by energetically feasible α/γ dihedral angles in nucleotide chains.", "Constraint 4: Non-local Interactions - Tertiary contacts (e.g., pseudoknots) involve nucleotides distant in sequence but proximal in 3D space."], "distractors": [{"option": "Employing a pre-trained genomic language model with self-supervised learning on unlabeled RNA sequences, using attention heads to predict atomic coordinates from nucleotide embeddings.", "label": "SOTA Bias", "analysis": "Violates Constraint 2 and 4: Lacks explicit geometric priors for phosphate repulsion and cannot resolve long-range spatial dependencies without 3D inductive biases."}, {"option": "Standard transformer with positional encoding of RNA sequences, trained end-to-end using MSE loss on predicted nucleotide distances and dihedral angles.", "label": "Naive Application", "analysis": "Violates Constraint 1 and 3: Ignores base-pairing rules and torsion angle feasibility, leading to physically implausible conformations despite sequence awareness."}, {"option": "UFold-inspired convolutional network processing sequence covariation and accessibility profiles, followed by kinematic chain assembly for 3D coordinates.", "label": "Cluster Competitor", "analysis": "Violates Constraint 4: Secondary-structure focused convolutions cannot capture non-local tertiary interactions critical for 3D folding."}]}}
{"id": 278258230, "title": "TransAgent: Dynamizing Transcriptional Regulation Analysis via Multi-omics-Aware AI Agent", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "AI Agent"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Transcriptional regulation involves dynamic, context-dependent interactions across genomics, epigenomics, and transcriptomics that static models fail to capture due to combinatorial complexity and cellular heterogeneity.", "adaptation_ground_truth": "TransAgent integrates ReAct-style reasoning-acting loops with multi-omics awareness, enabling iterative hypothesis generation (e.g., TF binding predictions) and environment interaction (e.g., querying FANTOM5/dbSUPER) to resolve regulatory dynamics.", "ground_truth_reasoning": "The agent framework handles sparse/noisy data (Constraint 1) via evidence accumulation, addresses combinatorial complexity (Constraint 3) through stepwise logical inference, and adapts to cellular context (Constraint 2) by dynamically retrieving context-specific omics data.", "atomic_constraints": ["Constraint 1: Data Sparsity - Single-cell omics measurements yield sparse matrices with high dropout rates, requiring probabilistic evidence aggregation.", "Constraint 2: Context Dependency - Regulatory mechanisms (e.g., enhancer-promoter loops) are cell-type-specific and stimulus-responsive, demanding adaptive context modeling.", "Constraint 3: Combinatorial Complexity - Transcription factor cooperativity involves exponential interaction spaces, necessitating modular reasoning about regulatory logic."], "distractors": [{"option": "Fine-tune GPT-4 on CCLE and FANTOM5 datasets using prompt engineering to directly predict gene expression from epigenetic marks. Leverage its few-shot learning capability to infer regulatory relationships from limited examples.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Foundation models lack iterative evidence aggregation for sparse data, producing overconfident predictions from noisy inputs without active verification."}, {"option": "Implement a static graph neural network integrating genomic, epigenomic, and transcriptomic features from dbSUPER. Use attention mechanisms to weight regulatory edges and predict expression changes via supervised learning.", "label": "Naive Application", "analysis": "Violates Constraint 2: Fixed graph architectures cannot dynamically adjust to cell-specific contexts or new experimental stimuli without retraining."}, {"option": "Develop a WebGPT-based system that retrieves multi-omics evidence from curated databases (e.g., ENCODE) via browser tools. Synthesize regulatory hypotheses by summarizing retrieved snippets using few-shot prompting.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Passive retrieval cannot reason about combinatorial TF interactions or simulate regulatory logic, relying on pre-existing annotations."}]}}
{"id": 281883384, "title": "Generative AI for computational chemistry: A roadmap to predicting emergent phenomena", "taxonomy": {"domain": "Life Sciences", "sub": "Systems biology", "method": "Deep Neural Network"}, "data": {"is_valid_benchmark": "true", "scientific_problem": "Predicting emergent biomolecular phenomena like RNA folding requires modeling high-dimensional conformational landscapes and rare transitions inaccessible to traditional simulations.", "adaptation_ground_truth": "SE(3)-equivariant denoising diffusion model incorporating physical constraints during latent space optimization, trained on quantum-mechanical energy landscapes to generate thermodynamically stable structures.", "ground_truth_reasoning": "The diffusion process inherently handles high-dimensional sampling while SE(3)-equivariance preserves rotational/translational symmetry of molecular systems. Physical constraint integration ensures generated structures obey quantum mechanical laws without requiring exhaustive sampling.", "atomic_constraints": ["Constraint 1: SE(3) Equivariance - Molecular energy landscapes must be invariant to rotation/translation.", "Constraint 2: Rare-event Sampling - Biologically critical transitions occur at timescales exceeding conventional simulation limits.", "Constraint 3: Quantum Consistency - Electron distributions must satisfy Pauli exclusion and Coulombic interactions.", "Constraint 4: Topological Conservation - Base-pairing hierarchies in nucleic acids must obey Watson-Crick rules."], "distractors": [{"option": "Transformer-based autoregressive generation using attention mechanisms over atomic token sequences, trained on structural databases with positional encoding for 3D coordinate prediction.", "label": "SOTA Bias", "analysis": "Violates Constraint 1: Standard transformers lack inherent SE(3)-equivariance, requiring exponentially more data to approximate symmetries. Attention mechanisms struggle with quantum constraints without explicit physics integration."}, {"option": "Standard denoising diffusion model with U-Net backbone processing Cartesian coordinates directly, trained on molecular dynamics trajectories using Gaussian noise schedules.", "label": "Naive Application", "analysis": "Violates Constraint 1 & 3: Cartesian coordinate processing breaks rotational invariance. Absence of physical regularization permits quantum-mechanically invalid electron densities and bond geometries."}, {"option": "GFlowNet sampling with reward parametrization based on molecular mechanics energy functions, using graph neural networks to propose sequential structural modifications.", "label": "Cluster Competitor", "analysis": "Violates Constraint 3: Classical force fields cannot capture quantum electronic effects. Sequential node updates struggle with Constraint 2's rare events due to local search limitations."}]}}
